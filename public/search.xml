<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Kubernetes 系统基础]]></title>
    <url>%2F2019%2F10%2F24%2F(%E4%B8%80)Kubernetes%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[Kubernetes介绍什么是Kubernetes？ Kubernetes是容器集群管理系统，是一个开源的平台，可以实现容器集群的自动化部署、自动扩缩容、维护等功能。 使用Kubernetes可以： 自动化容器的部署和复制 随时扩展或收缩容器规模 将容器组织成组，并且提供容器间的负载均衡 很容易地升级应用程序容器的新版本 节省资源，优化硬件资源的使用 提供容器弹性，如果容器失效就替换它，等等… Kubernetes特点 便携性：支持公有云、私有云、混合云、多重云(multi-cloud) 可扩展：模块化、插件化、可组合、可挂载 自修复：自动部署，自动重启，自动复制，自动伸缩扩展 ##Kubernetes特性 Kubernetes是一种用于在一组主机上运行和协同容器化应用程序的系统，旨在提供可预测性、可扩展性与高可用的性的方法来完全管理容器化应用程序和服务的生命周期的平台。 它具有以下几个重要的特性： 自动装箱：构建于容器之上，基于资源依赖及其他约束自动完成容器部署且不影响其可用性，并通过调度机制混合关键型应用和非关键型应用的工作负载于同一节点以提升资源利用率。 自我修复：支持容器故障后自动重启、节点故障候重行调度容器，以及其他可用节点、健康状态检查失败后关闭容器并重新创建等自我修复机制。 水平扩展：支持通过简单命令或UI手动水平扩展，以及基于CPU等资源负载率的自动水平扩展机制。 服务器发现和负载均衡：Kubernetes通过其附加组件之一的KubeDNS（或CoreDNS）为系统内置了服务发现功能，它会为每个Service配置DNS名称，并允许集群内的客户端直接使用此名称发出访问请求，而Service则通过iptables或ipvs内建了负载均衡机制。 自动发布和回滚：Kubernetes支持“灰度”更新应用程序或其配置信息，它会监控更新过程中应用程序的健康状态，以确保它不会在同一时刻杀掉所有的实例，而此过程中一旦有故障发生，就会立即自动执行回滚操作。 秘钥和配置管理：Kubernetes允许存储和管理敏感信息，例如密码，Oauth令牌和ssh秘钥。可以部署和更新密码和应用程序配置，而无需重建容器，也不会再堆栈配置中暴露机密。 存储编排：Kubernetes支持Pod对象按需自动挂载不同类型的存储系统，这包括节点本地存储、公有云服务商的云存储（如AWS和GCP等），以及网络存储系统（例如，NFS、ISCSI、GlusterFS、Ceph、Cinder和Flocker等） 批量处理执行：除了服务型应用，Kubernetes还支持批处理作业及CI（持续集成），如果需要，一样可以实现容器故障后修复。 Kubernetes概述和术语 Kubernetes使用共享网络将多个物理机或虚拟机汇集到一个集群中，在各服务器之间进行通信，该集群是配置Kubernetes的所有组件、功能和工作负载的物理平台。集群中一台服务器（或高可用部署中的一组服务器）用作Master，负责管理整个集群，余下的其他机器用作Worker Node,它们是使用本地和外部资源接收和运行工作负载的服务器。集群中的这些主机可以是物理服务器，也可以是虚拟机（包括IaaS云端的VPS） Kubernetes系统基础/kubernetescluster.png) Master Master是集群的网关和中枢，负责诸如为用户和客户端暴露API、跟踪其它服务器的健康状态、以最优方式调度工作负载，以及编排其他组件之间的通信等任务，它是用户或客户端与集群之间的核心联络点，并负责Kubernetes系统的大多数集中式管控逻辑。单个Master节点即可完成其所有的功能，但出于冗余及负载均衡等目的，生产环境中通常需要协同部署多个此类主机。Master节点类似于蜂群中的蜂王。 Node Node是Kubernetes集群的工作节点，负责接收来自Master的工作指令并根据指令相应的创建或删除Pod对象，以及调整网络规则以合理地路由和转发流量等。理论上讲，Node可以是任何形式的计算设备，不过Master会统一将其抽象为Node对象进行管理。Node类似于蜂群中的工蜂，生产环境中，它们通常数量众多。 Kubernetes将所有Node的资源集结于一处形成一台更强大的“服务器”，如下图，在用户将应用部署于其上时，Master会使用调度算法将其自动指派某个特定的Node运行，在Node加入集群或从集群中移除时，Master也会按需重行编排影响到的Pod（容器）。于是，用户无需关心其应用究竟运行于何处。 Kubernetes系统基础/cluster.png) 从抽象的角度来讲，Kubernetes还有着众多的组件来支撑其内部的业务逻辑，包括运行应用、应用编排、服务暴露、应用恢复等，它们在Kubernetes中被抽象为Pod、Service、Controller等资源类型。 常用的资源对象十分钟带你理解Kubernetes核心概念 （1）Pod Kubernetes并不直接运行容器，而是使用一个抽象的资源对象来封装一个或者多个容器，这个抽象即为Pod，它是Kubernetes的最小调度单元。同一Pod中的容器共享网络名称空间和存储资源，这些容器可经由本地回环接口lo直接通信，但彼此之间又在Mount、User及PID等名称空间上保持了隔离。尽管Pod中可以包含多个容器，但是作为最小调度单元，它应该尽可能地保持“小”，即通常只应该包含一个主容器，以及必要的辅助型容器（sidecar） Kubernetes系统基础/pod.png) （2）资源标签 标签（Label）是将资源进行分类的标识符，资源标签其实就是一个键值型（key/values）数据。标签旨在指定对象（如Pod等）辨识性的属性，这些属性仅对用户存在特定的意义，对Kubernetes集群来说并不直接表达核心系统语意。标签可以在对象创建时附加其上，并能够在创建后的任意时间进行添加和修改。一个对象可以拥有多个标签，一个标签也可以附加于多个对象（通常是同一类对象）之上。 Kubernetes系统基础/label.png) （3）标签选择器 标签选择器（Selector）全称为”Label Selector“，它是一种根据Label来过滤符合条件的资源对象的机制。例如，将附有标签”role：backend“的所有Pod对象挑选出来归为一组就是标签选择器的一种应用，如下图所示，通常使用标签对资源对象进行分类，而后使用标签选择器挑选出它们，例如将其创建未某Service的端点。 Kubernetes系统基础/labels.png) （4）Pod控制器 尽管Pod是kubernetes的最小调度单元，但用户通常并不会直接部署及管理Pod对象，而是要借助于另一类抽象——控制器（Controller）对其进行管理。用于工作负载的控制器是一种管理Pod生命周期的资源抽象，它们是kubernetes上的一类对象，而非单个资源对象，包括ReplicationController，ReplicaSet、Deployment、StatefulSet、Job等。已下图所示的Deployment控制器为例，它负责确保指定的Pod对象的副本数量精确符合定义，否则“多退少补”。使用控制器之后就不再需要手动管理Pod对象了，只需要声明应用的期望状态，控制器就会自动对其进行进程管理。 Kubernetes系统基础/deployment.png) （5）服务资源（Service） Service是建立在一组Pod对象之上的资源抽象，它通过标签选择器选定一组Pod对象，并为这组Pod对象定义一个统一的固定访问入口（通常是一个IP地址），若Kubernetes集群存在DNS附件，它就会在Service创建时为其自动配置一个DNS名称以便客户端进行服务发现。到达Service IP的请求将被负载均衡至其后的端点——各个Pod对象之上，因此Service从本质上来讲是一个四层代理服务。另外，service还可以将集群外部流量引入到集群中来。 （6）存储卷 存储卷（Volume）是独立于容器文件系统之外的存储空间，常用于扩展容器的存储空间并为它提供持久存储能力。Kubernetes集群上的存储卷大体可以分为临时卷、本地卷和网络卷。临时卷和本地卷都位于Node本地，一旦Pod被调度至其他Node，此种类型的存储卷将无法访问到，因此临时卷和本地卷通常用于数据缓存，持久化的数据则需要放置于持久卷（persistent volume）之上。 （7）Name和Namespace 名称（Name）是Kubernetes集群中资源对象的标识符，它们的作用域通常是名称空间（Namespace），因此名称空间是名称的额外的限定机制。在同一名称空间中，同一类型资源对象的名称必须具有唯一性。名称空间通常用于实现租户或项目的资源隔离，从而形成逻辑分组，如下图所示，创建的Pod和Service等资源对象都属于名称空间级别，未指定时，他们都属于默认的名称空间“default”。 Kubernetes系统基础/namespace.png) （8）Annotation Annotation（注释）是另一种附加在对象之上的键值类型的数据，但它拥有更大的数据容量。Annotation常用于将各种非标识型元数据（metadata）附加到对象上，但它不能用于标识和选择对象，通常也不会被Kubernetes直接使用，其主要目的是方便工具或用户的阅读和查找等。 （9）Ingress Kubernetes将Pod对象和外部网络环境进行了隔离，Pod和Service等对象间的通信都使用其内部专用地址进行，如若需要开放某些Pod对象提供给外部用户访问，则需要为其请求流量打开一个通往Kubernetes集群内部的通道，除了Service之外，Ingress也是这类通道的实现方式之一。 Kubernetes集群组件Kubernetes系统基础/system.png) Master组件 Kubernetes的集群控制平面由多个组件组成，这些组件可统一运行于单一Master节点，也可以以多副本的方式同时运行于多个节点，以为Master提供高可用功能，甚至还可以运行于Kubernetes集群自身之上。Master主要包括以下几个组件。 （1）API Server Api Server负责输出RESTful风格的Kubernetes API，它是发往集群的所有REST操作命令的接入点，并负责接收、校验并响应所有的REST请求，结果状态被持久存储于etcd中。因此，API Server是整个集群的网关。 （2）ETCD Kubernetes集群的所有状态信息都需要持久存储于存储系统etcd中，不过，etcd是由CoreOS基于Raft协议开发的分布式键值存储，可用于服务发现、共享配置以及一致性保障（例如数据库主节点选择、分布式锁等）。因此，etcd是独立的服务组件，并不隶属于Kubernetes集群自身。生产环境中应该以etcd集群的方式运行以确保其服务可用性。 etcd不仅能够提供键值数据存储，而且还为其提供了监听（watch）机制，用于监听和推送变更。Kubernetes集群系统中，etcd中的键值发生变化时会通知到API Server，并由其通过watch API向客户端输出。基于watch机制，Kubernetes集群的各组件实现了高效协同。 （3）Controller Manager Kubernetes中，集群级别的大多数功能都是由几个被称为控制器的进程执行实现的，这几个进程被集成与kube-controller-manager守护进程中。由控制器完成的功能主要包括生命周期功能和API业务逻辑，具体如下 生命周期功能：包括Namespace创建和生命周期、Event垃圾回收、Pod终止相关的垃圾回收、级联垃圾回收及Node垃圾回收等。 API业务逻辑：例如，由ReplicaSet执行的Pod扩展等。 （4）Scheduler Kubernetes是用于部署和管理大规模容器应用的平台，根据集群规模的不同，其托管运行的容器很可能会数以千计甚至更多。API Server确认Pod对象的创建请求之后，便需要由Scheduler根据集群内各节点的可用资源状态，以及要运行的容器的资源需求做出调度决策，如下图所示。另外，Kubernetes还支持用户自定义调度器。 Kubernetes系统基础/scheduler.png) Node组件 Node负责提供运行容器的各种依赖环境，并接收Master的管理。每个Node主要由以下几个组件构成。 （1）kubelet kubelet是运行于工作节点之上的守护进程，它从API Server接收关于Pod对象的配置信息并确保它们处于期望的状态（desired state，也可以说目标状态）kubelet会在API Server上注册当前工作节点，定期向Master汇报节点资源使用情况，并通过cAdvisor监控容器和节点的资源占用情况。 （2）kube-proxy 每个工作节点都需要运行一个kube-proxy守护进程，它能够按需为Service资源对象生成iptables或ipvs规则，从而捕获访问当前Service的ClusterIP的流量并将其转发至正确的后端Pod对象。 （3）docker docker用于运行容器 核心附件 Kubernetes集群还依赖于一组称为”附件”(add-ons)的组件以提供完整的功能，它们通常是由第三方提供的特定应用程序，且托管运行于Kubernetes集群之上，如上图所示。 KubeDNS 在Kubernetes集群中调度运行提供DNS服务的Pod，同一集群中的其他pod可使用此DNS服务解决主机名。Kubernetes从1.11版本开始默认使用CoreDNS项目为集群提供服务注册和服务发现的动态名称解析服务，之前的版本中用到的是kube-dns项目，而SKyDNS则是更早一代的项目。 Kubernetes Dashboard Kubernetes集群的全部功能都要基于Web的UI，来管理集群中应用甚至是集群自身。 Heapster 容器和节点的性能监控与分析系统，它收集并解析多种指标数据，如资源利用率、生命周期事件等。新版本的Kubernetes中，其功能会逐渐由Prometheus结合其他组件所取代。 Ingress Controller Service是一种工作于传输层的负载均衡器，而Ingress是在应用层实现的HTTP(s)负载均衡机制。不过，Ingress资源自身不能进行“流量穿透”，它仅是一组路由规则的集合，这些规则需要通过Ingress控制器（Ingress Controller）发挥作用。目前，此类的可用项目有Nginx、Traefik、Envoy及HAProxy等。]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络RAID技术DRBD]]></title>
    <url>%2F2019%2F08%2F08%2F%E7%BD%91%E7%BB%9CRAID%E6%8A%80%E6%9C%AFDRBD%2F</url>
    <content type="text"><![CDATA[DRBD简介官方文档 DRBD的全称为：Distributed Replicated Block Device(DRBD)分布式块设备复制，DRBD是由内核模块和相关脚本构成，用以构建高可用的集群。其实现方式是通过网络来镜像整个设备。可以把它看作是一种网络RAID。它允许用户在远程机器上建立一个本地块设备的实时镜像。 DRBD是如何工作的 （DRBD Primary）负责接收数据，把数据写到本地磁盘并发送给另一台主机（DRBD Secondary）。另一个主机再将数据存到自己的磁盘中。目前，DRBD每次只允许对一个节点进行读写访问，但这对于通常的故障切换高可用集群来说已经足够用了，有可能以后的版本支持两个节点进行读写存取。DRBD协议说明1）数据一旦写入磁盘并发送到网络中就认为完成了写入操作。2）收到接收确认就认为完成了写入操作。3）收到写入确认就认为完成了写入操作。 DRBD与HA的关系 一个DRBD系统由两个节点构成，与HA集群类似，也有主节点和备用节点之分，在带有主要设备的节点上，应用程序和操作系统可以运行和访问DRBD设备（/dev/drbd*）。在主节点写入的数据通过DRBD设备存储到主节点的设备写入到备用节点的磁盘中。现在大部分的高可用性集群都会使用共享存储，而DRBD也可以作为一个共享存储设备，使用DRBD不需要太多的硬件的设备。因为它在TCP/IP网络中运行，所以，利用DRBD作为共享存储设备，节约很多成本，因为价格要比专用的存储网络便宜很多；其性能与稳定性方面也不错。 DRBD工作原理 DRBD是linux的内核的存储层中的一个分布式存储系统，可用使用DRBD在两台Linux服务器之间共享块设备，共享文件系统和数据。类似网络RAID-1，其工作的原理架构图如下： DRBD的特性 分布式复制块设备（DRBD技术）是一种基于软件的，无共享，复制的存储解决方案，在服务器之间的对块设备（硬盘、分区、逻辑卷等）进行镜像。DRBD镜像数据的特性：1）实时性：当某个应用程序完成对数据的修改时，复制功能立即发生2）透明性：应用程序的数据存储在镜像块设备上是独立透明的，他们的数据在两个节点上都保存一份，因此，无论哪一台服务器宕机，都不会影响应用程序读取数据的操作，所以说是透明的。3）同步镜像和异步镜像：同步镜像表示当应用程序提交本地的写操作后，数据会同步写到两个节点上去；异步镜像表示当应用程序提交写操作后，只有当本地的节点上完成写操作后，另一个节点才可以完成写操作。 DRBD的模式 DRBD共有2种模式，一种是DRBD的主从模式，另一种是DRBD的双主模式1）DRBD的主从模式： 这种模式下，其中一个节点作为主节点，另一个节点作为从节点。其中主节点可以执行读、写操作；从节点不可以挂载文件系统，因此也不可以执行读写操作。在这种模式下，资源在任何时间只能存储在主节点上。这种模式可用在任何的文件系统（ext3、ext4、xfs）等。默认这种模式下，一旦主节点发生故障，从节点需要手工将资源进行转移，且主节点变成从节点和从节点变成主节点需要手动进行切换。不能自动进行转移，因此比较麻烦。为了解决手动将资源和节点进行转移，可以将DRBD做成高可用集群的资源代理（RA），这样一旦其中的一个节点宕机，资源就会自动转移到另一个节点，从而保证服务的连续性。2）DRBD的双主模式:这是DRBD8.0之后的新特性，在双主模式下，任何资源在任何特定的时间都存在两个主节点。这种模式需要一个共享的集群文件系统，利用分布式的锁机制进行管理，如GFS和OCFS2。部署双主模式时，DRBD可以是负载均衡的集群，这就需要从两个并发的主节点中选取一个首选的访问数据。这种模式默认是禁用的，如果要是用的话必须在配置文件中进行申明。 DRBD的复制模式 DRBD的复制功能就是将应用程序提交的数据一份保存在本地节点，一份复制传输保存在另一份节点上。但是DRBD需要对传输的数据进行确认以便保证另一个节点的写操作完成，就需要用到DRBD的复制模式，DRBD有三种复制模式： 1）协议A：异步复制协议 一旦本地磁盘写入已经完成，数据包已在发送队列中，则写被认为是完成的。在一个节点上发生故障时，可能发生数据丢失，因为被写入到远程节点上的数据可能仍在发送队列。尽管，在故障转移节点上的数据是一致的，但没有及时更新。这通常是用于地理上分开的节点。 2）协议B：内存同步（半同步）复制协议 一旦本地磁盘写入已完成且复制数据包达到了对等节点则认为写在主节点上被认为是完成的。数据丢失可能发生在参加的两个节点同时故障的情况下，因为在传输中的数据可能不会被提交到磁盘。 3）协议C：同步复制协议 只有在本地和远程节点的磁盘已经确认了写操作完成，写才被认为完成。没有任何数据丢失，所以这是一个集群节点的流行模式，但I/O吞吐量依赖于网络带宽。一般使用协议C，但选择C协议将影响流量，从而影响网络延迟。为了数据可靠性，我们在生产环境使用时须慎重选择使用哪一种协议。 DRBD配置文件说明123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100DRBD的主配置文件为/etc/drbd.conf；为了管理的便捷性，目前通常会将配置文件分成多个部分，且都保存至/etc/drbd.d目录中，主配置文件中仅使用"include"指令将这些配置文件片断整合起来。通常，/etc/drbd.d目录中的配置文件为global_common.conf和所有以.res结尾的文件。其中global_common.conf中主要定义global段和common段，而每一个.res的文件用于定义一个资源。 在配置文件中，global段仅能出现一次，且如果所有的配置信息都保存至同一个配置文件中而不分开为多个文件的话，global段必须位于配置文件的最开始处。目前global段中可以定义的参数仅有minor-count, dialog-refresh, disable-ip-verification和usage-count。 common段则用于定义被每一个资源默认继承的参数，可以在资源定义中使用的参数都可以在common段中定义。实际应用中，common段并非必须，但建议将多个资源共享的参数定义为common段中的参数以降低配置文件的复杂度。 resource段则用于定义DRBD资源，每个资源通常定义在一个单独的位于/etc/drbd.d目录中的以.res结尾的文件中。资源在定义时必须为其命名，名字可以由非空白的ASCII字符组成。每一个资源段的定义中至少要包含两个host子段，以定义此资源关联至的节点，其它参数均可以从common段或DRBD的默认中进行继承而无须定义。 DRBD配置文件[root@drbd1 ~]# cat /etc/drbd.d/global_common.confglobal &#123; usage-count yes; //是否参加DRBD使用者统计，默认是参加 # minor-count dialog-refresh disable-ip-verification //这里是global可以使用的参数 #minor-count：32 //从（设备）个数，取值范围1~255，默认值为32。该选项设定了允许定义的resource个数，当要定义的resource超过了此选项的设定时，需要重新载入DRBD内核模块。 #disable-ip-verification：no //是否禁用ip检查 &#125;common &#123; protocol C; //指定复制协议,复制协议共有三种，为协议A，B，C，默认协议为协议C handlers &#123; //该配置段用来定义一系列处理器，用来回应特定事件。 # These are EXAMPLE handlers only. # They may have severe implications, # like hard resetting the node under certain circumstances. # Be careful when chosing your poison. # pri-on-incon-degr "/usr/lib/DRBD/notify-pri-on-incon-degr.sh; /usr/lib/DRBD/notify-emergency-reboot.sh; echo b &gt; /proc/sysrq-trigger ; reboot -f"; # pri-lost-after-sb "/usr/lib/DRBD/notify-pri-lost-after-sb.sh; /usr/lib/DRBD/notify-emergency-reboot.sh; echo b &gt; /proc/sysrq-trigger ; reboot -f"; # local-io-error "/usr/lib/DRBD/notify-io-error.sh; /usr/lib/DRBD/notify-emergency-shutdown.sh; echo o &gt; /proc/sysrq-trigger ; halt -f"; # fence-peer "/usr/lib/DRBD/crm-fence-peer.sh"; # split-brain "/usr/lib/DRBD/notify-split-brain.sh root"; # out-of-sync "/usr/lib/DRBD/notify-out-of-sync.sh root"; # before-resync-target "/usr/lib/DRBD/snapshot-resync-target-lvm.sh -p 15 -- -c 16k"; # after-resync-target /usr/lib/DRBD/unsnapshot-resync-target-lvm.sh; &#125; startup &#123; //#DRBD同步时使用的验证方式和密码。该配置段用来更加精细地调节DRBD属性，它作用于配置节点在启动或重启时。常用选项有： # wfc-timeout degr-wfc-timeout outdated-wfc-timeout wait-after-sb wfc-timeout： //该选项设定一个时间值，单位是秒。在启用DRBD块时，初始化脚本DRBD会阻塞启动进程的运行，直到对等节点的出现。该选项就是用来限制这个等待时间的，默认为0，即不限制，永远等待。 degr-wfc-timeout： //该选项也设定一个时间值，单位为秒。也是用于限制等待时间，只是作用的情形不同：它作用于一个降级集群（即那些只剩下一个节点的集群）在重启时的等待时间。 outdated-wfc-timeout： //同上，也是用来设定等待时间，单位为秒。它用于设定等待过期节点的时间 &#125; disk &#123; # on-io-error fencing use-bmbv no-disk-barrier no-disk-flushes //这里是disk段内可以定义的参数 # no-disk-drain no-md-flushes max-bio-bvecs //这里是disk段内可以定义的参数 on-io-error： detach //选项：此选项设定了一个策略，如果底层设备向上层设备报告发生I/O错误，将按照该策略进行处理。有效的策略包括： detach //发生I/O错误的节点将放弃底层设备，以diskless mode继续工作。在diskless mode下，只要还有网络连接，DRBD将从secondary node读写数据，而不需要failover（故障转移）。该策略会导致一定的损失，但好处也很明显，DRBD服务不会中断。官方推荐和默认策略。 pass_on //把I/O错误报告给上层设备。如果错误发生在primary节点，把它报告给文件系统，由上层设备处理这些错误（例如，它会导致文件系统以只读方式重新挂载），它可能会导致DRBD停止提供服务；如果发生在secondary节点，则忽略该错误（因为secondary节点没有上层设备可以报告）。该策略曾经是默认策略，但现在已被detach所取代。 call-local-io-error //调用预定义的本地local-io-error脚本进行处理。该策略需要在resource（或common）配置段的handlers部分，预定义一个相应的local-io-error命令调用。该策略完全由管理员通过local-io-error命令（或脚本）调用来控制如何处理I/O错误。 fencing： //该选项设定一个策略来避免split brain的状况。有效的策略包括： dont-care： //默认策略。不采取任何隔离措施。 resource-only： //在此策略下，如果一个节点处于split brain状态，它将尝试隔离对端节点的磁盘。这个操作通过调用fence-peer处理器来实现。fence-peer处理器将通过其它通信路径到达对等节点，并在这个对等节点上调用DRBDadm outdate res命令 resource-and-stonith： //在此策略下，如果一个节点处于split brain状态，它将停止I/O操作，并调用fence-peer处理器。处理器通过其它通信路径到达对等节点，并在这个对等节点上调用DRBDadm outdate res命令。如果无法到达对等节点，它将向对等端发送关机命令。一旦问题解决，I/O操作将重新进行。如果处理器失败，你可以使用resume-io命令来重新开始I/O操作。 &#125; net &#123; //该配置段用来精细地调节DRBD的属性，网络相关的属性。常用的选项有： # sndbuf-size rcvbuf-size timeout connect-int ping-int ping-timeout max-buffers //这里是net段内可以定义的参数 # max-epoch-size ko-count allow-two-primaries cram-hmac-alg shared-secret //这里是net段内可以定义的参数 # after-sb-0pri after-sb-1pri after-sb-2pri data-integrity-alg no-tcp-cork //这里是net段内可以定义的参数 sndbuf-size： //该选项用来调节TCP send buffer的大小，DRBD 8.2.7以前的版本，默认值为0，意味着自动调节大小；新版本的DRBD的默认值为128KiB。高吞吐量的网络（例如专用的千兆网卡，或负载均衡中绑定的连接）中，增加到512K比较合适，或者可以更高，但是最好不要超过2M。 timeout： //该选项设定一个时间值，单位为0.1秒。如果搭档节点没有在此时间内发来应答包，那么就认为搭档节点已经死亡，因此将断开这次TCP/IP连接。默认值为60，即6秒。该选项的值必须小于connect-int和ping-int的值。 connect-int： //如果无法立即连接上远程DRBD设备，系统将断续尝试连接。该选项设定的就是两次尝试间隔时间。单位为秒，默认值为10秒。 ping-timeout： //该选项设定一个时间值，单位是0.1秒。如果对端节点没有在此时间内应答keep-alive包，它将被认为已经死亡。默认值是500ms。 max-buffers： //该选项设定一个由DRBD分配的最大请求数，单位是页面大小（PAGE_SIZE），大多数系统中，页面大小为4KB。这些buffer用来存储那些即将写入磁盘的数据。最小值为32（即128KB）。这个值大一点好。 max-epoch-size： //该选项设定了两次write barriers之间最大的数据块数。如果选项的值小于10，将影响系统性能。大一点好 ko-count： //该选项设定一个值，把该选项设定的值 乘以 timeout设定的值，得到一个数字N，如果secondary节点没有在此时间内完成单次写请求，它将从集群中被移除（即，primary node进入StandAlong模式）。取值范围0~200，默认值为0，即禁用该功能。 allow-two-primaries： //这个是DRBD8.0及以后版本才支持的新特性，允许一个集群中有两个primary node。该模式需要特定文件系统的支撑，目前只有OCFS2和GFS可以，传统的ext3、ext4、xfs等都不行！ cram-hmac-alg： //该选项可以用来指定HMAC算法来启用对端节点授权。DRBD强烈建议启用对端点授权机制。可以指定/proc/crypto文件中识别的任一算法。必须在此指定算法，以明确启用对端节点授权机制，实现数据加密传输。 shared-secret： //该选项用来设定在对端节点授权中使用的密码，最长64个字符。 data-integrity-alg： //该选项设定内核支持的一个算法，用于网络上的用户数据的一致性校验。通常的数据一致性校验，由TCP/IP头中所包含的16位校验和来进行，而该选项可以使用内核所支持的任一算法。该功能默认关闭。 &#125; syncer &#123; //该配置段用来更加精细地调节服务的同步进程。常用选项有 # rate after al-extents use-rle cpu-mask verify-alg csums-alg rate： //设置同步时的速率，默认为250KB。默认的单位是KB/sec，也允许使用K、M和G，如40M。注意：syncer中的速率是以bytes，而不是bits来设定的。配置文件中的这个选项设置的速率是永久性的，但可使用下列命令临时地改变rate的值：DRBDsetup /dev/DRBDN syncer -r 100M。如果想重新恢复成drbd.conf配置文件中设定的速率，执行如下命令： DRBDadm adjust resource verify-alg： //该选项指定一个用于在线校验的算法，内核一般都会支持md5、sha1和crc32c校验算法。在线校验默认关闭，必须在此选项设定参数，以明确启用在线设备校验。DRBD支持在线设备校验，它以一种高效的方式对不同节点的数据进行一致性校验。在线校验会影响CPU负载和使用，但影响比较轻微。DRBD 8.2.5及以后版本支持此功能。一旦启用了该功能，你就可以使用下列命令进行一个在线校验： DRBDadm verify resource。该命令对指定的resource进行检验，如果检测到有数据块没有同步，它会标记这些块，并往内核日志中写入一条信息。这个过程不会影响正在使用该设备的程序。 如果检测到未同步的块，当检验结束后，你就可以如下命令重新同步它们：DRBDadm disconnect resource or DRBDadm connetc resource &#125;&#125; common段是用来定义共享的资源参数，以减少资源定义的重复性。common段是非必须的。resource段一般为DRBD上每一个节点来定义其资源参数的。资源配置文件详解 [root@drbd1 ~]# cat /etc/drbd.d/web.resresource web &#123; //web为资源名称 on ha1.xsl.com &#123; //on后面为节点的名称,有几个节点就有几个on段，这里是定义节点ha1.xsl.com上的资源 device /dev/DRBD0; //定义DRBD虚拟块设备，这个设备事先不要格式化。 disk /dev/sda6; //定义存储磁盘为/dev/sda6,该分区创建完成之后就行了，不要进行格式化操作 address 192.168.108.199:7789; //定义DRBD监听的地址和端口，以便和对端进行通信 meta-disk internal; //该参数有2个选项：internal和externally，其中internal表示将元数据和数据存储在同一个磁盘上；而externally表示将元数据和数据分开存储，元数据被放在另一个磁盘上。 &#125; on ha2.xsl.com &#123; //这里是定义节点ha2.xsl.com上的资源 device /dev/DRBD0; disk /dev/sda6; address 192.168.108.201:7789; meta-disk internal; &#125;&#125; DRBD安装配置（主从模式）环境说明 这里使用所用环境为centos7.4操作系统在下面的操作步骤中# command的命令表示主从节点服务器都需要执行。 IP 主机名 角色说明 192.168.1.31 drbd1.cluster.com 主服务器 192.168.1.32 drbd2.cluster.com 从服务器 具体步骤1）两台机器上添加DRBD磁盘，进行分区，不做格式化，并在本地系统创建/data目录，不做挂载操作12345678910111213在drbd1服务器上操作[root@drbd1 ~]# lsblk ......sdb 8:16 0 10G 0 disk[root@drbd1 ~]# fdisk /dev/sdb依次输入"n-&gt;p-&gt;1-&gt;回车-&gt;回车-w"在drbd2服务器上操作[root@drbd2 ~]# lsblk ......sdb 8:16 0 10G 0 disk[root@drbd2 ~]# fdisk /dev/sdb依次输入"n-&gt;p-&gt;1-&gt;回车-&gt;回车-w" 2）两台服务器之间的防火墙要允许互相访问，这里测试关闭selinux和防火墙(两台服务器同样操作)12# systemctl stop firewalld# setenforce 0 //临时性关闭；永久关闭需修改/etc/sysconfig/selinux的SELINUX为disable 3）hosts绑定（两台服务器同样操作）123# vim /etc/hosts192.168.1.31 drbd1.cluster.com192.168.1.32 drbd2.cluster.com 4）两台服务器进行时间同步（两台服务器同样操作）12# yum -y install ntpdate# ntpdate -u asia.pool.ntp.org 5）DRBD安装配置（两台服务器同样操作）12345678910111213这里使用yum方式安装# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org# yum -y install https://www.elrepo.org/elrepo-release-7.0-4.el7.elrepo.noarch.rpm# yum -y install kmod-drbd84 drbd86-utils 因为在执行yum -y install kmod-drbd84 drbd86-utils时，对内核kernel进行了update，要重新启动服务器，更新后的内核才会生效。加载模块：# reboot# modprobe drbd查看模块是否已添加上# lsmod |grep drbddrbd 397041 0 libcrc32c 12644 4 xfs,drbd,nf_nat,nf_conntrack 6）DRBD配置（两台服务器上同样操作）12345678910111213141516171819202122232425262728293031323334353637383940414243444546# cat /etc/drbd.conf //主配置文件# You can find an example in /usr/share/doc/drbd.../drbd.conf.exampleinclude "drbd.d/global_common.conf";include "drbd.d/*.res";备份配置文件# cp /etc/drbd.d/global_common.conf&#123;,.bak&#125; # vim /etc/drbd.d/global_common.conf //全局配置文件global &#123; usage-count no; #是否参加DRBD使用统计，默认为yes。官方统计drbd的装机量&#125;common &#123; protocol C; #使用DRBD的同步协议 handlers &#123; &#125; startup &#123; &#125; options &#123; &#125; disk &#123; on-io-error detach; #配置I/O错误处理策略为分离 &#125; net &#123; &#125; syncer &#123; rate 30M; #设置主备节点同步时的网络速率 &#125;&#125;# vim /etc/drbd.d/r0.res #创建资源resource r0 &#123; on drbd1.cluster.com &#123; device /dev/drbd0; #drbd1服务器上的DRBD虚拟块设备，事先不要格式化 disk /dev/sdb1; address 192.168.1.31:7789; #DRBD监听的地址和端口，端口可以自定义。 meta-disk internal; &#125; on drbd2.cluster.com &#123; device /dev/drbd0; #drbd2服务器上的DRBD虚拟块设备，事先不要格式化 disk /dev/sdb1; address 192.168.1.32:7789; meta-disk internal; &#125;&#125; 7）在两台服务器上分别启用r0资源（两台服务器上同样操作）123456789101112131415161718# drbdadm create-md r0initializing activity loginitializing bitmap (320 KB) to all zeroWriting meta data...New drbd meta data block successfully created.# 启动drbd(注意：需要两边服务器同时启动方能生效)[root@drbd1 ~]# systemctl start drbd[root@drbd2 ~]# systemctl start drbd# 查看状态（两台机器上都执行查看）# cat /proc/drbdversion: 8.4.11-1 (api:1/proto:86-101)GIT-hash: 66145a308421e9c124ec391a7848ac20203bb03c build by mockbuild@, 2018-11-03 01:26:55 0: cs:Connected ro:Secondary/Secondary ds:Inconsistent/Inconsistent C r----- ns:0 nr:0 dw:0 dr:0 al:8 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:10484380#由上面两台主机的DRBD状态查看结果里的ro:Secondary/Secondary表示两台主机的状态都是备机状态，ds是磁盘状态， 8）将drbd1服务器配置为DRBD的主节点，进行初始化设备同步1234567891011121314151617181920212223[root@drbd1 ~]# drbdsetup /dev/drbd0 primary --force[root@drbd1 ~]# cat /proc/drbd version: 8.4.11-1 (api:1/proto:86-101)GIT-hash: 66145a308421e9c124ec391a7848ac20203bb03c build by mockbuild@, 2018-11-03 01:26:55 0: cs:SyncSource ro:Primary/Secondary ds:UpToDate/Inconsistent C r----- ns:2418688 nr:0 dw:0 dr:2420808 al:8 bm:0 lo:0 pe:1 ua:0 ap:0 ep:1 wo:f oos:8066716 [===&gt;................] sync'ed: 23.1% (7876/10236)M finish: 0:03:08 speed: 42,792 (37,776) K/sec###同步完成时状态如下[root@drbd1 ~]# cat /proc/drbd #drbd1上查看version: 8.4.11-1 (api:1/proto:86-101)GIT-hash: 66145a308421e9c124ec391a7848ac20203bb03c build by mockbuild@, 2018-11-03 01:26:55 0: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r----- ns:10484380 nr:0 dw:0 dr:10486500 al:8 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0[root@drbd2 ~]# cat /proc/drbd #drbd2上查看version: 8.4.11-1 (api:1/proto:86-101)GIT-hash: 66145a308421e9c124ec391a7848ac20203bb03c build by mockbuild@, 2018-11-03 01:26:55 0: cs:Connected ro:Secondary/Primary ds:UpToDate/UpToDate C r----- ns:0 nr:10484380 dw:10484380 dr:0 al:8 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0#说明：ro在主从服务器上分别显示 Primary/Secondary和Secondary/Primaryds显示UpToDate/UpToDate 表示主从配置成功 9）挂载DRBD（drbd1主节点服务器上操作）1234567891011121314151617181920#先格式化/dev/drbd0[root@drbd1 ~]# mkfs.ext4 /dev/drbd0#创建挂载目录，然后进行挂载[root@drbd1 ~]# mkdir /data[root@drbd1 ~]# mount /dev/drbd0 /data[root@drbd1 ~]# df -h文件系统 容量 已用 可用 已用% 挂载点/dev/mapper/centos-root 20G 3.7G 16G 20% /devtmpfs 471M 0 471M 0% /devtmpfs 487M 0 487M 0% /dev/shmtmpfs 487M 7.1M 480M 2% /runtmpfs 487M 0 487M 0% /sys/fs/cgroup/dev/sda1 497M 193M 305M 39% /boottmpfs 98M 0 98M 0% /run/user/0/dev/drbd0 9.8G 37M 9.2G 1% /data特别注意：从节点(Secondary)上不允许对DRBD设备进行任何操作，包括只读，所有的读写操作只能在主节点(Primary)上进行。只有当主节点(Primary)挂掉时，从节点(Secondary)才能提示为主节点(Primary)。 DRBD主备故障切换测试 模拟主节点(drbd1.cluster.com）发生故障，从节点（drbd2.cluster.com）接管并提升为主节点。 1）主节点上操作1234567891011121314# 创建测试文件并取消挂载[root@drbd1 ~]# cd /data/[root@drbd1 data]# touch test01 test02 test03[root@drbd1 data]# cd [root@drbd1 ~]# umount /data[root@drbd1 ~]# drbdsetup /dev/drbd0 secondary //将主节点设置为DRBD的备节点。在实际生产环境中，直接在(Secondary）节点上提权（即设置为主节点）即可。[root@drbd1 ~]# cat /proc/drbd version: 8.4.11-1 (api:1/proto:86-101)GIT-hash: 66145a308421e9c124ec391a7848ac20203bb03c build by mockbuild@, 2018-11-03 01:26:55 0: cs:Connected ro:Secondary/Secondary ds:UpToDate/UpToDate C r----- ns:10783808 nr:0 dw:299428 dr:10488701 al:82 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0注意：这里实际生产环境若Primary主节点宕机，在Secondary状态信息中ro的值会显示为Secondary/Unknown,只需要进行DRBD提权操作即可。 2）备节点上操作123456789101112131415161718192021222324[root@drbd2 ~]# drbdsetup /dev/drbd0 primary[root@drbd2 ~]# cat /proc/drbd version: 8.4.11-1 (api:1/proto:86-101)GIT-hash: 66145a308421e9c124ec391a7848ac20203bb03c build by mockbuild@, 2018-11-03 01:26:55 0: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r----- ns:0 nr:10783808 dw:10783808 dr:2120 al:8 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0#挂载验证数据[root@drbd2 ~]# mkdir /data[root@drbd2 ~]# mount /dev/drbd0 /data[root@drbd2 ~]# df -h文件系统 容量 已用 可用 已用% 挂载点/dev/mapper/centos-root 20G 3.7G 16G 20% /devtmpfs 471M 0 471M 0% /devtmpfs 487M 0 487M 0% /dev/shmtmpfs 487M 7.1M 480M 2% /runtmpfs 487M 0 487M 0% /sys/fs/cgroup/dev/sda1 497M 193M 305M 39% /boottmpfs 98M 0 98M 0% /run/user/0/dev/drbd0 9.8G 37M 9.2G 1% /data[root@drbd2 ~]# cd /data/[root@drbd2 data]# lstest01 test02 test03 到此，DRBD的主从环境的部署工作已经完成。不过上面是记录的是主备手动切换，至于保证DRBD主从结构的智能切换，实现高可用，还需里用到Keepalived或Heartbeat来实现了（会在DRBD主端挂掉的情况下，自动切换从端为主端并自动挂载/data分区） 相关配置操作资源的连接状态详细介绍如何查看资源连接状态？12[root@drbd1 ~]# drbdadm cstate r0 #r0为资源名称Connected 资源的连接状态；一个资源可能有一下连接状态的一种12345678910111213141516171819202122StandAlone 独立的：网络配置不可用；资源还没有被连接或是被管理断开（使用 drbdadm disconnect 命令），或是由于出现认证失败或是脑裂的情况 Disconnecting 断开：断开只是临时状态，下一个状态是StandAlone独立的 modprobe drbdUnconnected 悬空：是尝试连接前的临时状态，可能下一个状态为WFconnection和WFReportParams Timeout 超时：与对等节点连接超时，也是临时状态，下一个状态为Unconected悬空 BrokerPipe：与对等节点连接丢失，也是临时状态，下一个状态为Unconected悬空 NetworkFailure：与对等节点推动连接后的临时状态，下一个状态为Unconected悬空 ProtocolError：与对等节点推动连接后的临时状态，下一个状态为Unconected悬空 TearDown 拆解：临时状态，对等节点关闭，下一个状态为Unconected悬空 WFConnection：等待和对等节点建立网络连接 WFReportParams：已经建立TCP连接，本节点等待从对等节点传来的第一个网络包 Connected 连接：DRBD已经建立连接，数据镜像现在可用，节点处于正常状态 StartingSyncS：完全同步，有管理员发起的刚刚开始同步，未来可能的状态为SyncSource或PausedSyncS StartingSyncT：完全同步，有管理员发起的刚刚开始同步，下一状态为WFSyncUUID WFBitMapS：部分同步刚刚开始，下一步可能的状态为SyncSource或PausedSyncS WFBitMapT：部分同步刚刚开始，下一步可能的状态为WFSyncUUID WFSyncUUID：同步即将开始，下一步可能的状态为SyncTarget或PausedSyncT SyncSource：以本节点为同步源的同步正在进行 SyncTarget：以本节点为同步目标的同步正在进行 PausedSyncS：以本地节点是一个持续同步的源，但是目前同步已经暂停，可能是因为另外一个同步正在进行或是使用命令(drbdadm pause-sync)暂停了同步 PausedSyncT：以本地节点为持续同步的目标，但是目前同步已经暂停，这可以是因为另外一个同步正在进行或是使用命令(drbdadm pause-sync)暂停了同步 VerifyS：以本地节点为验证源的线上设备验证正在执行 VerifyT：以本地节点为验证目标的线上设备验证正在执行 资源角色查看资源角色命令123456789101112[root@drbd1 ~]# drbdadm role r0Secondary/Primary[root@drbd1 ~]# cat /proc/drbd version: 8.4.11-1 (api:1/proto:86-101)GIT-hash: 66145a308421e9c124ec391a7848ac20203bb03c build by mockbuild@, 2018-11-03 01:26:55 0: cs:Connected ro:Secondary/Primary ds:UpToDate/UpToDate C r----- ns:10783808 nr:24 dw:299452 dr:10488701 al:82 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0注释：Parimary 主：资源目前为主，并且可能正在被读取或写入，如果不是双主只会出现在两个节点中的其中一个节点上 Secondary 次：资源目前为次，正常接收对等节点的更新 Unknown 未知：资源角色目前未知，本地的资源不会出现这种状态 硬盘数据状态查看硬盘状态命令12[root@drbd1 ~]# drbdadm dstate r0UpToDate/UpToDate 本地和对等节点的硬盘有可能为下列状态之一：123456789Diskless 无盘：本地没有块设备分配给DRBD使用，这表示没有可用的设备，或者使用drbdadm命令手工分离或是底层的I/O错误导致自动分离 Attaching：读取无数据时候的瞬间状态 Failed 失败：本地块设备报告I/O错误的下一个状态，其下一个状态为Diskless无盘 Negotiating：在已经连接的DRBD设置进行Attach读取无数据前的瞬间状态 Inconsistent：数据是不一致的，在两个节点上（初始的完全同步前）这种状态出现后立即创建一个新的资源。此外，在同步期间（同步目标）在一个节点上出现这种状态 Outdated：数据资源是一致的，但是已经过时 DUnknown：当对等节点网络连接不可用时出现这种状态 Consistent：一个没有连接的节点数据一致，当建立连接时，它决定数据是UpToDate或是Outdated UpToDate：一致的最新的数据状态，这个状态为正常状态]]></content>
      <categories>
        <category>Storage</category>
      </categories>
      <tags>
        <tag>Storage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[存储服务之NFS]]></title>
    <url>%2F2019%2F08%2F08%2F%E5%AD%98%E5%82%A8%E6%9C%8D%E5%8A%A1%E4%B9%8BNFS%2F</url>
    <content type="text"><![CDATA[NFS介绍官方文档 NFS（Network File System）即网络文件系统，它最大的功能就是通过TCP/IP网络共享资源。在NFS的应用中，本地NFS的客户端应用可以透明地读写位于远端NFS服务器上的文件，就像访问本地文件一样。 NFS客户端一般是应用服务器（比如web，负载均衡等），可以通过挂载的方式将NFS服务器端共享的目录挂载到NFS客户端本地的目录下。 因为NFS支持的功能相当的多，而不同的功能都会使用不同的程序来启动，每启动一个功能就会启用一些端口来传输数据，因此，NFS的功能所对应的端口才没有固定住，而是随机取用一些未被使用的小于1024的端口来作为传输之用。但如此一来又造成了客户端想要连上服务器时的困扰，因为客户端得要知道服务器端的相关端口才能够进行连接。 因此就需要远程过程调用(RPC)的服务，RPC最主要的功能就是在指定每个NFS功能所对应的port number，并且回报给客户端，让客户端可以连接正确的端口上去。那RPC又是如何知道每个NFS的端口呢？这是因为当服务器在启动NFS时会随机取用数个端口，并主动的想RPC注册，因此RPC可以知道每个端口对应的NFS功能，然后RPC又是固定使用port 111来监听客户端的需求并回报给客户端正确的端口，所以当然可以让NFS的启动更为轻松愉快了。 NFS在文件传送过程中依赖与RPC（远程过程调用）协议。NFS本身是没有提供信息传送的协议和功能的，但是能够用过网络进行图片，视频，附件等分享功能。只要用到NFS的地方都需要启动RPC服务，不论是NFS的服务端还是客户端。 NFS和RPC的关系：可以理解为NFS是一个网络文件系统（比喻为租房的房主），而RPC是负责信息的传输（中介），客户端（相当于租房的租客）。 NFS网络文件系统存在的意义实现数据共享，数据保持一致。如图所示： NFS网络文件系统工作方式 1、在nfs服务器端创建共享目录2、通过mount网络挂载，将NFS服务端共享目录挂载到NFS客户端本地目录3、NFS客户端在挂载目录上创建、删除、查看数据等操作，等价于在服务端进行的创建、删除、查看数据等操作。 如上图所示，在NFS服务器端设置一个共享目录/web后，其他有权限访问NFS服务器端的客户端都可以将这个共享目录/web挂载到客户端本地的某个挂载点（其实就是一个目录，这个挂载点可以自己随意指定），不同的客户端的挂载点可以不相同。客户端正确挂载完毕后，就可以通过NFS客户端的挂载点所在的/opt/www目录查看到NFS服务端/web共享出来的目录下的所有数据。在客户端查看时，NFS服务端的/web目录就相当于客户端本地的磁盘分区或目录，几乎感觉不到使用上的区别，根据NFS服务器端授予的NFS共享权限以及共享目录的本地系统权限，只要在指定的NFS客户端操作挂载的/opt/www目录，就可以将数据轻松的存取到NFS服务器端上的/web目录中了。 NFS工作流程 RPC服务工作原理 NFS部署示例环境规划 操作系统 角色 IP HOST CentOS release 7.4 NFS Server 192.168.1.31 nfs-server.com CentOS release 7.4 NFS Client1 (web server1) 192.168.1.32 nfs-client1.com CentOS release 7.4 NFS Client2 (web server2) 192.168.1.33 nfs-client2.com 这里NFS客户端是web服务器，站点目录挂载NFS服务端。实验环境关闭防火墙、selinux、时间同步等 具体操作步骤服务端安装配置1）检查是否安装NFS、RPC服务123456[root@nfs-server ~]# rpm -aq |egrep "nfs-utils|rpcbind"rpcbind-0.2.0-42.el7.x86_64nfs-utils-1.3.0-0.48.el7.x86_64# 如果没有安装则进行安装# yum -y install nfs-utils rpcbind 2）启动rpc和nfs服务123[root@nfs-server ~]# systemctl start rpcbind[root@nfs-server ~]# systemctl enable rpcbind[root@nfs-server ~]# systemctl start nfs 3）可以通过rpcinfo -p localhost可以查看到绑定了nfs12345678910111213141516171819202122232425262728[root@nfs-server ~]# rpcinfo -p localhost program vers proto port service 100000 4 tcp 111 portmapper 100000 3 tcp 111 portmapper 100000 2 tcp 111 portmapper 100000 4 udp 111 portmapper 100000 3 udp 111 portmapper 100000 2 udp 111 portmapper 100005 1 udp 20048 mountd 100005 1 tcp 20048 mountd 100005 2 udp 20048 mountd 100005 2 tcp 20048 mountd 100005 3 udp 20048 mountd 100005 3 tcp 20048 mountd 100024 1 udp 60358 status 100024 1 tcp 34912 status 100003 3 tcp 2049 nfs 100003 4 tcp 2049 nfs 100227 3 tcp 2049 nfs_acl 100003 3 udp 2049 nfs 100003 4 udp 2049 nfs 100227 3 udp 2049 nfs_acl 100021 1 udp 38577 nlockmgr 100021 3 udp 38577 nlockmgr 100021 4 udp 38577 nlockmgr 100021 1 tcp 42193 nlockmgr 100021 3 tcp 42193 nlockmgr 100021 4 tcp 42193 nlockmgr 4）配置共享目录并重启nfs123456[root@nfs-server ~]# mkdir /web[root@nfs-server ~]# vim /etc/exports/web 192.168.1.0/24(rw,sync,no_root_squash) #不压制root(当client端使用root挂载时，也有root权限)[root@nfs-server ~]# systemctl restart nfs[root@nfs-server ~]# exportfs -v/web 192.168.1.0/24(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash) 客户端挂载测试1）创建挂载目录并进行挂载123456789101112131415[root@nfs-client1 ~]# mkdir /opt/www[root@nfs-client1 ~]# showmount -e 192.168.1.31 #查看服务器共享目录Export list for 192.168.1.31:/web 192.168.1.0/24[root@nfs-client1 ~]# mount -t nfs 192.168.1.31:/web /opt/www/[root@nfs-client1 ~]# df -h文件系统 容量 已用 可用 已用% 挂载点/dev/mapper/centos-root 20G 3.6G 16G 19% /devtmpfs 473M 0 473M 0% /devtmpfs 489M 0 489M 0% /dev/shmtmpfs 489M 14M 475M 3% /runtmpfs 489M 0 489M 0% /sys/fs/cgroup/dev/sda1 497M 154M 344M 31% /boottmpfs 98M 0 98M 0% /run/user/0192.168.1.31:/web 20G 3.6G 16G 19% /opt/www 2）在client1上的/opt/www创建一个测试文件1[root@nfs-client1 ~]# echo "client1 create test file" &gt;&gt; /opt/www/client1.txt 3）回到nfs上进行查看123[root@nfs-server ~]# ll /web/总用量 4-rw-r--r-- 1 root root 25 8月 9 14:41 client1.txt 4）安装nginx并配置站点目录为/opt/www123456789[root@nfs-client1 ~]# yum -y install nginx[root@nfs-client1 ~]# vim /etc/nginx/nginx.confserver &#123; listen 80 default_server; listen [::]:80 default_server; server_name _; root /opt/www;...[root@nfs-client1 ~]# systemctl start nginx 上面的步骤同样在client2上面操作。 在nfs服务端创建站点首页，访问client客户端测试1234567891011121314151617181920212223[root@nfs-server ~]# ll /web/总用量 8-rw-r--r-- 1 root root 25 8月 9 14:41 client1.txt-rw-r--r-- 1 root root 25 8月 9 15:37 client2.txt[root@nfs-server ~]# echo "&lt;h1&gt;NFS server&lt;/h1&gt;" &gt;&gt; /web/index.html #nfs服务端共享目录创建首页文件[root@nfs-server ~]# curl 192.168.1.32&lt;h1&gt;NFS server&lt;/h1&gt;[root@nfs-server ~]# [root@nfs-server ~]# curl 192.168.1.33&lt;h1&gt;NFS server&lt;/h1&gt;[root@nfs-client1 ~]# ll /opt/www/ #nfs客户端1上查看总用量 12-rw-r--r-- 1 root root 25 8月 9 14:41 client1.txt-rw-r--r-- 1 root root 25 8月 9 15:37 client2.txt-rw-r--r-- 1 root root 20 8月 9 15:41 index.html[root@nfs-client2 ~]# ll /opt/www/ #nfs客户端2上查看总用量 12-rw-r--r-- 1 root root 25 8月 9 14:41 client1.txt-rw-r--r-- 1 root root 25 8月 9 15:37 client2.txt-rw-r--r-- 1 root root 20 8月 9 15:41 index.html 通过测试可以看出，客户端挂载后，就完全相当于自己的一个目录或者文件，在负载均衡架构中一般通过这种方式做共享存储。 NFS配置参数说明nfs共享参数及作用通过 man exports可以查看帮助手册 共享参数 作用 rw* 读写权限 ro 只读权限 root_squash 当NFS客户端以root管理员访问时，映射为NFS服务器的匿名用户(不常用) no_root_squash 当NFS客户端以root管理员访问时，映射为NFS服务器的root管理员(常用) all_squash 无论NFS客户端使用什么账户访问，均映射为NFS服务器的匿名用户(常用) no_all_squash 无论NFS客户端使用什么账户访问，都不进行压缩 sync* 同时将数据写入到内存与硬盘中，保证不丢失数据 async 优先将数据保存到内存，然后再写入硬盘；这样效率更高，但可能会丢失数据 anonuid* 配置all_squash使用,指定NFS的用户UID,必须存在系统 anongid* 配置all_squash使用,指定NFS的用户GID,必须存在系统 配置实例说明123[root@nfs-server ~]# cat /etc/exports#/web 192.168.1.0/24(rw,sync,no_root_squash) #不压制root(当client端使用root挂载时，也有root权限)/web 192.168.1.32(rw,sync,no_root_squash) 192.168.1.33(rw,sync,all_squash,anonuid=2000,anongid=2000) #[共享目录]&emsp; [客户端地址1(权限)]&emsp; [客户端地址2(权限)]1、共享目录：每一行最前面是共享出来的目录，比如上面我要共享/web目录，那么此选项就可以直接写/web目录，这个目录可以依照不同的权限共享给不同的目录。2、客户端地址：客户端地址能够设置一个网络，也可以是单个主机。参数：如上面的读写权限rw，同步更新sync等待。 1、客户端地址可以使用完整的IP或者网络号，例如192.168.1.33或192.168.1.0/242、同样可以使用主机名，但是这个主机名必须要在/etc/hosts中存在，或者可以通过DNS找到才行。]]></content>
      <categories>
        <category>Storage</category>
      </categories>
      <tags>
        <tag>Storage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP配合inotify实现光闸ftp功能]]></title>
    <url>%2F2019%2F08%2F06%2FPHP%E9%85%8D%E5%90%88inotify%E5%AE%9E%E7%8E%B0%E5%85%89%E9%97%B8ftp%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[PHP配合inotify实现光闸ftp功能前言 由于业务需要，内外网的数据需要进行实时交互同步，由于环境因素，只能通过ftp进行文件的交互传输。且没有硬件设备（光闸）；鉴于这种情况下，首先考虑到的是通过shell脚本进行自动上传下载文件。然而在实施部署过程中发现，通过shell脚本方式进行文件的上传下载每一个文件都会建立一次连接。（由于环境因素，ftp通过网闸再通过vpn建立一次连接巨慢，大概需要几十秒。）可是项目站点必须是实时进行文件的传输，且每次可能是多个文件，多者甚至上百个。经过探讨考虑到使用rsync+inotify，通过rsync的守护进程模式进行文件的传输。可是这种项目迟早得换成光闸走ftp进行文件的传输。而后又考虑直接在代码里面实现建立ftp的长连接进行文件传输（建立一次连接，然后一直处于监听模式，发现有文件就及时进行传送）,这样一来又考虑到目录的监听情况，开始准备使用inotify-tools进行监听然后调用php代码里面的ftp长连接进程，随后又在php官方发现php也有一个inotify可以用来进行目录的监听，然后触发动作。于是，最终方案就是在内外网两台服务器上面搭建ftp服务，然后通过php脚本进行目录的监听并上传到ftp服务器。 具体实现流程说明1、内外网的服务器均安装ftp。2、内外网的服务器均安装php及php的inotify插件。3、内外网的服务器均安装redis。（php分为两个文件，一个为文件(listen.php)用于监听，一个文件(upFile.php)用于上传文件，由于开始没有存入redis，导致文件堆积，然后一直阻塞。故最终方案通过监听文件将监听到发生变化的文件路径存入到redis里，然后通过upfile脚本将文件上传到ftp） 环境说明 IP 主机名 系统版本 网络说明 192.168.1.32 internet.example.com CenTOS7.4 模拟外网 192.168.1.33 lan.example.com CenTOS7.4 模拟内网 1、这里是通过两台虚拟机进行模拟内外网。且上面真实项目操作系统为CenTOS6.5；这里使用CenTOS7.5模拟。2、两台服务器需要安装软件有：vsftpd、php、php插件inotify3、外网的/opt/ftp/out目录只要有新的文件产生就会自动触发监听然后传送至内网的/opt/ftp/come目录。4、反之一样，内网的/opt/ftp/out目录只要有新的文件产生就会自动触发监听然后传送至外网的/opt/ftp/come目录。 项目结构图 具体实验步骤安装配置vsftpd 说明：内外网都需要配置vsftpd，配置vsftpd虚拟用户可参考 https://www.cnblogs.com/yanjieli/p/10723108.html 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# mkdir -p /opt/ftp/&#123;come,out&#125; #创建对应的数据目录# yum -y install vsftpd #安装vsftpd# useradd -s /sbin/nologin apache #由于项目站点所使用的apache用户，所以这里为了防止权限问题，这里也是用apache用户作为虚拟用户# chown apache.apache /opt/ftp/ -R #将数据目录权限改为apache用户# cat &gt;/etc/vsftpd/logins.txt&lt;&lt;-EOF #创建ftp虚拟账号和密码，一行账号一行密码ftp_come_userFtp_come_password@Qaz123EOF# db_load -T -t hash -f /etc/vsftpd/logins.txt /etc/vsftpd/login.db #将密码文件转换成db格式# chmod 600 /etc/vsftpd/login.db #更改db文件的权限# cat &gt;/etc/pam.d/ftp&lt;&lt;-EOF #定义pam认证文件auth required /lib64/security/pam_userdb.so db=/etc/vsftpd/loginaccount required /lib64/security/pam_userdb.so db=/etc/vsftpd/loginEOF# cp /etc/vsftpd/vsftpd.conf&#123;,.back&#125; #备份配置文件# vim /etc/vsftpd/vsftpd.conf #编辑vsftpd主配置文件anonymous_enable=NOlocal_enable=YESwrite_enable=NOanon_upload_enable=NOanon_mkdir_write_enable=NOanon_other_write_enable=NOchroot_local_user=YESallow_writeable_chroot=YESguest_enable=YESguest_username=apachelisten=YESlisten_port=21pasv_enable=YESpasv_address=192.168.1.32 #内网的那台改成对应内网的IP地址pasv_min_port=10000pasv_max_port=10088anon_world_readable_only=NOuser_config_dir=/etc/vsftpd/user_conftcp_wrappers=YES# vim /etc/vsftpd/user_conf/ftp_come_user #编辑虚拟用户的子配置文件write_enable=YESanon_world_readable_only=noanon_upload_enable=YESanon_mkdir_write_enable=YESanon_other_write_enable=YESanon_umask=022local_root=/opt/ftp/come# systemctl start vsftpd #启动ftp 测试ftp1）在外网（192.168.1.32）上访问内网（192.168.1.33）的ftp12345678[root@internet ~]# dd if=/dev/zero of=testfile bs=1M count=100 #创建一个测试文件用于测试上传[root@internet ~]# lftp ftp_come_user:Ftp_come_password@Qaz123@192.168.1.33 #通过lftp工具连接ftplftp ftp_come_user@192.168.1.33:~&gt; lcdlcd 成功, 本地目录=/rootlftp ftp_come_user@192.168.1.33:~&gt; put testfile104857600 bytes transferred in 2 seconds (55.59M/s)lftp ftp_come_user@192.168.1.33:/&gt; ls-rw-r--r-- 1 1001 1001 104857600 Aug 05 12:41 testfile 2）在内网（192.168.1.33）上访问外网（192.168.1.32）的ftp1234567[root@lan ~]# dd if=/dev/zero of=testfile1 bs=1M count=100 #同样创建一个测试文件[root@lan ~]# lftp ftp_come_user:Ftp_come_password@Qaz123@192.168.1.32lftp ftp_come_user@192.168.1.32:~&gt; put testfile1104857600 bytes transferred in 3 seconds (31.87M/s) lftp ftp_come_user@192.168.1.32:/&gt;lftp ftp_come_user@192.168.1.32:/&gt; ls-rw-r--r-- 1 1001 1001 104857600 Aug 05 12:45 testfile1 3）在内网的/opt/ftp/come目录验证是否存在testfile文件123[root@lan ~]# ll /opt/ftp/come/总用量 102400-rw-r--r-- 1 apache apache 104857600 8月 5 20:41 testfile 4）在外网的/opt/ftp/come目录验证是否存在testfile1文件123[root@internet ~]# ll /opt/ftp/come/总用量 102400-rw-r--r-- 1 apache apache 104857600 8月 5 20:45 testfile1 通过测试ftp已经ok 安装配置redis 生产环境redis可能端口不一样，然后根据不同的进行配置即可。同时设置连接密码，（这里只是测试，所以两边的配置一样） 12345678# yum -y install redis #安装redis# cp /etc/redis.conf&#123;,.back&#125; #拷贝redis配置文件# vim /etc/redis.conf #编辑redis配置文件daemonize yes #开启后台常驻进程requirepass obohna7Fogai9ahnahth #设置redis连接密码# systemctl start redis #启动redis# netstat -nlutp |grep 6379 #检查端口是否打开tcp 0 0 127.0.0.1:6379 0.0.0.0:* LISTEN 7139/redis-server 1 安装php及inotify插件1）yum安装php123# wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo# rpm -Uvh http://rpms.famillecollet.com/enterprise/remi-release-7.rpm# yum install --enablerepo=remi,remi-php56 php php-opcache php-pecl-apcu php-devel php-mbstring php-mcrypt php-mysqlnd php-phpunit-PHPUnit php-pecl-xdebug php-pecl-xhprof php-pdo php-pear php-fpm php-cli php-xml php-bcmath php-process php-gd php-common php-memcache php-redis php-pspell php-fpm php-mysql php-common php-mssql 2）安装php插件inotify1234567891011# wget http://pecl.php.net/get/inotify-0.1.6.tgz #下载软件包# tar xf inotify-0.1.6.tgz #解压软件包# yum install gcc gcc-c++ make #安装编译工具# tar xf inotify-0.1.6.tgz# cd inotify-0.1.6/# phpize# ./configure --with-php-config=/usr/bin/php-config --enable-inotify# make &amp;&amp; make install# echo "extension = inotify.so" &gt;&gt; /etc/php.d/inotify.ini# php -i|grep inotify# systemctl start php-fpm 编写PHP脚本 内外网其实一样，只是配置的连接信息不一样。 1）监听脚本(listen.php)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235[root@internet ~]# mkdir /home/script[root@internet ~]# vim /home/script/listen.php&lt;?php/** * Created by PhpStorm. * User: ADMIN * Date: 2019/7/31 * Time: 19:21 */define('REDIS_HOST','127.0.0.1'); //redis连接主机define('REDIS_PASSWORD','obohna7Fogai9ahnahth'); //redis连接密码define('REDIS_PORT','6379'); //redis端口define('__LISTEN_DIR__',"/opt/ftp/out"); //本地监听目录class Monitor&#123; private $dir = ''; //文件新增事件回调 private $onfileAdd; public $timeout = 1; public function __construct($dir,$onfileAdd) &#123; if (!extension_loaded('inotify')) &#123; echo '请安装inotify扩展', PHP_EOL; exit; &#125; $this-&gt;dir = $dir; $this-&gt;onfileAdd = $onfileAdd; $this-&gt;run(); &#125; //对目录进行监控 public function run() &#123; $dirs = $this-&gt;getDirs($this-&gt;dir); if (empty($dirs)) &#123; return false; &#125; $fd = inotify_init(); //设置为非阻塞模式 stream_set_blocking($fd, 0); $watcher = []; foreach ($dirs as $dir) &#123; $watch = inotify_add_watch($fd, $dir, IN_MODIFY | IN_CREATE | IN_DELETE | IN_DELETE_SELF | IN_CLOSE_WRITE); if (!$watch) &#123; echo "&#123;$dir&#125; 添加监控错误", PHP_EOL; exit; &#125; $watcher[$watch] = $dir; &#125; while (true) &#123; $reads = [$fd]; $write = []; $except = []; if (stream_select($reads, $write, $except, $this-&gt;timeout) &gt; 0) &#123; if (!empty($reads)) &#123; foreach ($reads as $read) &#123; //从可读流中读取数据 $events = inotify_read($read); $fileName = ''; foreach ($events as $event) &#123; //文件改变 $fileChg = false; //目录改变 $dirChg = false; $fileName = $event['name']; switch ($event['mask']) &#123; case IN_CREATE: //创建 IN_CLOSE_WRITE 写入结束 case IN_DELETE: $fileChg = true; break; case 1073742080: case 1073742336: $dirChg = true; break; &#125; if ($fileChg) &#123; $filepath = $watcher[$event['wd']] . '/' . $fileName; if($watcher[$event['wd']] == $this-&gt;dir)&#123; $newfile = $fileName; &#125;else&#123; $newfile = str_replace($this-&gt;dir,'.',$filepath); &#125; echo date('Y-m-d H:i:s') . " | 事件目录：" . $filepath . "，事件出现：&#123;$event['mask']&#125; ，文件/目录 &#123;$fileName&#125; 发生改变" . PHP_EOL; echo date('Y-m-d H:i:s') . " | " . $newfile . PHP_EOL;// echo "文件 &#123;$fileName&#125; 发生改变", PHP_EOL;// echo shell_exec($this-&gt;cmd), PHP_EOL;// SysutilLogger::instance()-&gt;fileAdd("事件目录：" . $watcher[$event['wd']] . "，事件出现：&#123;$event['mask']&#125; ，文件/目录 &#123;$fileName&#125; 发生改变"); call_user_func_array($this-&gt;onfileAdd,[$filepath,$newfile]); &#125; if ($dirChg) &#123;// echo "目录 &#123;$fileName&#125; 发生改变", PHP_EOL;// echo shell_exec($this-&gt;cmd), PHP_EOL; //目录发生改变时 重新监听 return $this-&gt;run(); &#125; &#125; &#125; &#125; &#125; &#125; return true; &#125; //递归的获取当前目录下所有子目录路径 public function getDirs($dir) &#123; $dir = realpath($dir); $dh = opendir($dir); if (!$dh) &#123; return []; &#125; $dirs = []; $dirs[] = $dir; while (($file = readdir($dh)) !== false) &#123; if ($file == '.' || $file == '..') &#123; continue; &#125; $full = $dir . DIRECTORY_SEPARATOR . $file; if (is_dir($full)) &#123; $dirs = array_merge($dirs, $this-&gt;getDirs($full)); &#125; &#125; closedir($dh); return $dirs; &#125;&#125;class SysutilRedis &#123; private static $_instance; private $m_redis = null; private $mid_redis = null; //律师中心redis实例 /** * @return SysutilRedis|Redis */ public static function instance() &#123; if ( ! isset(SysutilRedis::$_instance)) &#123; SysutilRedis::$_instance = new SysutilRedis(); &#125; return SysutilRedis::$_instance; &#125; /** * @return null|Redis */ public function getRedis() &#123; if( !$this-&gt;m_redis )&#123; $this-&gt;m_redis = new Redis(); $this-&gt;m_redis-&gt;connect(REDIS_HOST,REDIS_PORT,10); //php客户端设置的ip及端口 $this-&gt;m_redis-&gt;auth(REDIS_PASSWORD); &#125; return $this-&gt;m_redis; &#125; public function close_redis() &#123; if($this-&gt;m_redis) &#123; $this-&gt;m_redis-&gt;close(); $this-&gt;m_redis = null; &#125; if($this-&gt;mid_redis) &#123; $this-&gt;mid_redis-&gt;close(); $this-&gt;mid_redis = null; &#125; &#125; public function __destruct() &#123; $this-&gt;close_redis(); &#125;&#125;class Listenner &#123; public function run()&#123; $callBack = [$this,"toRedis"]; $listen_dir = __LISTEN_DIR__; $monitor = new Monitor($listen_dir,$callBack); $monitor-&gt;run(); &#125; /** * @describe 文件回调 * @author zhouyong */ public function toRedis($filename,$newfile)&#123; $redis = SysutilRedis::instance()-&gt;getRedis(); $redis-&gt;select(9); $cache_id = 'upload_file_list'; $data = [ 'local' =&gt; $filename, 'remote' =&gt; $newfile ]; $redis-&gt;rpush($cache_id, json_encode($data)); &#125;&#125;$listenner = new Listenner();$listenner-&gt;run(); 2）上传文件脚本(upFile.php)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281[root@internet ~]# vim /home/script/upFile.php&lt;?php/** * Created by PhpStorm. * User: ADMIN * Date: 2019/7/31 * Time: 19:21 */define('REDIS_HOST','127.0.0.1'); //redis连接主机define('REDIS_PASSWORD','obohna7Fogai9ahnahth'); //redis连接密码define('REDIS_PORT','6379'); //redis端口define('FTP_HOST','192.168.1.33'); //内网ftp服务器define('FTP_PORT',21); //ftp端口define('FTP_USER','ftp_come_user'); //ftp连接账号define('FTP_PASS','Ftp_come_password@Qaz123'); //ftp连接密码class SysutilFtp&#123; private static $instance; public $off; // 返回操作状态(成功/失败) public $conn_id; // FTP连接 /** * 方法：FTP连接 * @FTP_HOST -- FTP主机 * @FTP_PORT -- 端口 * @FTP_USER -- 用户名 * @FTP_PASS -- 密码 */ private function __construct($FTP_HOST,$FTP_PORT,$FTP_USER,$FTP_PASS) &#123; $this-&gt;conn_id = @ftp_connect($FTP_HOST,$FTP_PORT) or die("FTP服务器连接失败"); @ftp_login($this-&gt;conn_id,$FTP_USER,$FTP_PASS) or die("FTP服务器登陆失败"); @ftp_pasv($this-&gt;conn_id,1); // 打开被动模拟 &#125; /** * @describe 获取ftp实例 */ public static function getInstance()&#123; if(!self::$instance)&#123; self::$instance = new self(FTP_HOST,FTP_PORT,FTP_USER,FTP_PASS); if(self::$instance)&#123; echo date('Y-m-d H:i:s') . " | connected !" . PHP_EOL; &#125; &#125; return self::$instance; &#125; /** * @describe 重连 * @author zhouyong */ private function reConnect()&#123; echo date('Y-m-d H:i:s') . " | closing connection ......" . PHP_EOL; //关闭当前链接 $this-&gt;close(); echo date('Y-m-d H:i:s') . " | reconnecting...." . PHP_EOL; //开辟新连接 self::$instance = new self(FTP_HOST,FTP_PORT,FTP_USER,FTP_PASS); echo date('Y-m-d H:i:s') . " | connected !" . PHP_EOL; &#125; /** * 方法：上传文件 * @path -- 本地路径 * @newpath -- 上传路径 * @type -- 若目标目录不存在则新建 * @return true */ function up_file($path,$newpath,$type=true) &#123; if($type) $this-&gt;dir_mkdirs($newpath); echo date('Y-m-d H:i:s') . " | 当前目录" . ftp_pwd($this-&gt;conn_id) . PHP_EOL; $newpath = ltrim($newpath,'.'); echo $newpath . PHP_EOL; $this-&gt;off = @ftp_put($this-&gt;conn_id,$newpath,$path,FTP_BINARY); if(!$this-&gt;off)&#123;// echo "文件上传失败，请检查权限及路径是否正确！";// echo "文件上传失败，正在发起重连.....";// $this-&gt;reConnect();// echo date('Y-m-d H:i:s') . " | re-uploading file &#123;$path&#125; to &#123;$newpath&#125; ...." . PHP_EOL;// $this-&gt;up_file($path,$newpath,$type=true); echo date('Y-m-d H:i:s') . " | 上传失败" . PHP_EOL;// exit; return false; &#125; else &#123; echo date('Y-m-d H:i:s') . " | upload success !" . PHP_EOL; return true; &#125; &#125; /** * 方法：移动文件 * @path -- 原路径 * @newpath -- 新路径 * @type -- 若目标目录不存在则新建 */ function move_file($path,$newpath,$type=true) &#123; if($type) $this-&gt;dir_mkdirs($newpath); $this-&gt;off = @ftp_rename($this-&gt;conn_id,$path,$newpath); if(!$this-&gt;off) echo "文件移动失败，请检查权限及原路径是否正确！"; &#125; /** * 方法：复制文件 * 说明：由于FTP无复制命令,本方法变通操作为：下载后再上传到新的路径 * @path -- 原路径 * @newpath -- 新路径 * @type -- 若目标目录不存在则新建 */ function copy_file($path,$newpath,$type=true) &#123; $downpath = "c:/tmp.dat"; $this-&gt;off = @ftp_get($this-&gt;conn_id,$downpath,$path,FTP_BINARY);// 下载 if(!$this-&gt;off) echo "文件复制失败，请检查权限及原路径是否正确！"; $this-&gt;up_file($downpath,$newpath,$type); &#125; /** * 方法：删除文件 * @path -- 路径 */ function del_file($path) &#123; $this-&gt;off = @ftp_delete($this-&gt;conn_id,$path); if(!$this-&gt;off) echo "文件删除失败，请检查权限及路径是否正确！"; &#125; /** * 方法：生成目录 * @path -- 路径 */ function dir_mkdirs($path) &#123; $path_arr = explode('/',$path); // 取目录数组 $file_name = array_pop($path_arr); // 弹出文件名 $path_div = count($path_arr); // 取层数 print_r($path_arr); foreach($path_arr as $val) // 创建目录 &#123; if(@ftp_chdir($this-&gt;conn_id,$val) == FALSE) &#123; $tmp = @ftp_mkdir($this-&gt;conn_id,$val); echo date('Y-m-d H:i:s') . " | 创建目录&#123;$val&#125;" . PHP_EOL; if($tmp == FALSE) &#123; echo date('Y-m-d H:i:s') . " | 目录创建失败，请检查权限及路径是否正确！" . PHP_EOL; exit; &#125; @ftp_chdir($this-&gt;conn_id,$val); &#125; &#125; echo date('Y-m-d H:i:s') . " | 创建目录成功" . PHP_EOL; echo date('Y-m-d H:i:s') . " | 当前目录" . ftp_pwd($this-&gt;conn_id) . PHP_EOL; for($i=1;$i&lt;$path_div;$i++) // 回退到根 &#123; @ftp_cdup($this-&gt;conn_id); echo date('Y-m-d H:i:s') . " | 当前目录" . ftp_pwd($this-&gt;conn_id) . PHP_EOL; &#125; &#125; function ping()&#123; echo date("Y-m-d H:i:s") . " | 发送心跳数据 " . PHP_EOL; @ftp_raw($this-&gt;conn_id,'pingpong'); &#125; /** * 方法：关闭FTP连接 */ function close() &#123; @ftp_close($this-&gt;conn_id); &#125;&#125;// class class_ftp endclass SysutilRedis &#123; private static $_instance; private $m_redis = null; private $mid_redis = null; //律师中心redis实例 /** * @return SysutilRedis */ public static function instance()&#123; if ( ! isset(SysutilRedis::$_instance)) &#123; SysutilRedis::$_instance = new SysutilRedis(); &#125; return SysutilRedis::$_instance; &#125; /** * @return null|Redis */ public function getRedis() &#123; if( !$this-&gt;m_redis )&#123; $this-&gt;m_redis = new Redis(); $this-&gt;m_redis-&gt;connect(REDIS_HOST,REDIS_PORT,10); //php客户端设置的ip及端口 $this-&gt;m_redis-&gt;auth(REDIS_PASSWORD); &#125; return $this-&gt;m_redis; &#125; public function close_redis() &#123; if($this-&gt;m_redis) &#123; $this-&gt;m_redis-&gt;close(); $this-&gt;m_redis = null; &#125; if($this-&gt;mid_redis) &#123; $this-&gt;mid_redis-&gt;close(); $this-&gt;mid_redis = null; &#125; &#125; public function __destruct() &#123; $this-&gt;close_redis(); &#125;&#125;$redis = SysutilRedis::instance()-&gt;getRedis();$redis-&gt;select(9);$cache_id = 'upload_file_list';$no_data_time = 0;$ping = 3; //心跳时间 每 x 秒发送一次心跳数据while (true)&#123; $data = $redis-&gt;lRange($cache_id, 0, -1); if (!empty($data)) &#123; $no_data_time = 0; foreach ($data as $key =&gt; $value) &#123; $v = json_decode($value, true); $result = up_file($v['local'],$v['remote']); if($result)&#123; $redis-&gt;select(9); $redis-&gt;lPop($cache_id); &#125;else&#123; echo date("Y-m-d H:i:s") . "| 退出脚本"; exit(); &#125; &#125; &#125;else&#123; if($no_data_time &gt; 0)&#123; //查看没有事件的时间 $time = time(); if(($time - $no_data_time) &gt; $ping)&#123; //发送心跳 并重置时间 ping_ftp(); $no_data_time = $time; &#125; &#125;else&#123; $no_data_time = time(); &#125; &#125;&#125;function up_file($filename,$newfile)&#123; echo date('Y-m-d H:i:s') . " | $filename , $newfile" . PHP_EOL; $ftp = SysutilFtp::getInstance(); return $ftp-&gt;up_file($filename,$newfile);&#125;function ping_ftp()&#123; $ftp = SysutilFtp::getInstance(); $ftp-&gt;ping();&#125; 3）启动脚本并测试12345678910111213141516171819202122232425262728#在一个终端启动listen.php脚本[root@internet ~]# php -f /home/script/listen.php#在另外一个终端启动upFile.php脚本[root@internet ~]# php -f /home/script/upFile.php 2019-08-06 11:24:11 | connected !2019-08-06 11:24:11 | 发送心跳数据#在第三个终端往监听目录/opt/ftp/out目录新建一个文件或目录进行测试[root@internet ~]# echo "test" &gt;&gt; /opt/ftp/out/file1#分别可以在上面第一个终端和第二个终端看到日志信息终端一的信息：2019-08-06 11:36:26 | 事件目录：/opt/ftp/out/file1，事件出现：256 ，文件/目录 file1 发生改变2019-08-06 11:36:26 | file1终端二的信息：2019-08-06 11:36:26 | /opt/ftp/out/file1 , file1Array()2019-08-06 11:36:26 | 创建目录成功2019-08-06 11:36:26 | 当前目录/2019-08-06 11:36:26 | 当前目录/file12019-08-06 11:36:26 | upload success ! 管理两个脚本1）编写监控脚本，当进程挂掉便重启1234567891011121314151617# vim /home/script/process_alive.sh#!/bin/bash#用来监听listen脚本和upFile脚本DATE=[`date "+%Y-%m-%d %H:%M:%S"`]function phpScriptAlive &#123; script_dir=/home/script/ ps aux |grep -v grep |grep $1 &gt;&gt; /dev/null if [[ $? != 0 ]]; then nohup php -f $&#123;script_dir&#125;/$1 &gt;&gt; /var/log/$2 &amp; echo "$&#123;DATE&#125; 重启php脚本 $1" &gt;&gt; /var/log/php_script_restart.log fi&#125;phpScriptAlive listen.php php_listen.logphpScriptAlive upFile.php php_upfile.log 2）添加到计划任务123# chmod +x /home/script/process_alive.sh # 监听php脚本进程* * * * * /bin/bash /home/script/process_alive.sh 3）查看日志；由于上面脚本里面写了将所有输出都记录到日志里面，所以后续直接查看日志即可123456789[root@internet ~]# tailf /var/log/phpphp-fpm/ php_listen.log php_script_restart.log php_upfile.log[root@internet ~]# tailf /var/log/php_listen.log ^C[root@internet ~]# tailf /var/log/php_upfile.log 2019-08-06 12:01:18 | 发送心跳数据 2019-08-06 12:01:22 | 发送心跳数据 2019-08-06 12:01:26 | 发送心跳数据 2019-08-06 12:01:30 | 发送心跳数据 4）再次往/opt/ftp/out/目录丢文件，便会自动同步到内网或者外网的/opt/ftp/come目录。123456789101112131415161718[root@internet ~]# echo "sjdklf" &gt;&gt; /opt/ftp/out/file111[root@internet ~]# for i in `seq 10`; do echo "sjdlfjskldjf" &gt;&gt; /opt/ftp/out/txt$i ; don#内网查看[root@lan ~]# ll /opt/ftp/come/总用量 48-rw-r--r-- 1 apache apache 5 8月 6 11:36 file1-rw-r--r-- 1 apache apache 7 8月 6 12:06 file111-rw-r--r-- 1 apache apache 13 8月 6 12:07 txt1-rw-r--r-- 1 apache apache 13 8月 6 12:07 txt10-rw-r--r-- 1 apache apache 13 8月 6 12:07 txt2-rw-r--r-- 1 apache apache 13 8月 6 12:07 txt3-rw-r--r-- 1 apache apache 13 8月 6 12:07 txt4-rw-r--r-- 1 apache apache 13 8月 6 12:07 txt5-rw-r--r-- 1 apache apache 13 8月 6 12:07 txt6-rw-r--r-- 1 apache apache 13 8月 6 12:07 txt7-rw-r--r-- 1 apache apache 13 8月 6 12:07 txt8-rw-r--r-- 1 apache apache 13 8月 6 12:07 txt9]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK快速入门五-配置nginx代理kibana]]></title>
    <url>%2F2019%2F07%2F05%2FELK%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E4%BA%94-%E9%85%8D%E7%BD%AEnginx%E4%BB%A3%E7%90%86kibana%2F</url>
    <content type="text"><![CDATA[由于kibana界面默认没有安全认证界面，为了保证安全，通过nginx进行代理并设置访问认证。 配置kibana12345[root@linux-elk1 ~]# vim /etc/kibana/kibana.ymlserver.host: "127.0.0.1" #将监听地址更改为127.0.0.1[root@linux-elk1 ~]# systemctl restart kibana[root@linux-elk1 ~]# netstat -nlutp |grep 5601tcp 0 0 127.0.0.1:5601 0.0.0.0:* LISTEN 72068/node 部署nginx1）安装nginx1[root@linux-elk1 ~]# yum -y install nginx httpd-tools 2）配置nginx12345678910111213141516171819202122232425[root@linux-elk1 ~]# vim /etc/nginx/conf.d/kibana.confupstream kibana_server &#123; server 127.0.0.1:5601 weight=1 max_fails=3 fail_timeout=60;&#125;server &#123; listen 80; server_name www.kibana.com; auth_basic "Restricted Access"; auth_basic_user_file /etc/nginx/conf.d/htpasswd.users; location / &#123; proxy_pass http://kibana_server; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; &#125;&#125;[root@linux-elk1 ~]# htpasswd -bc /etc/nginx/conf.d/htpasswd.users admin 123456Adding password for user admin[root@linux-elk1 ~]# cat /etc/nginx/conf.d/htpasswd.usersadmin:$apr1$ro5tQZp9$grhByziZtm3ZpZCsSFzsQ1[root@linux-elk1 ~]# systemctl start nginx 3）windows上添加hosts, 路径C:\Windows\System32\drivers\etc\hosts1192.168.1.31 www.kibana.com 4）测试验证]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK快速入门四-filebeat替代logstash收集日志]]></title>
    <url>%2F2019%2F07%2F05%2FELK%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%9B%9B-filebeat%E6%9B%BF%E4%BB%A3logstash%E6%94%B6%E9%9B%86%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[filebeat简介 Filebeat是轻量级单用途的日志收集工具，用于在没有安装java的服务器上专门收集日志，可以将日志转发到logstash、elasticsearch或redis等场景中进行下一步处理。官网下载地址：https://www.elastic.co/cn/downloads/past-releases#filebeat官方文档：https://www.elastic.co/guide/en/beats/filebeat/current/configuring-howto-filebeat.html filebeat安装配置1）下载filebeat123# 这里是在logstash服务器上面做的，为了测试，所以先将logstash停止。[root@logstash ~]# systemctl stop logstash[root@logstash ~]# wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.8.1-x86_64.rpm 2）安装filebeat1[root@logstash ~]# yum -y localinstall filebeat-6.8.1-x86_64.rpm 配置filebeat收集系统日志输出到文件1）编辑filebeat配置文件12345678910111213141516171819202122[root@logstash ~]# cp /etc/filebeat/filebeat.yml&#123;,.bak&#125;[root@logstash ~]# grep -v "#" /etc/filebeat/filebeat.yml |grep -v "^$"filebeat.inputs:- type: log # 默认值 log ，表示一个日志读取源 enabled: true # 该配置是否生效，如果设置为 false 将不会收集该配置的日志 paths: - /var/log/messages # 要抓取的日志路径，写绝对路径,可以多个 - /var/log/*.logfilebeat.config.modules: path: $&#123;path.config&#125;/modules.d/*.yml reload.enabled: falsesetup.template.settings: index.number_of_shards: 3setup.kibana:output.file: path: "/tmp" filename: "filebeat.txt"processors: - add_host_metadata: ~ - add_cloud_metadata: ~[root@logstash ~]# systemctl start filebeat 2）测试验证数据1234567[root@logstash ~]# echo "test" &gt;&gt; /var/log/messages[root@logstash ~]# tail /tmp/filebeat.txt &#123;"@timestamp":"2019-07-11T02:18:10.331Z","@metadata":&#123;"beat":"filebeat","type":"doc","version":"6.8.1"&#125;,"prospector":&#123;"type":"log"&#125;,"input":&#123;"type":"log"&#125;,"beat":&#123;"name":"logstash","hostname":"logstash","version":"6.8.1"&#125;,"host":&#123;"architecture":"x86_64","os":&#123;"platform":"centos","version":"7 (Core)","family":"redhat","name":"CentOS Linux","codename":"Core"&#125;,"id":"12bcfdc379904e4eb20173a568ecd7df","containerized":false,"name":"logstash"&#125;,"source":"/var/log/messages","offset":53643,"log":&#123;"file":&#123;"path":"/var/log/messages"&#125;&#125;,"message":"Jul 11 10:18:10 node01 systemd: Stopping Filebeat sends log files to Logstash or directly to Elasticsearch...."&#125;&#123;"@timestamp":"2019-07-11T02:18:13.324Z","@metadata":&#123;"beat":"filebeat","type":"doc","version":"6.8.1"&#125;,"prospector":&#123;"type":"log"&#125;,"beat":&#123;"version":"6.8.1","name":"logstash","hostname":"logstash"&#125;,"host":&#123;"name":"logstash","architecture":"x86_64","os":&#123;"family":"redhat","name":"CentOS Linux","codename":"Core","platform":"centos","version":"7 (Core)"&#125;,"id":"12bcfdc379904e4eb20173a568ecd7df","containerized":false&#125;,"log":&#123;"file":&#123;"path":"/var/log/messages"&#125;&#125;,"message":"Jul 11 10:18:10 node01 systemd: Started Filebeat sends log files to Logstash or directly to Elasticsearch..","source":"/var/log/messages","offset":53754,"input":&#123;"type":"log"&#125;&#125;&#123;"@timestamp":"2019-07-11T02:18:13.324Z","@metadata":&#123;"beat":"filebeat","type":"doc","version":"6.8.1"&#125;,"host":&#123;"architecture":"x86_64","name":"logstash","os":&#123;"codename":"Core","platform":"centos","version":"7 (Core)","family":"redhat","name":"CentOS Linux"&#125;,"id":"12bcfdc379904e4eb20173a568ecd7df","containerized":false&#125;,"source":"/var/log/messages","offset":53862,"log":&#123;"file":&#123;"path":"/var/log/messages"&#125;&#125;,"message":"Jul 11 10:18:10 node01 systemd: Starting Filebeat sends log files to Logstash or directly to Elasticsearch....","prospector":&#123;"type":"log"&#125;,"input":&#123;"type":"log"&#125;,"beat":&#123;"name":"logstash","hostname":"logstash","version":"6.8.1"&#125;&#125;&#123;"@timestamp":"2019-07-11T02:18:48.328Z","@metadata":&#123;"beat":"filebeat","type":"doc","version":"6.8.1"&#125;,"offset":53973,"log":&#123;"file":&#123;"path":"/var/log/messages"&#125;&#125;,"message":"test","input":&#123;"type":"log"&#125;,"prospector":&#123;"type":"log"&#125;,"beat":&#123;"name":"logstash","hostname":"logstash","version":"6.8.1"&#125;,"host":&#123;"name":"logstash","os":&#123;"version":"7 (Core)","family":"redhat","name":"CentOS Linux","codename":"Core","platform":"centos"&#125;,"id":"12bcfdc379904e4eb20173a568ecd7df","containerized":false,"architecture":"x86_64"&#125;,"source":"/var/log/messages"&#125; 配置filebeat收集系统日志输出redis1）编辑filebeat配置文件，修改输出123456789101112131415161718192021222324[root@logstash ~]# grep -v "#" /etc/filebeat/filebeat.yml |grep -v "^$"filebeat.inputs:- type: log enabled: true paths: - /var/log/messages - /var/log/*.logfilebeat.config.modules: path: $&#123;path.config&#125;/modules.d/*.yml reload.enabled: falsesetup.template.settings: index.number_of_shards: 3setup.kibana:output.redis: hosts: ["192.168.1.30:6379"] #redis服务器及端口 key: "system-log-33" #这里自定义key的名称，为了后期处理 db: 1 #使用第几个库 timeout: 5 #超时时间 password: 123321 #redis 密码processors: - add_host_metadata: ~ - add_cloud_metadata: ~[root@logstash ~]# systemctl restart filebeat 2）验证redis中是否有数据123456789[root@linux-redis ~]# redis-cli -h 192.168.1.30192.168.1.30:6379&gt; AUTH 123321OK192.168.1.30:6379&gt; SELECT 1OK192.168.1.30:6379[1]&gt; KEYS *1) "system-log-33"192.168.1.30:6379[1]&gt; LLEN system-log-33(integer) 3 3）logstash服务器上面配置从redis服务器中取数据1234567891011121314151617181920[root@linux-elk1 ~]# cat /etc/logstash/conf.d/redis-filebeat.conf input &#123; redis &#123; data_type =&gt; "list" host =&gt; "192.168.1.30" password =&gt; "123321" port =&gt; "6379" db =&gt; "1" key =&gt; "system-log-33" &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; ["192.168.1.31:9200"] index =&gt; "file-systemlog-%&#123;+YYYY.MM.dd&#125;" &#125;&#125;[root@linux-elk1 ~]# systemctl restart logstash 4）输入测试数据到日志文件里123[root@logstash ~]# echo "11111111111111" &gt;&gt; /var/log/messages[root@logstash ~]# echo "2222222222" &gt;&gt; /var/log/messages[root@logstash ~]# echo "33333333" &gt;&gt; /var/log/messages 5）kibana界面创建索引模式6）验证数据]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK快速入门三-logstash收集日志写入redis]]></title>
    <url>%2F2019%2F07%2F05%2FELK%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E4%B8%89-logstash%E6%94%B6%E9%9B%86%E6%97%A5%E5%BF%97%E5%86%99%E5%85%A5redis%2F</url>
    <content type="text"><![CDATA[用一台服务器部署redis服务，专门用于日志缓存使用，一般用于web服务器产生大量日志的场景。 这里是使用一台专门用于部署redis ，一台专门部署了logstash，在linux-elk1ELK集群上面进行日志收集存到了redis服务器上面，然后通过专门的logstash服务器去redis服务器里面取出数据在放到kibana上面进行展示 部署redis下载安装redis12345678[root@linux-redis ~]# wget http://download.redis.io/releases/redis-5.0.0.tar.gz[root@linux-redis ~]# tar -xvzf redis-5.0.0.tar.gz[root@linux-redis ~]# mv redis-5.0.0 /usr/local/src/[root@linux-redis ~]# ln -sv /usr/local/src/redis-5.0.0 /usr/local/redis"/usr/local/redis" -&gt; "/usr/local/src/redis-5.0.0"[root@linux-redis ~]# cd /usr/local/redis/[root@linux-redis ~]# make distclean[root@linux-redis ~]# make 配置redis1234567891011121314[root@linux-redis redis]# vim redis.confdaemonize yesbind 192.168.1.30requirepass 123321[root@linux-redis redis]# cp /usr/local/redis/src/redis-server /usr/bin/[root@linux-redis redis]# cp /usr/local/redis/src/redis-cli /usr/bin/[root@linux-redis redis]# redis-server /usr/local/redis/redis.conf 4007:C 10 Jul 2019 12:24:30.367 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo4007:C 10 Jul 2019 12:24:30.367 # Redis version=5.0.0, bits=64, commit=00000000, modified=0, pid=4007, just started4007:C 10 Jul 2019 12:24:30.367 # Configuration loaded[root@linux-redis redis]# netstat -nlutp |grep 6379tcp 0 0 192.168.1.30:6379 0.0.0.0:* LISTEN 4008/redis-server 1 测试redis12345678[root@linux-redis redis]# redis-cli -h 192.168.1.30192.168.1.30:6379&gt; AUTH 123321OK192.168.1.30:6379&gt; pingPONG192.168.1.30:6379&gt; KEYS *(empty list or set)192.168.1.30:6379&gt; quit 配置logstash将日志写入redis 将系统日志的通过logstash收集之后写入redis，然后通过另外的logstash将redis服务器的数据取出来。 配置logstash的配置文件12345678910111213141516171819202122[root@linux-elk1 ~]# vim /etc/logstash/conf.d/system.confinput &#123; file &#123; path =&gt; "/var/log/messages" type =&gt; "systemlog" start_position =&gt; "beginning" stat_interval =&gt; "2" &#125;&#125;output &#123; if [type] == "systemlog" &#123; redis &#123; data_type =&gt; "list" host =&gt; "192.168.1.30" password =&gt; "123321" port =&gt; "6379" db =&gt; "0" key =&gt; "systemlog" &#125; &#125;&#125; 检查logstash配置语法是否正确1234567[root@linux-elk1 ~]# /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/system.conf -tWARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaultsCould not find log4j2 configuration at path /usr/share/logstash/config/log4j2.properties. Using default config which logs errors to the console[WARN ] 2019-07-10 14:46:46.324 [LogStash::Runner] multilocal - Ignoring the 'pipelines.yml' file because modules or command line options are specifiedConfiguration OK[root@linux-elk1 ~]# systemctl restart logstash 写入messages日志测试12[root@linux-elk1 ~]# echo "redis-test" &gt;&gt; /var/log/messages[root@linux-elk1 ~]# echo "systemlog" &gt;&gt; /var/log/messages 登录redis进行查看123456789[root@linux-redis ~]# redis-cli -h 192.168.1.30192.168.1.30:6379&gt; AUTH 123321OK192.168.1.30:6379&gt; SELECT 0OK192.168.1.30:6379&gt; KEYS *1) "systemlog"192.168.1.30:6379&gt; LLEN systemlog(integer) 126 配置logstash从redis中取出数据到elasticsearch 配置专门logstash服务器从redis服务器读取指定的key的数据，并写入到elasticsearch 编辑logstash配置文件123456789101112131415161718[root@logstash ~]# vim /etc/logstash/conf.d/redis-read.confinput &#123; redis &#123; data_type =&gt; "list" host =&gt; "192.168.1.30" password =&gt; "123321" port =&gt; "6379" db =&gt; "0" key =&gt; "systemlog" &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; ["192.168.1.31:9200"] index =&gt; "redis-systemlog-%&#123;+YYYY.MM.dd&#125;" &#125;&#125; 测试logstash配置是否正确12345678910[root@logstash ~]# /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/redis-read.conf -tOpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=NWARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaultsCould not find log4j2 configuration at path /usr/share/logstash/config/log4j2.properties. Using default config which logs errors to the console[INFO ] 2019-07-10 16:41:50.576 [main] writabledirectory - Creating directory &#123;:setting=&gt;"path.queue", :path=&gt;"/usr/share/logstash/data/queue"&#125;[INFO ] 2019-07-10 16:41:50.649 [main] writabledirectory - Creating directory &#123;:setting=&gt;"path.dead_letter_queue", :path=&gt;"/usr/share/logstash/data/dead_letter_queue"&#125;[WARN ] 2019-07-10 16:41:51.498 [LogStash::Runner] multilocal - Ignoring the 'pipelines.yml' file because modules or command line options are specifiedConfiguration OK[root@logstash ~]# systemctl restart logstash 验证redis的数据是否被取出1234567891011[root@linux-redis ~]# redis-cli -h 192.168.1.30192.168.1.30:6379&gt; AUTH 123321OK192.168.1.30:6379&gt; SELECT 0OK192.168.1.30:6379&gt; KEYS *(empty list or set) #这里数据已经为空192.168.1.30:6379&gt; SELECT 1OK192.168.1.30:6379[1]&gt; KEYS *(empty list or set) #这里数据已经为空 head插件上验证数据 kibana界面创建索引模式并查看数据]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK快速入门二-通过logstash收集日志]]></title>
    <url>%2F2019%2F07%2F05%2FELK%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E4%BA%8C-%E9%80%9A%E8%BF%87logstash%E6%94%B6%E9%9B%86%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[说明 这里的环境接着上面的ELK快速入门-基本部署文章继续下面的操作。 收集多个日志文件1）logstash配置文件编写123456789101112131415161718192021222324252627282930[root@linux-elk1 ~]# vim /etc/logstash/conf.d/system-log.confinput &#123; file &#123; path =&gt; "/var/log/messages" type =&gt; "systemlog" start_position =&gt; "beginning" stat_interval =&gt; "3" &#125; file &#123; path =&gt; "/var/log/secure" type =&gt; "securelog" start_position =&gt; "beginning" stat_interval =&gt; "3" &#125;&#125;output &#123; if [type] == "systemlog" &#123; elasticsearch &#123; hosts =&gt; ["192.168.1.31:9200"] index =&gt; "system-log-%&#123;+YYYY.MM.dd&#125;" &#125; &#125; if [type] == "securelog" &#123; elasticsearch &#123; hosts =&gt; ["192.168.1.31:9200"] index =&gt; "secure-log-%&#123;+YYYY.MM.dd&#125;" &#125; &#125;&#125; 2）给日志文件赋予可读权限并重启logstash123[root@linux-elk1 ~]# chmod 644 /var/log/secure [root@linux-elk1 ~]# chmod 644 /var/log/messages[root@linux-elk1 ~]# systemctl restart logstash 3）向被收集的文件中写入数据；是为了马上能在elasticsearch的web界面和klbana的web界面里面查看到数据。12[root@linux-elk1 ~]# echo "test" &gt;&gt; /var/log/secure [root@linux-elk1 ~]# echo "test" &gt;&gt; /var/log/messages 4）在kibana界面添加system-log索引模式5）在kibana界面添加secure-log索引模式6）kibana查看日志 收集tomcat和java日志 收集Tomcat服务器的访问日志以及Tomcat错误日志进行实时统计，在kibana页面进行搜索展示，每台Tomcat服务器需要安装logstash负责收集日志，然后将日志发给elasticsearch进行分析，在通过kibana在前端展示。 部署tomcat服务 说明，我这里在linux-elk2节点上面装tomcat 1）下载并安装tomcat1234[root@linux-elk2 ~]# cd /usr/local/[root@linux-elk2 local]# wget http://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-9/v9.0.21/bin/apache-tomcat-9.0.21.tar.gz[root@linux-elk2 local]# tar xvzf apache-tomcat-9.0.21.tar.gz[root@linux-elk2 local]# ln -s /usr/local/apache-tomcat-9.0.21 /usr/local/tomcat 2）测试页面准备123[root@linux-elk2 local]# cd /usr/local/tomcat/webapps/[root@linux-elk2 webapps]# mkdir webdir[root@linux-elk2 webapps]# echo "&lt;h1&gt;Welcome to Tomcat&lt;/h1&gt;" &gt; /usr/local/tomcat/webapps/webdir/index.html 3）tomcat日志转json1234[root@linux-elk2 tomcat]# vim /usr/local/tomcat/conf/server.xml &lt;Valve className="org.apache.catalina.valves.AccessLogValve" directory="logs" prefix="localhost_access_log" suffix=".txt" pattern="&#123;&amp;quot;clientip&amp;quot;:&amp;quot;%h&amp;quot;,&amp;quot;ClientUser&amp;quot;:&amp;quot;%l&amp;quot;,&amp;quot;authenticated&amp;quot;:&amp;quot;%u&amp;quot;,&amp;quot;AccessTime&amp;quot;:&amp;quot;%t&amp;quot;,&amp;quot;method&amp;quot;:&amp;quot;%r&amp;quot;,&amp;quot;status&amp;quot;:&amp;quot;%s&amp;quot;,&amp;quot;SendBytes&amp;quot;:&amp;quot;%b&amp;quot;,&amp;quot;Query?string&amp;quot;:&amp;quot;%q&amp;quot;,&amp;quot;partner&amp;quot;:&amp;quot;%&#123;Referer&#125;i&amp;quot;,&amp;quot;AgentVersion&amp;quot;:&amp;quot;%&#123;User-Agent&#125;i&amp;quot;&#125;"/&gt; 4）启动tomcat，并进行访问测试生成日志1234567891011121314[root@linux-elk2 tomcat]# /usr/local/tomcat/bin/startup.sh[root@linux-elk2 tomcat]# ss -nlt |grep 8080LISTEN 0 100 :::8080 :::*[root@linux-elk2 tomcat]# ab -n100 -c100 http://192.168.1.32:8080/webdir/[root@linux-elk2 ~]# tailf /usr/local/tomcat/logs/localhost_access_log.2019-07-05.log &#123;"clientip":"192.168.1.32","ClientUser":"-","authenticated":"-","AccessTime":"[05/Jul/2019:16:39:18 +0800]","method":"GET /webdir/ HTTP/1.0","status":"200","SendBytes":"27","Query?string":"","partner":"-","AgentVersion":"ApacheBench/2.3"&#125;&#123;"clientip":"192.168.1.32","ClientUser":"-","authenticated":"-","AccessTime":"[05/Jul/2019:16:39:18 +0800]","method":"GET /webdir/ HTTP/1.0","status":"200","SendBytes":"27","Query?string":"","partner":"-","AgentVersion":"ApacheBench/2.3"&#125;&#123;"clientip":"192.168.1.32","ClientUser":"-","authenticated":"-","AccessTime":"[05/Jul/2019:16:39:18 +0800]","method":"GET /webdir/ HTTP/1.0","status":"200","SendBytes":"27","Query?string":"","partner":"-","AgentVersion":"ApacheBench/2.3"&#125;&#123;"clientip":"192.168.1.32","ClientUser":"-","authenticated":"-","AccessTime":"[05/Jul/2019:16:39:18 +0800]","method":"GET /webdir/ HTTP/1.0","status":"200","SendBytes":"27","Query?string":"","partner":"-","AgentVersion":"ApacheBench/2.3"&#125;&#123;"clientip":"192.168.1.32","ClientUser":"-","authenticated":"-","AccessTime":"[05/Jul/2019:16:39:18 +0800]","method":"GET /webdir/ HTTP/1.0","status":"200","SendBytes":"27","Query?string":"","partner":"-","AgentVersion":"ApacheBench/2.3"&#125;&#123;"clientip":"192.168.1.32","ClientUser":"-","authenticated":"-","AccessTime":"[05/Jul/2019:16:39:18 +0800]","method":"GET /webdir/ HTTP/1.0","status":"200","SendBytes":"27","Query?string":"","partner":"-","AgentVersion":"ApacheBench/2.3"&#125;&#123;"clientip":"192.168.1.32","ClientUser":"-","authenticated":"-","AccessTime":"[05/Jul/2019:16:39:18 +0800]","method":"GET /webdir/ HTTP/1.0","status":"200","SendBytes":"27","Query?string":"","partner":"-","AgentVersion":"ApacheBench/2.3"&#125;&#123;"clientip":"192.168.1.32","ClientUser":"-","authenticated":"-","AccessTime":"[05/Jul/2019:16:39:18 +0800]","method":"GET /webdir/ HTTP/1.0","status":"200","SendBytes":"27","Query?string":"","partner":"-","AgentVersion":"ApacheBench/2.3"&#125; 5）验证日志是否为json格式，http://www.kjson.com/ 配置logstash收集tomcat日志 说明：如果是需要收集别的服务器上面的tomcat日志，那么在所需要收集的服务器上面都得安装logstash。此处是在linux-elk2节点上面部署的tomcat，之前安装过logstash。 1）配置logstash12345678910111213141516171819[root@linux-elk2 ~]# vim /etc/logstash/conf.d/tomcat.confinput &#123; file &#123; path =&gt; "/usr/local/tomcat/logs/localhost_access_log.*.log" type =&gt; "tomcat-access-log" start_position =&gt; "beginning" stat_interval =&gt; "2" &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; ["192.168.1.31:9200"] index =&gt; "logstash-tomcat-132-accesslog-%&#123;+YYYY.MM.dd&#125;" &#125; file &#123; path =&gt; "/tmp/logstash-tomcat-132-accesslog-%&#123;+YYYY.MM.dd&#125;" &#125;&#125; 2）检测配置文件语法，并重启logstash1234567[root@linux-elk2 ~]# /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/tomcat.conf -tWARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaultsCould not find log4j2 configuration at path /usr/share/logstash/config/log4j2.properties. Using default config which logs errors to the console[WARN ] 2019-07-05 17:04:34.583 [LogStash::Runner] multilocal - Ignoring the 'pipelines.yml' file because modules or command line options are specifiedConfiguration OK[root@linux-elk2 ~]# /usr/share/logstash/bin/system-install /etc/logstash/startup.options systemd[root@linux-elk2 ~]# systemctl start logstash 3）权限修改，不然elasticsearch界面和kibana界面是无法查看到的12345678910111213141516171819[root@linux-elk2 ~]# ll /usr/local/tomcat/logs/ -ddrwxr-xr-x 2 root root 197 7月 5 16:36 /usr/local/tomcat/logs/[root@linux-elk2 ~]# ll /usr/local/tomcat/logs/总用量 64-rw-r----- 1 root root 14228 7月 5 16:36 catalina.2019-07-05.log-rw-r----- 1 root root 14228 7月 5 16:36 catalina.out-rw-r----- 1 root root 0 7月 5 16:25 host-manager.2019-07-05.log-rw-r----- 1 root root 1074 7月 5 16:36 localhost.2019-07-05.log-rw-r----- 1 root root 26762 7月 5 17:23 localhost_access_log.2019-07-05.log-rw-r----- 1 root root 0 7月 5 16:25 manager.2019-07-05.log[root@linux-elk2 ~]# chown logstash.logstash /usr/local/tomcat/logs/ -R[root@linux-elk2 ~]# ll /usr/local/tomcat/logs/总用量 64-rw-r----- 1 logstash logstash 14228 7月 5 16:36 catalina.2019-07-05.log-rw-r----- 1 logstash logstash 14228 7月 5 16:36 catalina.out-rw-r----- 1 logstash logstash 0 7月 5 16:25 host-manager.2019-07-05.log-rw-r----- 1 logstash logstash 1074 7月 5 16:36 localhost.2019-07-05.log-rw-r----- 1 logstash logstash 26762 7月 5 17:23 localhost_access_log.2019-07-05.log-rw-r----- 1 logstash logstash 0 7月 5 16:25 manager.2019-07-05.log 4）访问elasticsearch界面验证插件数据浏览 5）在kibana上添加索引模式6）kibana验证数据 配置logstash收集java日志 使用codec的multiline插件实现多行匹配，这是一个可以将多行进行合并的插件，而且可以使用what指定将匹配到的行与前面的行合并还是和后面的行合并，https://www.elastic.co/guide/en/logstash/current/plugins-codecs-multiline.html 语法格式：123456789input &#123; stdin &#123; codec =&gt; multiline &#123; pattern =&gt; "^\[" #当遇到[开头的行时候将多行进行合并 negate =&gt; true #true为匹配成功进行操作，false为不成功进行操 what =&gt; "previous" #与上面的行合并，如果是下面的行合并就是 &#125; &#125;&#125; 命令行测试输入输出：12345678910111213141516171819202122232425262728293031323334[root@linux-elk2 ~]# /usr/share/logstash/bin/logstash -e 'input &#123; stdin &#123; codec =&gt; multiline &#123; pattern =&gt; "^\[" negate =&gt; true what =&gt; "previous" &#125; &#125; &#125; output &#123; stdout &#123; codec =&gt; rubydebug &#125;&#125;'WARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaultsCould not find log4j2 configuration at path /usr/share/logstash/config/log4j2.properties. Using default config which logs errors to the console[WARN ] 2019-07-08 15:28:04.938 [LogStash::Runner] multilocal - Ignoring the 'pipelines.yml' file because modules or command line options are specified[INFO ] 2019-07-08 15:28:04.968 [LogStash::Runner] runner - Starting Logstash &#123;"logstash.version"=&gt;"6.8.1"&#125;[INFO ] 2019-07-08 15:28:19.167 [Converge PipelineAction::Create&lt;main&gt;] pipeline - Starting pipeline &#123;:pipeline_id=&gt;"main", "pipeline.workers"=&gt;1, "pipeline.batch.size"=&gt;125, "pipeline.batch.delay"=&gt;50&#125;[INFO ] 2019-07-08 15:28:19.918 [Converge PipelineAction::Create&lt;main&gt;] pipeline - Pipeline started successfully &#123;:pipeline_id=&gt;"main", :thread=&gt;"#&lt;Thread:0xc8dd9a1 run&gt;"&#125;The stdin plugin is now waiting for input:111111222222aaaaaa[44444&#123; "@timestamp" =&gt; 2019-07-08T07:34:48.063Z, "tags" =&gt; [ [0] "multiline" ], "@version" =&gt; "1", "message" =&gt; "[12\n111111\n222222\naaaaaa", "host" =&gt; "linux-elk2.exmaple.com"&#125;444444aaaaaa[77777&#123; "@timestamp" =&gt; 2019-07-08T07:35:51.522Z, "tags" =&gt; [ [0] "multiline" ], "@version" =&gt; "1", "message" =&gt; "[44444\n444444\naaaaaa", "host" =&gt; "linux-elk2.exmaple.com"&#125; 示例：收集ELK集群日志1）观察日志文件，elk集群日志都是以&quot;[&quot;开头并且每一个信息都是如此。1234567891011[root@linux-elk2 ~]# tailf /elk/logs/ELK-Cluster.log [2019-07-08T11:26:37,774][INFO ][o.e.c.m.MetaDataIndexTemplateService] [elk-node2] adding template [kibana_index_template:.kibana] for index patterns [.kibana][2019-07-08T11:26:47,664][INFO ][o.e.c.m.MetaDataIndexTemplateService] [elk-node2] adding template [kibana_index_template:.kibana] for index patterns [.kibana][2019-07-08T11:33:55,150][INFO ][o.e.c.m.MetaDataIndexTemplateService] [elk-node2] adding template [kibana_index_template:.kibana] for index patterns [.kibana][2019-07-08T11:33:55,197][INFO ][o.e.c.m.MetaDataMappingService] [elk-node2] [.kibana_1/yRee-8HYS8KiVwnuADXAbA] update_mapping [doc][2019-07-08T11:33:55,822][INFO ][o.e.c.m.MetaDataIndexTemplateService] [elk-node2] adding template [kibana_index_template:.kibana] for index patterns [.kibana][2019-07-08T11:33:55,905][INFO ][o.e.c.m.MetaDataMappingService] [elk-node2] [.kibana_1/yRee-8HYS8KiVwnuADXAbA] update_mapping [doc][2019-07-08T11:33:57,026][INFO ][o.e.c.m.MetaDataIndexTemplateService] [elk-node2] adding template [kibana_index_template:.kibana] for index patterns [.kibana][2019-07-08T11:43:20,262][WARN ][o.e.m.j.JvmGcMonitorService] [elk-node2] [gc][young][8759][66] duration [1.3s], collections [1]/[1.7s], total [1.3s]/[4s], memory [176mb]-&gt;[111.6mb]/[1.9gb], all_pools &#123;[young] [64.8mb]-&gt;[706.4kb]/[66.5mb]&#125;&#123;[survivor] [3.3mb]-&gt;[3mb]/[8.3mb]&#125;&#123;[old] [107.8mb]-&gt;[107.8mb]/[1.9gb]&#125;[2019-07-08T11:43:20,388][WARN ][o.e.m.j.JvmGcMonitorService] [elk-node2] [gc][8759] overhead, spent [1.3s] collecting in the last [1.7s][2019-07-08T11:44:42,955][INFO ][o.e.x.m.p.NativeController] [elk-node2] Native controller process has stopped - no new native processes can be started 2）配置logstash1234567891011121314151617181920212223[root@linux-elk2 ~]# vim /etc/logstash/conf.d/java.confinput &#123; file &#123; path =&gt; "/elk/logs/ELK-Cluster.log" type =&gt; "java-elk-cluster-log" start_position =&gt; "beginning" stat_interval =&gt; "2" code =&gt; multiline &#123; pattern =&gt; "^\[" #以"["开头进行正则匹配，匹配规则 negate =&gt; "true" #正则匹配成功，false匹配不成功 what =&gt; "previous" #和前面的内容进行合并,如果是和下面的合并就是next &#125; &#125;&#125;output &#123; if [type] == "java-elk-cluster-log" &#123; elasticsearch &#123; hosts =&gt; ["192.168.1.31:9200"] index =&gt; "java-elk-cluster-log-%&#123;+YYYY.MM.dd&#125;" &#125; &#125;&#125; 3）检查配置文件语法是否有误并重启logstash12345678[root@linux-elk2 ~]# /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/java.conf -tWARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaultsCould not find log4j2 configuration at path /usr/share/logstash/config/log4j2.properties. Using default config which logs errors to the console[WARN ] 2019-07-08 15:49:51.996 [LogStash::Runner] multilocal - Ignoring the 'pipelines.yml' file because modules or command line options are specifiedConfiguration OK[INFO ] 2019-07-08 15:50:04.438 [LogStash::Runner] runner - Using config.test_and_exit mode. Config Validation Result: OK. Exiting Logstash[root@linux-elk2 ~]# systemctl restart logstash 4）访问elasticsearch界面验证数据5）在kibana上添加索引验证模式6）kibana验证数据 收集Nginx访问日志 收集nginx的json访问日志，这里为了测试，是在一台新的服务器上面安装了nginx和logstash 1）安装nginx并准备一个测试页面12345[root@node01 ~]# yum -y install nginx[root@node01 ~]# echo "&lt;h1&gt;whelcom to nginx server&lt;/h1&gt;" &gt; /usr/share/nginx/html/index.html[root@node01 ~]# systemctl start nginx [root@node01 ~]# curl localhost&lt;h1&gt;whelcom to nginx server&lt;/h1&gt; 2）将nginx日志转换成json格式123456789101112131415161718192021[root@node01 ~]# vim /etc/nginx/nginx.conf log_format access_json '&#123;"@timestamp":"$time_iso8601",' '"host":"$server_addr",' '"clientip":"$remote_addr",' '"size":$body_bytes_sent,' '"responsetime":$request_time,' '"upstreamtime":"$upstream_response_time",' '"upstreamhost":"$upstream_addr",' '"http_host":"$host",' '"url":"$uri",' '"domain":"$host",' '"xff":"$http_x_forwarded_for",' '"referer":"$http_referer",' '"status":"$status"&#125;'; access_log /var/log/nginx/access.log access_json;[root@node01 ~]# nginx -tnginx: the configuration file /etc/nginx/nginx.conf syntax is oknginx: configuration file /etc/nginx/nginx.conf test is successful[root@node01 ~]# systemctl restart nginx 3）访问一次，确认日志为json格式 12[root@node01 ~]# tail /var/log/nginx/access.log&#123;"@timestamp":"2019-07-09T11:21:28+08:00","host":"192.168.1.30","clientip":"192.168.1.144","size":33,"responsetime":0.000,"upstreamtime":"-","upstreamhost":"-","http_host":"192.168.1.30","url":"/index.html","domain":"192.168.1.30","xff":"-","referer":"-","status":"200"&#125; 4）安装logstash并配置收集nginx日志12345678910111213141516171819202122232425262728293031#将logstash软件包copy到nginx服务器上[root@linux-elk1 ~]# scp logstash-6.8.1.rpm 192.168.1.30:/root/#安装logstash[root@node01 ~]# yum -y localinstall logstash-6.8.1.rpm#生成logstash.service启动文件[root@node01 ~]# /usr/share/logstash/bin/system-install /etc/logstash/startup.options systemd#将logstash启动用户更改为root，不然可能会导致收集不到日志[root@node01 ~]# vim /etc/systemd/system/logstash.serviceUser=rootGroup=root[root@node01 ~]# systemctl daemon-reload[root@node01 ~]# vim /etc/logstash/conf.d/nginx.confinput &#123; file &#123; path =&gt; "/var/log/nginx/access.log" type =&gt; "nginx-accesslog" start_position =&gt; "beginning" stat_interval =&gt; "2" codec =&gt; json &#125;&#125;output &#123; if [type] == "nginx-accesslog" &#123; elasticsearch &#123; hosts =&gt; ["192.168.1.31:9200"] index =&gt; "logstash-nginx-accesslog-30-%&#123;+YYYY.MM.dd&#125;" &#125; &#125;&#125; 5）检查配置文件语法是否有误并重启logstash12345678[root@node01 ~]# /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/nginx.conf -tWARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaultsCould not find log4j2 configuration at path /usr/share/logstash/config/log4j2.properties. Using default config which logs errors to the console[WARN ] 2019-07-09 11:26:04.277 [LogStash::Runner] multilocal - Ignoring the 'pipelines.yml' file because modules or command line options are specifiedConfiguration OK[INFO ] 2019-07-09 11:26:09.055 [LogStash::Runner] runner - Using config.test_and_exit mode. Config Validation Result: OK. Exiting Logstash[root@node01 ~]# systemctl restart logstash 6）在kibana上添加索引验证模式 7）在kibana上验证数据,可以通过添加筛选，让日志更加了然名目 收集TCP/UDP日志 通过logstash的tcp/udp插件收集日志，通常用于在向elasticsearch日志补录丢失的部分日志，可以将丢失的日志通过一个TCP端口直接写入到elasticsearch服务器。 进行收集测试1）logstash配置1234567891011121314[root@linux-elk1 ~]# vim /etc/logstash/conf.d/tcp.confinput &#123; tcp &#123; port =&gt; 9889 type =&gt; "tcplog" mode =&gt; "server" &#125;&#125;output &#123; stdout &#123; codec =&gt; rubydebug &#125;&#125; 2）验证端口是否启动成功123456789101112131415[root@linux-elk1 ~]# /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/tcp.conf WARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaultsCould not find log4j2 configuration at path /usr/share/logstash/config/log4j2.properties. Using default config which logs errors to the console[WARN ] 2019-07-09 18:12:07.538 [LogStash::Runner] multilocal - Ignoring the 'pipelines.yml' file because modules or command line options are specified[INFO ] 2019-07-09 18:12:07.551 [LogStash::Runner] runner - Starting Logstash &#123;"logstash.version"=&gt;"6.8.1"&#125;[INFO ] 2019-07-09 18:12:14.416 [Converge PipelineAction::Create&lt;main&gt;] pipeline - Starting pipeline &#123;:pipeline_id=&gt;"main", "pipeline.workers"=&gt;2, "pipeline.batch.size"=&gt;125, "pipeline.batch.delay"=&gt;50&#125;[INFO ] 2019-07-09 18:12:14.885 [Converge PipelineAction::Create&lt;main&gt;] pipeline - Pipeline started successfully &#123;:pipeline_id=&gt;"main", :thread=&gt;"#&lt;Thread:0x240c27a6 sleep&gt;"&#125;[INFO ] 2019-07-09 18:12:14.911 [[main]&lt;tcp] tcp - Starting tcp input listener &#123;:address=&gt;"0.0.0.0:9889", :ssl_enable=&gt;"false"&#125;[INFO ] 2019-07-09 18:12:14.953 [Ruby-0-Thread-1: /usr/share/logstash/lib/bootstrap/environment.rb:6] agent - Pipelines running &#123;:count=&gt;1, :running_pipelines=&gt;[:main], :non_running_pipelines=&gt;[]&#125;[INFO ] 2019-07-09 18:12:15.223 [Api Webserver] agent - Successfully started Logstash API endpoint &#123;:port=&gt;9600&#125;# 新开一个终端验证端口[root@linux-elk1 ~]# netstat -nlutp |grep 9889tcp6 0 0 :::9889 :::* LISTEN 112455/java 3）在别的服务器通过nc命令进行测试，查看logstash是否收到数据123456789101112# echo "nc test" | nc 192.168.1.31 9889 #在另外一台服务器上执行# 在上面启动logstash的那个终端查看&#123; "message" =&gt; "nc test", "host" =&gt; "192.168.1.30", "type" =&gt; "tcplog", "@version" =&gt; "1", "@timestamp" =&gt; 2019-07-09T10:16:48.139Z, "port" =&gt; 37102&#125; 4）通过nc命令发送一个文件，查看logstash收到的数据12345678910111213141516171819# nc 192.168.1.31 9889 &lt; /etc/passwd #同样在上面执行nc那台服务器上执行# 同样还是在上面启动logstash的那个终端查看&#123; "message" =&gt; "mysql:x:27:27:MariaDB Server:/var/lib/mysql:/sbin/nologin", "host" =&gt; "192.168.1.30", "type" =&gt; "tcplog", "@version" =&gt; "1", "@timestamp" =&gt; 2019-07-09T10:18:29.186Z, "port" =&gt; 37104&#125;&#123; "message" =&gt; "logstash:x:989:984:logstash:/usr/share/logstash:/sbin/nologin", "host" =&gt; "192.168.1.30", "type" =&gt; "tcplog", "@version" =&gt; "1", "@timestamp" =&gt; 2019-07-09T10:18:29.187Z, "port" =&gt; 37104&#125; 5）通过伪设备的方式发送消息：在类Unix操作系统中，设备节点并不一定要对应物理设备。没有这种对应关系的设备是伪设备。操作系统运用了它们提供的多种功能，tcp只是dev下面众多伪设备当中的一种设备。1234567891011# echo "伪设备" &gt;/dev/tcp/192.168.1.31/9889 #同样在上面执行nc那台服务器上执行# 同样还是在上面启动logstash的那个终端查看&#123; "message" =&gt; "伪设备", "host" =&gt; "192.168.1.30", "type" =&gt; "tcplog", "@version" =&gt; "1", "@timestamp" =&gt; 2019-07-09T10:21:32.487Z, "port" =&gt; 37106&#125; 6）将输出更改到elasticsearch123456789101112131415[root@linux-elk1 ~]# vim /etc/logstash/conf.d/tcp.confinput &#123; tcp &#123; port =&gt; 9889 type =&gt; "tcplog" mode =&gt; "server" &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; ["192.168.1.31:9200"] index =&gt; "logstash-tcp-log-%&#123;+YYYY.MM.dd&#125;" &#125;&#125; 7）通过nc命令或伪设备输入日志12# echo "伪设备 1" &gt;/dev/tcp/192.168.1.31/9889# echo "伪设备 2" &gt;/dev/tcp/192.168.1.31/9889 8）在kibana界面创建索引模式9）验证数据]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK快速入门一-基本部署]]></title>
    <url>%2F2019%2F07%2F05%2FELK%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E4%B8%80-%E5%9F%BA%E6%9C%AC%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[ELK简介 什么是ELK？通俗来讲，ELK是由Elasticsearch、Logstash、Kibana 三个开源软件组成的一个组合体，这三个软件当中，每个软件用于完成不同的功能，ELK又称ELKstack，官网 https://www.elastic.co/ ， ELK主要优点有如下几个：1、处理方式灵活：elasticsearch是实时全文索引，具有强大的搜索功能2、配置相对简单：elasticsearch全部使用JSON接口，logstash使用模块配置，kibana的配置文件部分更简单3、检索性能高：基于优秀的设计，虽然每次查询都是实时，但是也可以达到百亿级数据的查询秒级响应4、集群线性扩展：elasticsearch和logstash都可以灵活线性扩展5、前端操作绚丽：kibana的前端设计比较绚丽，而且操作简单 Elasticsearch elasticsearch是一个高度可扩展全文搜索和分析引擎，基于Apache Lucene 构建，能对大容量的数据进行接近实时的存储、搜索和分析操作，可以处理大规模日志数据，比如Nginx、Tomcat、系统日志等功能。 Logstash 数据收集引擎。它支持动态的从各种数据源搜集数据，并对数据进行过滤、分析、丰富、统一格式等操作，然后存储到用户指定的位置；支持普通log、自定义json格式的日志解析。 Kibana 数据分析和可视化平台。通常与 Elasticsearch 配合使用，对其中数据进行搜索、分析和以统计图表的方式展示。 ELK部署环境准备 这里实验所使用系统CentOS 7.4 x86_64，服务器信息如下。并关闭防火墙和selinux，及host绑定等。本文所使用所有的软件包 下载 提取码：ow1b IPAddr HostName Mem 192.168.1.31 linux-elk1.exmaple.com 3G 192.168.1.32 linux-elk2.exmaple.com 3G 12epel源配置# wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo Elasticsearch部署 因为elasticsearch服务运行需要java环境，因此两台elasticsearch服务器需要安装java环境。 安装JDK centos7默认是安装了jdk，如果需要安装高版本可以使用一下步骤，这里使用下面的yum安装jdk 1.8.0_211 。注意：两个节点都要安装。 123456789101112131415161718192021方法一：yum安装下载好的JDK包，将下载好的软件包上传到服务器进行安装，首先卸载自带的jdk；再进行安装。下载地址：https://pan.baidu.com/s/1VK1iCnvouppZ06jsVBOaRw 提取码：lofc[root@linux-elk1 ~]# rpm -qa |grep jdk |xargs yum -y remove &#123;&#125;\;[root@linux-elk1 ~]# yum -y localinstall jdk-8u211-linux-x64.rpm[root@linux-elk1 ~]# java -versionjava version "1.8.0_211"Java(TM) SE Runtime Environment (build 1.8.0_211-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode)方法二：源码安装JDK，将下载的软件包上传到服务器进行安装。下载地址：https://pan.baidu.com/s/1AAPyPzhdclNNCb0m6ooVYQ 提取码：x18u[root@linux-elk1 ~]# tar xf jdk-8u211-linux-x64.tar.gz -C /usr/local/[root@linux-elk1 ~]# ln -s /usr/local/jdk1.8.0_211 /usr/local/java[root@linux-elk1 ~]# sed -i.ori '$a export JAVA_HOME=/usr/local/java \nexport PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH \nexport CLASSPATH=.$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$JAVA_HOME/lib/tools.jar' /etc/profile[root@linux-elk1 ~]# source /etc/profile[root@linux-elk1 ~]# java -versionjava version "1.8.0_211"Java(TM) SE Runtime Environment (build 1.8.0_211-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode) 安装Elasticsearch 两台节点都需要安装elasticsearch，使用yum安装会很慢，所以先下载下来传到服务器进行安装，官网下载地址：https://www.elastic.co/cn/downloads/past-releases#elasticsearch本文所使用的包下载：https://pan.baidu.com/s/1djYOs3PQjtq16VkPMETAWg 提取码：b15v 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364将下载的elasticsearch包上传到服务器进行安装。[root@linux-elk1 ~]# yum -y localinstall elasticsearch-6.8.1.rpm[root@linux-elk2 ~]# yum -y localinstall elasticsearch-6.8.1.rpm配置elasticsearch，linux-elk2配置一个相同的节点，通过组播进行通信，如果无法通过组播查询，修改成单播即可。[root@linux-elk1 ~]# vim /etc/elasticsearch/elasticsearch.ymlcluster.name: ELK-Cluster #ELK的集群名称，名称相同即属于是同一个集群node.name: elk-node1 #本机在集群内的节点名称path.data: /elk/data #数据存放目录path.logs: /elk/logs #日志保存目录bootstrap.memory_lock: true #服务启动的时候锁定足够的内存，防止数据写入swapnetwork.host: 192.168.1.31 #监听的IP地址http.port: 9200 #服务监听的端口discovery.zen.ping.unicast.hosts: ["192.168.1.31", "192.168.1.32"] #单播配置一台即可修改内存限制，内存锁定需要进行配置需要2g以上内存，否则会导致无法启动elasticsearch。[root@linux-elk1 ~]# vim /usr/lib/systemd/system/elasticsearch.service# 在[Service]下加入下面这行内容LimitMEMLOCK=infinity[root@linux-elk1 ~]# systemctl daemon-reload[root@linux-elk1 ~]# vim /etc/elasticsearch/jvm.options-Xms2g-Xmx2g #最小和最大内存限制，为什么最小和最大设置一样大？参考：https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html创建数据目录和日志目录及权限修改[root@linux-elk1 ~]# mkdir -p /elk/&#123;data,logs&#125;[root@linux-elk1 ~]# chown elasticsearch.elasticsearch /elk/ -R启动elasticsearch及检查端口是否处于监听状态[root@linux-elk1 ~]# systemctl start elasticsearch[root@linux-elk1 ~]# netstat -nltup |grep javatcp6 0 0 192.168.1.31:9200 :::* LISTEN 12887/java tcp6 0 0 192.168.1.31:9300 :::* LISTEN 12887/java将配置文件copy到linux-elk2上面并进行修改，配置启动等。[root@linux-elk1 ~]# scp /etc/elasticsearch/elasticsearch.yml 192.168.1.32:/etc/elasticsearch/elasticsearch.yml[root@linux-elk2 ~]# grep ^[a-Z] /etc/elasticsearch/elasticsearch.yml cluster.name: ELK-Clusternode.name: elk-node2path.data: /elk/datapath.logs: /elk/logsbootstrap.memory_lock: truenetwork.host: 192.168.1.32http.port: 9200discovery.zen.ping.unicast.hosts: ["192.168.1.31", "192.168.1.32"][root@linux-elk2 ~]# vim /usr/lib/systemd/system/elasticsearch.service# 在[Service]下加入下面这行内容LimitMEMLOCK=infinity[root@linux-elk2 ~]# systemctl daemon-reload[root@linux-elk2 ~]# vim /etc/elasticsearch/jvm.options-Xms2g-Xmx2g[root@linux-elk2 ~]# mkdir -p /elk/&#123;data,logs&#125;[root@linux-elk2 ~]# chown elasticsearch.elasticsearch /elk/ -R[root@linux-elk2 ~]# systemctl start elasticsearch[root@linux-elk2 ~]# netstat -nltup |grep javatcp6 0 0 192.168.1.32:9200 :::* LISTEN 18667/java tcp6 0 0 192.168.1.32:9300 :::* LISTEN 18667/java 通过浏览器访问elasticsearch端口 监控elasticsearch集群状态 通过shell命令获取集群状态，这里获取到的是一个json格式的返回值，例如对status进行分析，如果等于green(绿色)就是运行在正常，等于yellow(黄色)表示副本分片丢失，red(红色)表示主分片丢失。12345678910111213141516171819202122232425262728293031323334353637[root@linux-elk1 ~]# curl http://192.168.1.31:9200/_cluster/health?pretty=true&#123; "cluster_name" : "ELK-Cluster", "status" : "green", "timed_out" : false, "number_of_nodes" : 2, "number_of_data_nodes" : 2, "active_primary_shards" : 0, "active_shards" : 0, "relocating_shards" : 0, "initializing_shards" : 0, "unassigned_shards" : 0, "delayed_unassigned_shards" : 0, "number_of_pending_tasks" : 0, "number_of_in_flight_fetch" : 0, "task_max_waiting_in_queue_millis" : 0, "active_shards_percent_as_number" : 100.0&#125;[root@linux-elk1 ~]# curl http://192.168.1.32:9200/_cluster/health?pretty=true&#123; "cluster_name" : "ELK-Cluster", "status" : "green", "timed_out" : false, "number_of_nodes" : 2, "number_of_data_nodes" : 2, "active_primary_shards" : 0, "active_shards" : 0, "relocating_shards" : 0, "initializing_shards" : 0, "unassigned_shards" : 0, "delayed_unassigned_shards" : 0, "number_of_pending_tasks" : 0, "number_of_in_flight_fetch" : 0, "task_max_waiting_in_queue_millis" : 0, "active_shards_percent_as_number" : 100.0&#125; 安装elasticsearch插件head 我们不可能经常通过命令来查看集群的信息，所以就使用到了插件 –head。件是为了完成不同的功能，官方提供了一些插件但大部分是收费的，另外也有一些开发爱好者提供的插件，可以实现对elasticsearch集群的状态监控与管理配置等功能。head：主要用来做集群管理的插件下载地址：https://github.com/mobz/elasticsearch-head 1）安装1234567891011121314151617181920212223242526272829303132333435363738# 安装npm和git[root@linux-elk1 ~]# yum -y install npm git# 安装elasticsearch-head插件[root@linux-elk1 ~]# cd /usr/local/src/[root@linux-elk1 src]# git clone git://github.com/mobz/elasticsearch-head.git[root@linux-elk1 src]# cd elasticsearch-head/[root@linux-elk1 elasticsearch-head]# npm install grunt -save --registry=https://registry.npm.taobao.org[root@linux-elk1 elasticsearch-head]# ll node_modules/grunt #确定该目录有生成文件总用量 24drwxr-xr-x. 2 root root 19 4月 6 2016 bin-rw-r--r--. 1 root root 7111 4月 6 2016 CHANGELOGdrwxr-xr-x. 4 root root 47 7月 4 09:21 lib-rw-r--r--. 1 root root 1592 3月 23 2016 LICENSEdrwxr-xr-x. 5 root root 50 7月 4 09:21 node_modules-rw-r--r--. 1 root root 4108 7月 4 09:21 package.json-rw-r--r--. 1 root root 878 2月 12 2016 README.md[root@linux-elk1 elasticsearch-head]# npm install --registry=https://registry.npm.taobao.org #执行安装[root@linux-elk1 elasticsearch-head]# npm run start &amp; #后台启动服务[root@linux-elk1 ~]# ss -nlt |grep 9100LISTEN 0 128 *:9100 *:* #------------------------补充说明------------------------由于上面npm安装时候超级慢，使用taobao源同样慢，这里将已安装的打成了包，可以直接下载使用即可下载地址：https://pan.baidu.com/s/16zDlecKVfmkEeInPcRx9NQ 提取码：h890[root@linux-elk1 ~]# yum -y install npm[root@linux-elk1 ~]# cd /usr/local/src/[root@linux-elk1 src]# lselasticsearch-head.tar.gz[root@linux-elk1 src]# tar xvzf elasticsearch-head.tar.gz[root@linux-elk1 src]# cd elasticsearch-head/[root@linux-elk1 elasticsearch-head]# npm run start &amp;#--------------------------------------------------------# 修改elasticsearch服务配置文件，开启跨域访问支持，然后重启elasticsearch服务[root@linux-elk1 ~]# vim /etc/elasticsearch/elasticsearch.ymlhttp.cors.enabled: true #最下方添加http.cors.allow-origin: "*" 为了方便管理elasticsearch-head插件，编写一个启动脚本123456789101112131415161718192021222324252627282930313233[root@linux-elk1 ~]# vim /usr/bin/elasticsearch-head#!/bin/bash#desc: elasticsearch-head service manager#date: 2019data="cd /usr/local/src/elasticsearch-head/; nohup npm run start &gt; /dev/null 2&gt;&amp;1 &amp; "function START ()&#123; eval $data &amp;&amp; echo -e "elasticsearch-head start\033[32m ok\033[0m"&#125;function STOP ()&#123; ps -ef |grep grunt |grep -v "grep" |awk '&#123;print $2&#125;' |xargs kill -s 9 &gt; /dev/null &amp;&amp; echo -e "elasticsearch-head stop\033[32m ok\033[0m"&#125;case "$1" in start) START ;; stop) STOP ;; restart) STOP sleep 3 START ;; *) echo "Usage: elasticsearch-head (start|stop|restart)" ;;esac[root@linux-elk1 ~]# chmod +x /usr/bin/elasticsearch-head 2）浏览器访问9100端口，将连接地址修改为elasticsearch地址。 3）测试提交数据 4）验证索引是否存在 5）查看数据 6）Master和Slave的区别： Master的职责：统计各node节点状态信息、集群状态信息统计、索引的创建和删除、索引分配的管理、关闭node节点等Savle的职责：同步数据、等待机会称为Master Logstash部署 Logstash 是一个开源的数据收集引擎，可以水平伸缩，而且logstash是整个ELK当中拥有最多插件的一个组件，其可以接收来自不同来源的数据并同意输出到指定的且可以是多个不同目的地。官网下载地址：https://www.elastic.co/cn/downloads/past-releases#logstash 安装logstash12[root@linux-elk1 ~]# wget https://artifacts.elastic.co/downloads/logstash/logstash-6.8.1.rpm[root@linux-elk1 ~]# yum -y localinstall logstash-6.8.1.rpm 测试logstash是否正常1）测试标准输入输出123456789[root@linux-elk1 ~]# /usr/share/logstash/bin/logstash -e 'input &#123; stdin &#123;&#125; &#125; output &#123; stdout &#123; codec =&gt; rubydebug&#125; &#125;' hello world #输入&#123; "@version" =&gt; "1", #事件版本号，一个事件就是一个ruby对象 "@timestamp" =&gt; 2019-07-04T04:30:35.106Z, #当前事件发生的事件 "host" =&gt; "linux-elk1.exmaple.com", #标记事件发生在哪里 "message" =&gt; "hello world" #消息的具体内容&#125; 2）测试输出到文件123456[root@linux-elk1 ~]# /usr/share/logstash/bin/logstash -e 'input &#123; stdin&#123;&#125; &#125; output &#123; file &#123; path =&gt; "/tmp/log-%&#123;+YYYY.MM.dd&#125;messages.gz"&#125;&#125;'hello world #输入[INFO ] 2019-07-04 17:33:06.065 [[main]&gt;worker0] file - Opening file &#123;:path=&gt;"/tmp/log-2019.07.04messages.gz"&#125;[root@linux-elk1 ~]# tail /tmp/log-2019.07.04messages.gz &#123;"message":"hello world","@version":"1","host":"linux-elk1.exmaple.com","@timestamp":"2019-07-04T09:33:05.698Z"&#125; 3）测试输出到elasticsearch1[root@linux-elk1 ~]# /usr/share/logstash/bin/logstash -e 'input &#123; stdin&#123;&#125; &#125; output &#123; elasticsearch &#123;hosts =&gt; ["192.168.1.31:9200"] index =&gt; "mytest-%&#123;+YYYY.MM.dd&#125;" &#125;&#125;' 4）elasticsearch服务器验证收到数据1234[root@linux-elk1 ~]# ll /elk/data/nodes/0/indices/总用量 0drwxr-xr-x. 8 elasticsearch elasticsearch 65 7月 4 17:23 4jaihRq6Qu6NQWVxbuRQZgdrwxr-xr-x. 8 elasticsearch elasticsearch 65 7月 4 17:22 kkd_RCldSeaCX3y1XKzdgA kibana部署 Kibana是一个通过调用elasticsearch服务器进行图形化展示搜索结果的开源项目。官网下载地址：https://www.elastic.co/cn/downloads/past-releases#kibana 安装kibana12345678[root@linux-elk1 ~]# wget https://artifacts.elastic.co/downloads/kibana/kibana-6.8.1-x86_64.rpm[root@linux-elk1 ~]# yum -y localinstall kibana-6.8.1-x86_64.rpm[root@linux-elk1 ~]# vim /etc/kibana/kibana.yml [root@linux-elk1 ~]# grep ^[a-Z] /etc/kibana/kibana.yml server.port: 5601 #监听端口server.host: "192.168.1.31" #监听地址elasticsearch.hosts: ["http://192.168.1.31:9200"] #elasticsearch服务器地址i18n.locale: "zh-CN" #修改为中文 启动kibana并验证1234[root@linux-elk1 ~]# systemctl start kibana[root@linux-elk1 ~]# systemctl enable kibana[root@linux-elk1 ~]# ss -nlt |grep 5601LISTEN 0 128 192.168.1.31:5601 *:* 查看状态 通过logstash收集系统message日志 说明：通过logstash收集别的日志文件，前提需要logstash用户对被收集的日志文件有读的权限并对写入的文件有写的权限 1）配置logstash配置文件12345678910111213141516[root@linux-elk1 ~]# vim /etc/logstash/conf.d/system-log.confinput &#123; file &#123; path =&gt; "/var/log/messages" #日志路径 type =&gt; "systemlog" #类型，自定义，在进行多个日志收集存储时可以通过该项进行判断输出 start_position =&gt; "beginning" #logstash 从什么位置开始读取文件数据，默认是结束位置，也就是说 logstash 进程会以类似 tail -F 的形式运行。如果你是要导入原有数据，把这个设定改成 "beginning"，logstash 进程就从头开始读取，类似 less +F 的形式运行。 stat_interval =&gt; "2" #logstash 每隔多久检查一次被监听文件状态（是否有更新），默认是 1 秒 &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; ["192.168.1.31:9200"] #elasticsearch服务器地址 index =&gt; "logstash-%&#123;type&#125;-%&#123;+YYYY.MM.dd&#125;" #索引名称 &#125;&#125; 2）检测配置文件语法是否有错误123456[root@linux-elk1 ~]# /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/system-log.conf -t #检测配置文件是否有语法错误WARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaultsCould not find log4j2 configuration at path /usr/share/logstash/config/log4j2.properties. Using default config which logs errors to the console[WARN ] 2019-07-05 10:09:59.423 [LogStash::Runner] multilocal - Ignoring the 'pipelines.yml' file because modules or command line options are specifiedConfiguration OK[INFO ] 2019-07-05 10:10:27.993 [LogStash::Runner] runner - Using config.test_and_exit mode. Config Validation Result: OK. Exiting Logstash 3）修改日志文件的权限并重启logstash12345[root@linux-elk1 ~]# ll /var/log/messages -rw-------. 1 root root 786219 7月 5 10:10 /var/log/messages#这里可以看到该日志文件是600权限，而elasticsearch是运行在elasticsearch用户下，这样elasticsearch是无法收集日志的。所以这里需要更改日志的权限，否则会报权限拒绝的错误。在日志中查看/var/log/logstash/logstash-plain.log 是否有错误。[root@linux-elk1 ~]# chmod 644 /var/log/messages[root@linux-elk1 ~]# systemctl restart logstash 4）elasticsearch界面查看并查询5）kibana界面创建索引并查看]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat+Nginx+Memcached综合案例]]></title>
    <url>%2F2019%2F06%2F27%2FTomcat%2BNginx%2BMemcached%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[说明 通过Nginx解析静态页面并将动态负载均衡调度给后面的多个Tomcat，Tomcat解析java动态程序。 由于http是无状态的协议，你访问了页面A，然后在访问B，http无法确定这2个访问来自一个人，因此要用cookie或session来跟踪用户，根据授权和用户身份来显示不同的页面。比如用户A登陆了，那么能看到自己的个人信息，而B没登陆，无法看到个人信息。还有A可能在购物，把商品放入购物车，此时B也有这个过程，你无法确定A，B的身份和购物信息，所以需要一个session ID来维持这个过程。所以就用到了session管理。 官网文档 环境规划 主机 hostname 环境 192.168.1.31 nginx.cluster.com nginx（yum安装） 192.168.1.32 tomcat1.cluster.com tomcat-9.0 192.168.1.33 tomcat2.cluster.com tomcat-9.0 192.168.1.34 memcached.cluster.com memcached（yum安装） 关闭防火墙，selinux；时间同步；host绑定等基本配置 (步骤略) 具体步骤本案例所使用的tomcat、jdk、和tomcat-session相关的jar包下载地址链接：https://pan.baidu.com/s/1ESm_RSrFx77kWObmCt1DQw提取码：f64n memcached部署 说明：memcached这里不需要配置太多，安装即可，然后检查端口是否处于监听中。tomcat服务器能成功连接即可12345678910[root@memcached ~]# yum -y install memcached[root@memcached ~]# systemctl enable memcached[root@memcached ~]# systemctl start memcached[root@memcached ~]# lsof -i:11211COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEmemcached 4419 memcached 26u IPv4 51021 0t0 TCP *:memcache (LISTEN)memcached 4419 memcached 27u IPv6 51022 0t0 TCP *:memcache (LISTEN)memcached 4419 memcached 28u IPv4 51025 0t0 UDP *:memcache memcached 4419 memcached 29u IPv6 51026 0t0 UDP *:memcache nginx部署1）安装nginx123[root@nginx ~]# yum -y install nginx[root@nginx ~]# nginx -vnginx version: nginx/1.12.2 2）配置文件配置1234567891011121314151617181920212223242526272829303132# 创建一个虚拟主机[root@nginx ~]# vim /etc/nginx/conf.d/www.confupstream tomcat &#123; server 192.168.1.32:8080 weight=1; server 192.168.1.33:8080 weight=1;&#125;server &#123; listen 88 default_server; server_name localhost; root /opt/project;#已jsp结尾的动态程序调度给tomcat去处理 location ~.*\.jsp$ &#123; proxy_pass http://tomcat; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $remote_addr; &#125; error_page 404 /404.html; location = /40x.html &#123; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; &#125;&#125;# 创建测试文件[root@nginx ~]# mkdir /opt/project[root@nginx ~]# echo "&lt;h1&gt;Nginx IP:192.168.1.31&lt;/h1&gt;" &gt;&gt; /opt/project/index.html tomcat部署详细安装参考：https://www.cnblogs.com/yanjieli/p/11092350.html两台tomcat服务器都要执行下面的所有操作，也可以在一台上面执行，然后copy过去。1）上传软件包到服务器，编写一个临时使用的安装脚本1234567891011121314151617181920212223[root@tomcat1 ~]# cat install_tomcat.sh #!/bin/bash#----安装java环境function InstallJava ()&#123; tar xf jdk-8u211-linux-x64.tar.gz -C /usr/local/ ln -s /usr/local/jdk1.8.0_211 /usr/local/java sed -i.ori '$a export JAVA_HOME=/usr/local/java \nexport PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH \nexport CLASSPATH=.$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$JAVA_HOME/lib/tools.jar' /etc/profile source /etc/profile java -version&#125;#----安装tomcat环境function InstallTomcat ()&#123; tar xf apache-tomcat-9.0.21.tar.gz -C /usr/local/ ln -s /usr/local/apache-tomcat-9.0.21 /usr/local/tomcat echo "export TOMCAT_HOME=/usr/local/tomcat" &gt;&gt; /etc/profile source /etc/profile&#125;InstallJavaInstallTomcat/usr/local/tomcat/bin/startup.sh 2）执行脚本123456789101112131415[root@tomcat1 ~]# bash install_tomcat.sh java version "1.8.0_211"Java(TM) SE Runtime Environment (build 1.8.0_211-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode)Using CATALINA_BASE: /usr/local/tomcatUsing CATALINA_HOME: /usr/local/tomcatUsing CATALINA_TMPDIR: /usr/local/tomcat/tempUsing JRE_HOME: /usr/local/javaUsing CLASSPATH: /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jarTomcat started.[root@tomcat1 ~]# ss -nltp |grep :80LISTEN 0 100 :::8080 :::* users:(("java",pid=2993,fd=54))LISTEN 0 1 ::ffff:127.0.0.1:8005 :::* users:(("java",pid=2993,fd=74))LISTEN 0 100 :::8009 :::* users:(("java",pid=2993,fd=59)) 3）下载memcached-session-manager等相关软件包并copy到tomcat安装目录的lib目录中1234567891011121314[root@tomcat1 ~]# mkdir tools &amp;&amp; cd tools[root@tomcat1 ~]# wget http://repo1.maven.org/maven2/de/javakaffee/msm/memcached-session-manager/2.3.0/memcached-session-manager-2.3.0.jar[root@tomcat1 ~]# wget http://repo1.maven.org/maven2/de/javakaffee/msm/memcached-session-manager-tc9/2.3.0/memcached-session-manager-tc9-2.3.0.jar[root@tomcat1 ~]# wget http://repo1.maven.org/maven2/de/javakaffee/msm/msm-kryo-serializer/2.3.0/msm-kryo-serializer-2.3.0.jar[root@tomcat1 ~]# wget http://repo1.maven.org/maven2/net/spy/spymemcached/2.12.2/spymemcached-2.12.2.jar[root@tomcat1 ~]# wget https://repo1.maven.org/maven2/de/javakaffee/kryo-serializers/0.42/kryo-serializers-0.42.jar[root@tomcat1 ~]# wget https://repo1.maven.org/maven2/com/esotericsoftware/reflectasm/1.11.0/reflectasm-1.11.0.jar[root@tomcat1 ~]# wget https://repo1.maven.org/maven2/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar[root@tomcat1 ~]# wget https://repo1.maven.org/maven2/com/esotericsoftware/kryo/4.0.0/kryo-4.0.0.jar[root@tomcat1 ~]# wget https://repo1.maven.org/maven2/org/ow2/asm/asm/7.0/asm-7.0.jar[root@tomcat1 ~]# wget http://repo1.maven.org/maven2/org/objenesis/objenesis/3.0.1/objenesis-3.0.1.jar[root@tomcat1 tools]# cp ./* /usr/local/tomcat/lib/ 4）编辑配置文件，添加连接memcached12345678910111213# No-Stick模式[root@tomcat1 ~]# vim /usr/local/tomcat/conf/context.xml# 在&lt;Context&gt;和&lt;/Context&gt;里面加上下面一段&lt;!-- 这里的ip为memcached服务器的IP,如果有多个memcached服务器，用逗号隔开 --&gt; &lt;Manager className="de.javakaffee.web.msm.MemcachedBackupSessionManager" memcachedNodes="n1:192.168.1.34:11211" lockingMode="auto" sticky="false" requestUriIgnorePattern= ".*\.(png|gif|jpg|css|js)$" sessionBackupAsync= "false" sessionBackupTimeout= "100" copyCollectionsForSerialization="true" transcoderFactoryClass="de.javakaffee.web.msm.serializer.kryo.KryoTranscoderFactory" /&gt; 5）准备测试文件123456[root@tomcat1 ~]# rm -rf /usr/local/tomcat/webapps/*[root@tomcat1 ~]# mkdir /usr/local/tomcat/webapps/ROOT[root@tomcat1 ~]# vim /usr/local/tomcat/webapps/ROOT/index.jspSessionID:&lt;%=session.getId()%&gt; &lt;BR&gt;SessionIP:&lt;%=request.getServerName()%&gt; &lt;BR&gt;SessionPort:&lt;%=request.getServerPort()%&gt; 6）重启tomcat12345[root@tomcat1 ~]# /usr/local/tomcat/bin/shutdown.sh[root@tomcat1 ~]# /usr/local/tomcat/bin/startup.sh[root@tomcat1 ~]# lsof -i:8080COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEjava 4672 root 63u IPv6 52084 0t0 TCP *:webcache (LISTEN) 7）测试访问tomcat1：访问tomcat2：访问nginx默认页面：访问jsp动态程序：通过上面的访问可以看出，访问静态页面时nginx自身处理，当访问jsp动态的时候会调度给tomcat去处理。并且当一个浏览器访问后，便会一直访问这一个，so，这样就达到了session会话保持。 补充memcached-session-manager 参数说明：123456789101112131415memcachedNodes 必选项，memcached的节点信息，多个memcached节点,中间需要使用空格failoverNodes="n2" 表示当前session保持到n1的memcached节点上failoverNodes 可选项，不能使用在non-sticky sessions模式。故障转移配置节点，多个使用空格或逗号分开，配置某个节点为备份节点，当其他节点都不可用时才会存储到备份节点，官方建议配置为和tomcat同服务器的节点。理由如下:假如有两台服务器m1,m2，其中m1部署tomcat和memcached节点n1，m2部署memcached节点n2。如果配置tomcat的failoverNodes值为n2或者不配置，则当服务器m1挂掉后n1和tomcat中保存的session会丢失，而n2中未保存或者只保存了部分session，这就造成 部分用户状态丢失。如果配置tomcat的failoverNodes值为n1，则当m1挂掉后因为n2中保存了所有的session，所以重启tomcat的时候用户状态不会丢失。为什么n2中保存了所有的session? 因为failoverNodes配置的值是n1，只有当n2节点不可用时才会把session存储到n1，所以这个时候n1中是没有保存任何session的。lockingMode 可选值，默认none，只对non-sticky有效。requestUriIgnorePattern 可选值，制定忽略那些请求的session操作，一般制定静态资源如css,js一类的。sessionBackupAsync 可选值，默认true，是否异步的方式存储到memcached。sessionBackupTimeout 可选项，默认100毫秒，异步存储session的超时时间。 相关软件包jar包下载：123456789101112131415161718192021222324252627memcached-session-manager 下载地址：http://repo1.maven.org/maven2/de/javakaffee/msm/http://repo1.maven.org/maven2/de/javakaffee/msm/memcached-session-manager/2.3.0/memcached-session-manager-2.3.0.jarhttp://repo1.maven.org/maven2/de/javakaffee/msm/memcached-session-manager-tc9/2.3.0/memcached-session-manager-tc9-2.3.0.jarmsm-kryo-serializer 下载地址：http://repo1.maven.org/maven2/de/javakaffee/msm/msm-kryo-serializer/http://repo1.maven.org/maven2/de/javakaffee/msm/msm-kryo-serializer/2.3.0/msm-kryo-serializer-2.3.0.jarspymemcached 下载地址：http://repo1.maven.org/maven2/net/spy/spymemcached/http://repo1.maven.org/maven2/net/spy/spymemcached/2.12.2/spymemcached-2.12.2.jarserializers 下载地址：https://repo1.maven.org/maven2/de/javakaffee/kryo-serializers/https://repo1.maven.org/maven2/de/javakaffee/kryo-serializers/0.42/kryo-serializers-0.42.jarreflectasm 下载地址：https://repo1.maven.org/maven2/com/esotericsoftware/reflectasmhttps://repo1.maven.org/maven2/com/esotericsoftware/reflectasm/1.11.0/reflectasm-1.11.0.jarminlog 下载地址：https://repo1.maven.org/maven2/com/esotericsoftware/minloghttps://repo1.maven.org/maven2/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jarkryo 下载地址：https://repo1.maven.org/maven2/com/esotericsoftware/kryohttps://repo1.maven.org/maven2/com/esotericsoftware/kryo/4.0.0/kryo-4.0.0.jarasm 下载地址：https://repo1.maven.org/maven2/org/ow2/asm/asm/https://repo1.maven.org/maven2/org/ow2/asm/asm/7.0/asm-7.0.jar objenesis 下载地址：http://repo1.maven.org/maven2/org/objenesis/objenesis/http://repo1.maven.org/maven2/org/objenesis/objenesis/3.0.1/objenesis-3.0.1.jar]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat安装与部署]]></title>
    <url>%2F2019%2F06%2F26%2FTomcat%E5%AE%89%E8%A3%85%E4%B8%8E%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[Tomcat简介官网：http://tomcat.apache.org/ Tomcat服务器是一个免费的开源代码的Web应用服务器，属于轻量级应用服务器，在中小型系统和并发访问用户不是很多的场合下使用，是开发和调试JSP程序的首选。Tomcat和Nginx、Apache等Web服务器一样，具有处理HTML页面的功能，另外它还是一个Servlet和JSP容器，独立的Servlet容器是Tomcat的默认模式。不过，Tomcat处理静态HTML的能力不如Nginx/Apache服务器。一般情况下多用Nginx+Tomcat，Nginx处理静态，Tomcat处理动态程序 Tomcat安装 软件下载：JDK下载 &emsp; Tomcat下载本文使用的软件包jdk-8u211,tomcat-9.0.21。 下载链接 &emsp; 提取码：f0ay 部署Java环境将下载的软件包上传到服务器12345678910111213141516171819# 解压软件包[root@tomcat ~]# tar xf jdk-8u211-linux-x64.tar.gz -C /usr/local/# 创建一个软链接[root@tomcat ~]# ln -s /usr/local/jdk1.8.0_211 /usr/local/java# 添加环境变量[root@tomcat ~]# sed -i.ori '$a export JAVA_HOME=/usr/local/java \nexport PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH \nexport CLASSPATH=.$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$JAVA_HOME/lib/tools.jar' /etc/profile# 重新加载环境变量[root@tomcat ~]# source /etc/profile[root@tomcat ~]# env |grep JAVAJAVA_HOME=/usr/local/java# 查看是否安装成功[root@tomcat ~]# java -versionjava version "1.8.0_211"Java(TM) SE Runtime Environment (build 1.8.0_211-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode) 部署Tomcat1234567891011# 解压软件包[root@tomcat ~]# tar xf apache-tomcat-9.0.21.tar.gz -C /usr/local/# 创建一个软链接[root@tomcat ~]# ln -s /usr/local/apache-tomcat-9.0.21 /usr/local/tomcat# 定义tomcat所需的环境变量[root@tomcat ~]# echo "export TOMCAT_HOME=/usr/local/tomcat" &gt;&gt; /etc/profile# 重新加载环境变量[root@tomcat ~]# source /etc/profile Tomcat启动1）默认的启动方式123456789101112131415161718192021[root@tomcat ~]# ls /usr/local/tomcat/bin/startup.sh #启动脚本/usr/local/tomcat/bin/startup.sh[root@tomcat ~]# ls /usr/local/tomcat/bin/shutdown.sh #停止脚本/usr/local/tomcat/bin/shutdown.sh# 默认方式启动[root@tomcat ~]# /usr/local/tomcat/bin/startup.sh Using CATALINA_BASE: /usr/local/tomcatUsing CATALINA_HOME: /usr/local/tomcatUsing CATALINA_TMPDIR: /usr/local/tomcat/tempUsing JRE_HOME: /usr/local/javaUsing CLASSPATH: /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jarTomcat started.# 默认方式停止[root@tomcat ~]# /usr/local/tomcat/bin/shutdown.sh Using CATALINA_BASE: /usr/local/tomcatUsing CATALINA_HOME: /usr/local/tomcatUsing CATALINA_TMPDIR: /usr/local/tomcat/tempUsing JRE_HOME: /usr/local/javaUsing CLASSPATH: /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar 2）加入/etc/init.d/支持centos6的service12345678910111213141516171819202122232425262728293031# 编辑/etc/init.d/tomcat脚本[root@tomcat ~]# vim /etc/init.d/tomcat#!/bin/bash# Init file for Tomcat server daemon## chkconfig: 2345 96 14# description: Tomcat server daemonJAVA_OPTS='-Xms64m -Xmx128m'JAVA_HOME=/usr/local/jdk1.8.0_211 #指定jdk安装路径CATALINA_HOME=/usr/local/tomcat #指定tomcat安装路径export JAVA_OPTS JAVA_HOME CATALINA_HOMEexec $CATALINA_HOME/bin/catalina.sh $*# 添加执行权限[root@tomcat ~]# chmod +x /etc/init.d/tomcat# 启动停止测试[root@tomcat ~]# service tomcat startUsing CATALINA_BASE: /usr/local/tomcatUsing CATALINA_HOME: /usr/local/tomcatUsing CATALINA_TMPDIR: /usr/local/tomcat/tempUsing JRE_HOME: /usr/local/jdk1.8.0_211Using CLASSPATH: /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jarTomcat started.[root@tomcat ~]# [root@tomcat ~]# service tomcat stopUsing CATALINA_BASE: /usr/local/tomcatUsing CATALINA_HOME: /usr/local/tomcatUsing CATALINA_TMPDIR: /usr/local/tomcat/tempUsing JRE_HOME: /usr/local/jdk1.8.0_211Using CLASSPATH: /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar 3）加入systemd管理tomcat123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 编辑启动脚本[root@tomcat ~]# vim /usr/lib/systemd/system/tomcat.service[Unit]Description=Tomcat server daemonAfter=syslog.target network.target remote-fs.target nss-lookup.target[Service]Type=oneshotExecStart=/usr/local/tomcat/bin/startup.shExecStop=/usr/local/tomcat/bin/shutdown.shExecReload=/bin/kill -HUP $MAINPIDRemainAfterExit=yes[Install]WantedBy=multi-user.target# 测试启动和关闭[root@tomcat ~]# systemctl start tomcat[root@tomcat ~]# ps -ef |grep tomcatroot 4810 1 35 12:37 ? 00:00:02 /usr/bin/java -Djava.util.logging.config.file=/usr/local/tomcat/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dignore.endorsed.dirs= -classpath /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar -Dcatalina.base=/usr/local/tomcat -Dcatalina.home=/usr/local/tomcat -Djava.io.tmpdir=/usr/local/tomcat/temp org.apache.catalina.startup.Bootstrap startroot 4852 1823 0 12:37 pts/1 00:00:00 grep --color=auto tomcat[root@tomcat ~]# [root@tomcat ~]# systemctl stop tomcat[root@tomcat ~]# ps -ef |grep tomcatroot 4888 1823 0 12:38 pts/1 00:00:00 grep --color=auto tomcat[root@tomcat ~]# [root@tomcat ~]# systemctl restart tomcat [root@tomcat ~]# ps -ef |grep tomcatroot 4909 1 43 12:38 ? 00:00:02 /usr/bin/java -Djava.util.logging.config.file=/usr/local/tomcat/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dignore.endorsed.dirs= -classpath /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar -Dcatalina.base=/usr/local/tomcat -Dcatalina.home=/usr/local/tomcat -Djava.io.tmpdir=/usr/local/tomcat/temp org.apache.catalina.startup.Bootstrap startroot 4959 1823 0 12:38 pts/1 00:00:00 grep --color=auto tomcat[root@tomcat ~]#[root@tomcat ~]# systemctl status tomcat● tomcat.service - Tomcat server daemon Loaded: loaded (/usr/lib/systemd/system/tomcat.service; disabled; vendor preset: disabled) Active: active (exited) since 二 2019-06-25 12:38:22 CST; 3min 53s ago Process: 4895 ExecStart=/usr/local/tomcat/bin/startup.sh (code=exited, status=0/SUCCESS) Main PID: 4895 (code=exited, status=0/SUCCESS) CGroup: /system.slice/tomcat.service └─4909 /usr/bin/java -Djava.util.logging.config.file=/usr/local/tomcat/conf/logging.pro...6月 25 12:38:22 tomcat systemd[1]: Starting Tomcat server daemon...6月 25 12:38:22 tomcat startup.sh[4895]: Tomcat started.6月 25 12:38:22 tomcat systemd[1]: Started Tomcat server daemon.# 加入开机启动[root@tomcat ~]# systemctl enable tomcat[root@tomcat ~]# systemctl list-unit-files |grep tomcattomcat.service enabled tomcat启动成功后默认端口为：8080，访问地址http://ip:8080 Tomcat目录结构tomcat安装主目录下的各个子目录说明：12345678910111213141516171819[root@tomcat ~]# cd /usr/local/tomcat/[root@tomcat tomcat]# tree -L 1.├── bin #存放启动、关闭tomcat或者其它功能的脚本(.bat文件和.sh文件)├── BUILDING.txt├── conf #存放tomcat配置相关的文件├── CONTRIBUTING.md├── lib #存放Web应用能访问的JAR包├── LICENSE├── logs #存放tomcat日志文件├── NOTICE├── README.md├── RELEASE-NOTES├── RUNNING.txt├── temp #临时文件├── webapps #Web应用程序的跟目录└── work #用以产生有JSP编译出的Servlet的.java和.class文件7 directories, 7 files webapps目录说明： 这里几个目录对应着主界面的上面的按钮，可以直接在主界面查看帮助文档，及web界面直接管理。 12345678[root@tomcat tomcat]# cd webapps/[root@tomcat webapps]# ll总用量 4drwxr-x--- 14 root root 4096 6月 25 11:39 docs #tomcat帮助文档drwxr-x--- 6 root root 83 6月 25 11:39 examples #web应用实例drwxr-x--- 5 root root 87 6月 25 11:39 host-manager #管理drwxr-x--- 5 root root 103 6月 25 11:39 manager #管理drwxr-x--- 3 root root 283 6月 25 11:39 ROOT #默认网站根目录 Tomcat日志文件 tomcat一般有几个日志文件，访问的一般放在localhost日志文件里面，管理的日志放在manager日志文件里面，实时日志一般在catalina.out里面 123456789101112131415161718192021[root@tomcat tomcat]# cd logs/[root@tomcat logs]# ll总用量 180-rw-r----- 1 root root 82372 6月 25 12:38 catalina.2019-06-25.log-rw-r----- 1 root root 82574 6月 25 12:38 catalina.out-rw-r----- 1 root root 0 6月 25 12:18 host-manager.2019-06-25.log-rw-r----- 1 root root 6246 6月 25 12:38 localhost.2019-06-25.log-rw-r----- 1 root root 3489 6月 25 14:30 localhost_access_log.2019-06-25.txt-rw-r----- 1 root root 0 6月 25 12:18 manager.2019-06-25.log[root@tomcat logs]# tailf catalina.out 25-Jun-2019 12:38:24.674 信息 [main] org.apache.catalina.startup.HostConfig.deployDirectory Deployment of web application directory [/usr/local/apache-tomcat-9.0.21/webapps/docs] has finished in [27] ms25-Jun-2019 12:38:24.675 信息 [main] org.apache.catalina.startup.HostConfig.deployDirectory 把web 应用程序部署到目录 [/usr/local/apache-tomcat-9.0.21/webapps/examples]25-Jun-2019 12:38:25.113 信息 [main] org.apache.catalina.startup.HostConfig.deployDirectory Deployment of web application directory [/usr/local/apache-tomcat-9.0.21/webapps/examples] has finished in [438] ms25-Jun-2019 12:38:25.113 信息 [main] org.apache.catalina.startup.HostConfig.deployDirectory 把web 应用程序部署到目录 [/usr/local/apache-tomcat-9.0.21/webapps/host-manager]25-Jun-2019 12:38:25.163 信息 [main] org.apache.catalina.startup.HostConfig.deployDirectory Deployment of web application directory [/usr/local/apache-tomcat-9.0.21/webapps/host-manager] has finished in [50] ms25-Jun-2019 12:38:25.163 信息 [main] org.apache.catalina.startup.HostConfig.deployDirectory 把web 应用程序部署到目录 [/usr/local/apache-tomcat-9.0.21/webapps/manager]25-Jun-2019 12:38:25.220 信息 [main] org.apache.catalina.startup.HostConfig.deployDirectory Deployment of web application directory [/usr/local/apache-tomcat-9.0.21/webapps/manager] has finished in [57] ms25-Jun-2019 12:38:25.238 信息 [main] org.apache.coyote.AbstractProtocol.start 开始协议处理句柄["http-nio-8080"]25-Jun-2019 12:38:25.298 信息 [main] org.apache.coyote.AbstractProtocol.start 开始协议处理句柄["ajp-nio-8009"]25-Jun-2019 12:38:25.302 信息 [main] org.apache.catalina.startup.Catalina.start Server startup in [1,290] milliseconds Tomcat配置文件 tomcat配置文件存放在安装目录下的conf目录下面 123456789101112131415# 进入到配置文件目录[root@tomcat ~]# cd /usr/local/tomcat/conf/[root@tomcat conf]# ll总用量 228drwxr-x--- 3 root root 23 6月 25 12:18 Catalina-rw------- 1 root root 12873 6月 5 04:23 catalina.policy-rw------- 1 root root 7243 6月 5 04:23 catalina.properties-rw------- 1 root root 1400 6月 5 04:23 context.xml-rw------- 1 root root 1149 6月 5 04:23 jaspic-providers.xml-rw------- 1 root root 2313 6月 5 04:23 jaspic-providers.xsd-rw------- 1 root root 4144 6月 5 04:23 logging.properties-rw------- 1 root root 7511 6月 5 04:23 server.xml #主配置文件-rw------- 1 root root 2164 6月 5 04:23 tomcat-users.xml #Tomcat管理用户配置文件-rw------- 1 root root 2633 6月 5 04:23 tomcat-users.xsd-rw------- 1 root root 171962 6月 5 04:23 web.xml Tomcat管理 配置tomcat的web界面管理功能，可以进行配置文件的管理，及部署在tomcat上的应用进行管理，默认情况是处于禁用状态。如果要开启这个功能，需要配置管理用户，即配置tomcat-users.xml文件。并且还需要修改manager项目下的content.xml文件，让其所有地址可访问 123456789101112131415161718192021222324# 编辑配置文件[root@tomcat ~]# vim /usr/local/tomcat/conf/tomcat-users.xml...&lt;role rolename="manager-gui"/&gt;&lt;role rolename="admin-gui"/&gt;&lt;user username="tomcat" password="tomcat" roles="manager-gui,admin-gui"/&gt;&lt;/tomcat-users&gt; #在这行前面加入上面三行# 编辑manager/META-INF/context.xml[root@tomcat ~]# vim /usr/local/tomcat/webapps/manager/META-INF/context.xml#将allow="127\.\d+\.\d+\.\d+|::1|0:0:0:0:0:0:0:1" /&gt;#改为allow="\d+\.\d+\.\d+\.\d+|::1|0:0:0:0:0:0:0:1" /&gt;# 编辑host-manager/META-INF/context.xml[root@tomcat ~]# vim /usr/local/tomcat/webapps/host-manager/META-INF/context.xml#将allow="127\.\d+\.\d+\.\d+|::1|0:0:0:0:0:0:0:1" /&gt;#改为allow="\d+\.\d+\.\d+\.\d+|::1|0:0:0:0:0:0:0:1" /&gt;# 重启tomcat[root@tomcat ~]# systemctl restart tomcat 访问测试状态页面：Manager页面：Host Manager页面： server.xml配置文件注释参考：https://www.cnblogs.com/sunshine-1/p/8990044.html123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&lt;Server port="8005" shutdown="SHUTDOWN"&gt; &lt;Listener className="org.apache.catalina.startup.VersionLoggerListener" /&gt; &lt;Listener className="org.apache.catalina.security.SecurityListener" /&gt; &lt;Listener className="org.apache.catalina.core.AprLifecycleListener" SSLEngine="on" /&gt; &lt;Listener className="org.apache.catalina.core.JasperListener" /&gt; &lt;Listener className="org.apache.catalina.core.JreMemoryLeakPreventionListener" /&gt; &lt;Listener className="org.apache.catalina.mbeans.GlobalResourcesLifecycleListener" /&gt; &lt;Listener className="org.apache.catalina.core.ThreadLocalLeakPreventionListener" /&gt; &lt;GlobalNamingResources&gt; &lt;!-- 全局命名资源，来定义一些外部访问资源，其作用是为所有引擎应用程序所引用的外部资源的定义 --!&gt; &lt;Resource name="UserDatabase" auth="Container" type="org.apache.catalina.UserDatabase" description="User database that can be updated and saved" factory="org.apache.catalina.users.MemoryUserDatabaseFactory" pathname="conf/tomcat-users.xml" /&gt; &lt;/GlobalNamingResources&gt; &lt;!-- 定义的一个名叫“UserDatabase”的认证资源，将conf/tomcat-users.xml加载至内存中，在需要认证的时候到内存中进行认证 --&gt; &lt;Service name="Catalina"&gt; &lt;!-- # 定义Service组件，同来关联Connector和Engine，一个Engine可以对应多个Connector，每个Service中只能一个Engine --!&gt; &lt;Connector port="80" protocol="HTTP/1.1" connectionTimeout="20000" redirectPort="8443" /&gt; &lt;!-- 修改HTTP/1.1的Connector监听端口为80.客户端通过浏览器访问的请求，只能通过HTTP传递给tomcat。还可以设置server与URIEncoding参数 --&gt; &lt;Connector port="8009" protocol="AJP/1.3" redirectPort="8443" /&gt; &lt;Engine name="Catalina" defaultHost="test.com"&gt; &lt;!-- 修改当前Engine，默认主机是，www.test.com --&gt; &lt;Realm className="org.apache.catalina.realm.LockOutRealm"&gt; &lt;Realm className="org.apache.catalina.realm.UserDatabaseRealm" resourceName="UserDatabase"/&gt; &lt;/Realm&gt; # Realm组件，定义对当前容器内的应用程序访问的认证，通过外部资源UserDatabase进行认证 &lt;Host name="test.com" appBase="/web" unpackWARs="true" autoDeploy="true"&gt; &lt;!-- 定义一个主机，域名为：test.com，应用程序的目录是/web，设置自动部署，自动解压 --&gt; &lt;Alias&gt;www.test.com&lt;/Alias&gt; &lt;!-- 定义一个别名www.test.com，类似apache的ServerAlias --&gt; &lt;Context path="" docBase="www/" reloadable="true" /&gt; &lt;!-- 定义该应用程序，访问路径""，即访问www.test.com即可访问，网页目录为：相对于appBase下的www/，即/web/www，并且当该应用程序下web.xml或者类等有相关变化时，自动重载当前配置，即不用重启tomcat使部署的新应用程序生效 --&gt; &lt;Context path="/bbs" docBase="/web/bbs" reloadable="true" /&gt; &lt;!-- 定义另外一个独立的应用程序(虚拟主机)，访问路径为：www.test.com/bbs，该应用程序网页目录为/web/bbs --&gt; &lt;Valve className="org.apache.catalina.valves.AccessLogValve" directory="/web/www/logs" prefix="www_access." suffix=".log" pattern="%h %l %u %t &amp;quot;%r&amp;quot; %s %b" /&gt; &lt;!-- 定义一个Valve组件，用来记录tomcat的访问日志，日志存放目录为：/web/www/logs如果定义为相对路径则是相当于$CATALINA_HOME，并非相对于appBase，这个要注意。定义日志文件前缀为www_access.并以.log结尾，pattern定义日志内容格式，具体字段表示可以查看tomcat官方文档 --&gt; &lt;/Host&gt; &lt;Host name="manager.test.com" appBase="webapps" unpackWARs="true" autoDeploy="true"&gt; &lt;!-- 定义一个主机名为man.test.com，应用程序目录是$CATALINA_HOME/webapps,自动解压，自动部署 --&gt; &lt;Valve className="org.apache.catalina.valves.RemoteAddrValve" allow="172.16.100.*" /&gt; &lt;!-- 定义远程地址访问策略，仅允许172.16.100.*网段访问该主机，其他的将被拒绝访问 --&gt; &lt;Valve className="org.apache.catalina.valves.AccessLogValve" directory="/web/bbs/logs" prefix="bbs_access." suffix=".log" pattern="%h %l %u %t &amp;quot;%r&amp;quot; %s %b" /&gt; &lt;!-- 定义该主机的访问日志 --&gt; &lt;/Host&gt; &lt;/Engine&gt; &lt;/Service&gt; &lt;/Server&gt; Tomcat单实例站点部署 以部署jspxcms为例，在上面已部署的环境下继续操作。 1）安装MySQL及创建库12345678[root@tomcat ~]# yum -y install mariadb mariadb-server[root@tomcat ~]# systemctl enable mariadb[root@tomcat ~]# systemctl start mariadb# 创建数据库并授权MariaDB [(none)]&gt; create database jspxcms_test character set utf8;MariaDB [(none)]&gt; grant all on jspxcms_test.* to 'jspxcmsuser'@'localhost' identified by '123';MariaDB [(none)]&gt; flush privileges; 2）部署jspxcms12345678910111213141516171819202122232425262728# tomcat默认的网站目录[root@tomcat ~]# ls /usr/local/tomcat/webapps/docs examples host-manager manager ROOT[root@tomcat ~]# ls /usr/local/tomcat/webapps/ROOT/# 上传jspxcms安装包并解压[root@tomcat ~]# mkdir tools[root@tomcat tools]# lsjspxcms-9.5.0-release.zip[root@tomcat tools]# unzip jspxcms-9.5.0-release.zip[root@tomcat tools]# lsCHANGELOG.txt database jspxcms-9.5.0-release.zip Jspxcms安装手册.pdf ROOT 用户许可协议.txt[root@tomcat tools]# rm -rf /usr/local/tomcat/webapps/*[root@tomcat tools]# cp -a ROOT /usr/local/tomcat/webapps/# 导入sql文件[root@tomcat tools]# mysql -D jspxcms_test &lt; database/mysql.sql# 编辑配置文件配置数据库连接信息[root@tomcat tools]# vim /usr/local/tomcat/webapps/ROOT/WEB-INF/classes/application.propertiesspring.datasource.url=jdbc:mysql://127.0.0.1:3306/jspxcms_test?characterEncoding=utf8# \u6570\u636E\u5E93\u7528\u6237\u540Dspring.datasource.username=jspxcmsuser# \u6570\u636E\u5E93\u5BC6\u7801spring.datasource.password=123# 重启tomcat[root@tomcat ~]# systemctl restart tomcat 3）web页面访问 前台地址：http://ip:8080 &emsp;后台地址：http://ip:8080/cmscp/index.do &emsp; 默认账号：admin 密码为空 Tomcat多实例站点部署 多实例作用运行不同的应用（类似虚拟主机）多实例运行相同的应用（实现负载均衡，支持高并发处理，session问题）这里基于上面已安装的tomcat环境 环境 Web监听端口 管理实例端口 站点家目录 tomcat9_1 8081 8091 /webapps/tomcat9_1 tomcat9_2 8082 8092 /webapps/tomcat9_2 1）拷贝tomcat目录12[root@tomcat ~]# cp -a /usr/local/apache-tomcat-9.0.21 /usr/local/tomcat9_1[root@tomcat ~]# cp -a /usr/local/apache-tomcat-9.0.21 /usr/local/tomcat9_2 2）编辑配置文件，修改监听端口和站点家目录123456789101112131415[root@tomcat ~]# vim /usr/local/tomcat9_1/conf/server.xml&lt;Server port="8091" shutdown="SHUTDOWN"&gt;&lt;Connector port="8081" protocol="HTTP/1.1" connectionTimeout="20000" redirectPort="8443" /&gt;&lt;Host name="localhost" appBase="/webapps/tomcat9_1" unpackWARs="true" autoDeploy="true"&gt;[root@tomcat ~]# vim /usr/local/tomcat9_2/conf/server.xml&lt;Server port="8092" shutdown="SHUTDOWN"&gt;&lt;Connector port="8082" protocol="HTTP/1.1" connectionTimeout="20000" redirectPort="8443" /&gt;&lt;Host name="localhost" appBase="/webapps/tomcat9_2" unpackWARs="true" autoDeploy="true"&gt; 3）创建站点家目录，及测试页面准备123456789101112131415161718192021222324[root@tomcat ~]# mkdir -p /webapps/tomcat9_&#123;1,2&#125;[root@tomcat ~]# mkdir /webapps/tomcat9_&#123;1,2&#125;/ROOT# tomcat9_1测试页面准备[root@tomcat ~]# vim /webapps/tomcat9_1/ROOT/index.jsp&lt;html&gt;&lt;body&gt;&lt;center&gt;&lt;H1&gt;&lt;%=new java.util.Date()%&gt;&lt;/H1&gt;&lt;H2&gt;tomcat9_1&lt;/H2&gt;&lt;/center&gt;&lt;/body&gt;&lt;/html&gt;# tomcat9_2测试页面准备[root@tomcat ~]# vim /webapps/tomcat9_2/ROOT/index.jsp&lt;html&gt;&lt;body&gt;&lt;center&gt;&lt;H1&gt;&lt;%=new java.util.Date()%&gt;&lt;/H1&gt;&lt;H2&gt;tomcat9_2&lt;/H2&gt;&lt;/center&gt;&lt;/body&gt;&lt;/html&gt; 4）删除掉之前的站点目录里面的东西，对这里没有用了。可以直接删掉12[root@tomcat ~]# rm -rf /usr/local/tomcat9_2/webapps/*[root@tomcat ~]# rm -rf /usr/local/tomcat9_1/webapps/* 5）启动tomcat1和tomcat21234567891011[root@tomcat ~]# for i in &#123;1..2&#125;;do /usr/local/tomcat9_$i/bin/startup.sh; done# 端口查看[root@tomcat ~]# ss -tnlp |grep :80LISTEN 0 100 :::8080 :::* users:(("java",pid=27023,fd=54))LISTEN 0 100 :::8081 :::* users:(("java",pid=28687,fd=54))LISTEN 0 100 :::8082 :::* users:(("java",pid=28708,fd=54))LISTEN 0 1 ::ffff:127.0.0.1:8091 :::* users:(("java",pid=28687,fd=65))LISTEN 0 1 ::ffff:127.0.0.1:8092 :::* users:(("java",pid=28708,fd=66))LISTEN 0 1 ::ffff:127.0.0.1:8005 :::* users:(("java",pid=27023,fd=71))LISTEN 0 100 :::8009 :::* users:(("java",pid=27023,fd=59) 6）访问测试 补充：Tomcat多实例启动脚本1234567891011121314151617181920212223242526272829303132333435[root@tomcat ~]# cat TomcatSys.sh #!/bin/bash#Desc：用于tomcat多实例部署启动脚本。#Date：2019-6-26#by：Lee-YJif [ "$1" == "tomcat1" ];then export CATALINA_HOME="/usr/local/tomcat9_1"elif [ "$1" == "tomcat2" ];then export CATALINA_HOME="/usr/local/tomcat9_2"else echo $"Usage: $0 &#123;[tomcat1|tomcat2] start|stop|restart&#125;" &amp;&amp; exitficase "$2" instart) $CATALINA_HOME/bin/startup.sh ;;stop) $CATALINA_HOME/bin/shutdown.sh ;;restart) $CATALINA_HOME/bin/shutdown.sh sleep 3 $CATALINA_HOME/bin/startup.sh ;; *) echo $"Usage: $0 &#123;[tomcat1|tomcat2] start|stop|restart&#125;" &amp;&amp; exit ;;esac# 使用说明：[root@tomcat ~]# ./TomcatSys.sh tomcat1 start #启动tomcat1[root@tomcat ~]# ./TomcatSys.sh tomcat1 stop #停止tomcat1[root@tomcat ~]# ./TomcatSys.sh tomcat2 restart #重启tomcat2]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cobbler自动化部署]]></title>
    <url>%2F2019%2F06%2F12%2FCobbler%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[cobbler简介 Cobbler 可以用来快速建立 Linux 网络安装环境，它已将Linux网络安装的技术门槛，从大专以上文化水平，成功降低到了初中水平，连补鞋匠都能学会。网络安装服务器套件Cobbler（补鞋匠）从前，我们一直在装机民工这份很有前途的职业。自打若干年前Red Hat推出了 Kickstart，此后我们顿觉身价增倍。不再需要刻了光盘一台一台的安装Linux，只要搞定PXE、DHCP、TFTP，还有那满屏眼花缭乱不知所云的Kickstart脚本，我们就可以像哈利波特一样，轻点魔棒，瞬间安装上百台服务器。这一堆花里胡哨的东西可不是一般人能够整明白的，没有大专以上的学历，通不过英语四级，根本别想玩转。总而言之，这是一份多么有前途，多么有技术含量的工作啊。很不幸，Red Hat 最新（Cobbler项目最初在2008年左右发布）发布了网络安装服务器套件Cobbler（补鞋匠），它已将Linux网络安装的技术门槛，从大专以上文化水平，成功降低到初中以下水平，连补鞋匠都能学会。 1、Cobbler是一个Linux服务器安装的服务，可以通过网络启动（PXE）的方式来快速安装、重装物理服务器和虚拟机，同时还可以管理DHCP，DNS等。2、Cobbler可以使用命令行方式管理，也提供了基于Web的界面管理工具（cobbler-web），还提供了API接口，可以方便二次开发使用。3、Cobbler是较早前的kickstart的升级版，优点是比较容易配置，还自带web界面比较易于管理。4、Cobbler内置了一个轻量级配置管理系统，但它也支持和其它配置管理系统集成，如Puppet。 参考网站 cobbler官网 Kickstart文件编写参考 cobbler对应关系 Cobbler的配置结构基于一组注册的对象。每个对象表示一个与另一个实体相关联的实体。当一个对象指向另一个对象时，它就继承了被指向对象的数据，并可覆盖或添加更多特定信息。 发行版(distros)： 表示一个操作系统。它承载了内核和initrd的信息，以及内核参数等其他数据。 配置文件(profiles)：包含一个发行版、一个kickstart文件以及可能的存储库，还包括更多特定的内核参数等其他数据。 系统(systems)：表示要配给的机器。它包括一个配置文件或一个镜像、IP和MAC地址、电源管理（地址、凭据、类型）以及更为专业的数据等信息。 镜像(images)：可以替换一个保函不屑于此类别的文件的发行版对象（例如，无法分为内核和initrd的对象）。 cobbler集成的服务 PXE服务支持 DHCP服务管理 DNS服务管理 电源管理 Kickstart服务支持 YUM仓库管理 TFTP Apache cobbler工作原理 Server端 启动Cobbler服务 进行Cobbler错误检查，执行cobbler check命令 进行配置同步，执行cobbler sync命令 复制相关启动文件到TFTP目录中 启动DHCP服务，提供地址分配 DHCP服务分配IP地址 TFTP传输启动文件 Server端接收安装信息 Server端发送ISO镜像与Kickstart文件 Client端 客户端以PXE模式启动 客户端获取IP地址 通过TFTP服务器获取启动文件 进入Cobbler安装选择界面 根据配置信息准备安装系统 加载Kickstart文件 传输系统安装的其它文件 进行安装系统 cobbler安装 说明：虚拟机网卡采用NAT模式，不要使用桥接模式，因为后面会搭建DHCP服务器，在同一个局域网多个DHCP服务会有冲突。VMware的NAT模式的dhcp服务也关闭，避免干扰。 环境准备12345# 关闭防火墙、selinux等[root@cobbler ~]# systemctl stop firewalld[root@cobbler ~]# systemctl disable firewalld[root@cobbler ~]# setenforce 0[root@cobbler ~]# sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/sysconfig/selinux 安装cobbler123456789# 配置epel源[root@cobbler ~]# yum -y install epel-release# 安装cobbler及dhcp httpd xinetd cobbler-web[root@cobbler ~]# yum -y install cobbler cobbler-web tftp-server dhcp httpd xinetd# 启动cobbler及httpd并加入开机启动[root@cobbler ~]# systemctl start httpd cobblerd[root@cobbler ~]# systemctl enable httpd cobblerd 查看安装后相关文件123456789101112131415161718192021222324[root@cobbler ~]# rpm -ql cobbler/etc/cobbler # 配置文件目录/etc/cobbler/settings # cobbler主配置文件，这个文件是YAML格式，Cobbler是python写的程序。/etc/cobbler/dhcp.template # DHCP服务的配置模板/etc/cobbler/tftpd.template # tftp服务的配置模板/etc/cobbler/rsync.template # rsync服务的配置模板/etc/cobbler/iso # iso模板配置文件目录/etc/cobbler/pxe # pxe模板文件目录/etc/cobbler/power # 电源的配置文件目录/etc/cobbler/users.conf # Web服务授权配置文件/etc/cobbler/users.digest # 用于web访问的用户名密码配置文件/etc/cobbler/dnsmasq.template # DNS服务的配置模板/etc/cobbler/modules.conf # Cobbler模块配置文件/var/lib/cobbler # Cobbler数据目录/var/lib/cobbler/config # 配置文件/var/lib/cobbler/kickstarts # 默认存放kickstart文件/var/lib/cobbler/loaders # 存放的各种引导程序/var/www/cobbler # 系统安装镜像目录/var/www/cobbler/ks_mirror # 导入的系统镜像列表/var/www/cobbler/images # 导入的系统镜像启动文件/var/www/cobbler/repo_mirror # yum源存储目录/var/log/cobbler # 日志目录/var/log/cobbler/install.log # 客户端系统安装日志/var/log/cobbler/cobbler.log # cobbler日志 配置cobbler 检查Cobbler的配置，如果看不到下面的结果，再次重启cobbler 1234567891011121314[root@cobbler ~]# cobbler checkThe following are potential configuration items that you may want to fix:1 : The 'server' field in /etc/cobbler/settings must be set to something other than localhost, or kickstarting features will not work. This should be a resolvable hostname or IP for the boot server as reachable by all machines that will use it.2 : For PXE to be functional, the 'next_server' field in /etc/cobbler/settings must be set to something other than 127.0.0.1, and should match the IP of the boot server on the PXE network.3 : change 'disable' to 'no' in /etc/xinetd.d/tftp4 : Some network boot-loaders are missing from /var/lib/cobbler/loaders, you may run 'cobbler get-loaders' to download them, or, if you only want to handle x86/x86_64 netbooting, you may ensure that you have installed a *recent* version of the syslinux package installed and can ignore this message entirely. Files in this directory, should you want to support all architectures, should include pxelinux.0, menu.c32, elilo.efi, and yaboot. The 'cobbler get-loaders' command is the easiest way to resolve these requirements.5 : enable and start rsyncd.service with systemctl6 : debmirror package is not installed, it will be required to manage debian deployments and repositories7 : ksvalidator was not found, install pykickstart8 : The default password used by the sample templates for newly installed machines (default_password_crypted in /etc/cobbler/settings) is still set to 'cobbler' and should be changed, try: "openssl passwd -1 -salt 'random-phrase-here' 'your-password-here'" to generate new one9 : fencing tools were not found, and are required to use the (optional) power management features. install cman or fence-agents to use themRestart cobblerd and then run 'cobbler sync' to apply changes. 看到上面出现的问题，然后一个一个的进行解决，先进行设置为可以动态配置，也可以直接更改配置文件。 12345# 设置可以动态修改配置文件[root@cobbler ~]# sed -ri '/allow_dynamic_settings:/c\allow_dynamic_settings: 1' /etc/cobbler/settings[root@cobbler ~]# grep allow_dynamic_settings /etc/cobbler/settings allow_dynamic_settings: 1[root@cobbler ~]# systemctl restart cobblerd 逐个解决上面的问题123456789101112131415161718192021222324252627282930311. server[root@cobbler ~]# cobbler setting edit --name=server --value=192.168.2.1282. next_server[root@cobbler ~]# cobbler setting edit --name=next_server --value=192.168.2.1283. tftp_server[root@cobbler ~]# sed -ri '/disable/c\disable = no' /etc/xinetd.d/tftp[root@cobbler ~]# systemctl enable xinetd[root@cobbler ~]# systemctl restart xinetd4. boot-loaders[root@cobbler ~]# cobbler get-loaders5. rsyncd[root@cobbler ~]# systemctl start rsyncd[root@cobbler ~]# systemctl enable rsyncd6. debmirror [optional]# 这个是可选项的，可以忽略。这里就忽略了7. pykickstart[root@cobbler ~]# yum -y install pykickstart8. default_password_crypted #注意：这里设置的密码，也就是后面安装完系统的初始化登录密码[root@cobbler ~]# openssl passwd -1 -salt `openssl rand -hex 4` 'admin'$1$675f1d08$oJoAMVxdbdKHjQXbGqNTX0[root@cobbler ~]# cobbler setting edit --name=default_password_crypted --value='$1$675f1d08$oJoAMVxdbdKHjQXbGqNTX0'9. fencing tools [optional][root@cobbler ~]# yum -y install fence-agents 解决完成再次查看123456[root@cobbler ~]# cobbler checkThe following are potential configuration items that you may want to fix:1 : debmirror package is not installed, it will be required to manage debian deployments and repositoriesRestart cobblerd and then run 'cobbler sync' to apply changes. 配置DHCP1234567891011[root@cobbler ~]# cobbler setting edit --name=manage_dhcp --value=1# 修改cobbler的dhcp模块，不要直接修改dhcp本身的配置文件，因为cobbler会覆盖[root@cobbler ~]# vim /etc/cobbler/dhcp.template...subnet 192.168.2.0 netmask 255.255.255.0 &#123; #这里改为分配的网段和掩码 #option routers 192.168.1.5; #如果有网关，这里改为网关地址 #option domain-name-servers 192.168.1.1; #如果有DNS，这里改为DNS地址 option subnet-mask 255.255.255.0; #改为分配的IP的掩码 range dynamic-bootp 192.168.2.100 192.168.2.254; #改为分配的IP的范围... 同步cobbler配置 同步cobbler配置，它会根据配置自动修改dhcp等服务。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[root@cobbler ~]# cobbler rsync No such command: rsync[root@cobbler ~]# cobbler synctask started: 2019-06-12_152623_synctask started (id=Sync, time=Wed Jun 12 15:26:23 2019)running pre-sync triggerscleaning treesremoving: /var/www/cobbler/images/centos6.9-x86_64removing: /var/lib/tftpboot/pxelinux.cfg/defaultremoving: /var/lib/tftpboot/grub/imagesremoving: /var/lib/tftpboot/grub/grub-x86.efiremoving: /var/lib/tftpboot/grub/grub-x86_64.efiremoving: /var/lib/tftpboot/grub/efidefaultremoving: /var/lib/tftpboot/images/centos6.9-x86_64removing: /var/lib/tftpboot/s390x/profile_listcopying bootloaderstrying hardlink /var/lib/cobbler/loaders/grub-x86.efi -&gt; /var/lib/tftpboot/grub/grub-x86.efitrying hardlink /var/lib/cobbler/loaders/grub-x86_64.efi -&gt; /var/lib/tftpboot/grub/grub-x86_64.eficopying distros to tftpbootcopying files for distro: centos6.9-x86_64trying hardlink /var/www/cobbler/ks_mirror/centos6.9-x86_64/images/pxeboot/vmlinuz -&gt; /var/lib/tftpboot/images/centos6.9-x86_64/vmlinuztrying hardlink /var/www/cobbler/ks_mirror/centos6.9-x86_64/images/pxeboot/initrd.img -&gt; /var/lib/tftpboot/images/centos6.9-x86_64/initrd.imgcopying imagesgenerating PXE configuration filesgenerating PXE menu structurecopying files for distro: centos6.9-x86_64trying hardlink /var/www/cobbler/ks_mirror/centos6.9-x86_64/images/pxeboot/vmlinuz -&gt; /var/www/cobbler/images/centos6.9-x86_64/vmlinuztrying hardlink /var/www/cobbler/ks_mirror/centos6.9-x86_64/images/pxeboot/initrd.img -&gt; /var/www/cobbler/images/centos6.9-x86_64/initrd.imgWriting template files for centos6.9-x86_64rendering DHCP filesgenerating /etc/dhcp/dhcpd.confrendering TFTPD filesgenerating /etc/xinetd.d/tftpprocessing boot_files for distro: centos6.9-x86_64cleaning link cachesrunning post-sync triggersrunning python triggers from /var/lib/cobbler/triggers/sync/post/*running python trigger cobbler.modules.sync_post_restart_servicesrunning: dhcpd -t -qreceived on stdout: received on stderr: running: service dhcpd restartreceived on stdout: received on stderr: Redirecting to /bin/systemctl restart dhcpd.servicerunning shell triggers from /var/lib/cobbler/triggers/sync/post/*running python triggers from /var/lib/cobbler/triggers/change/*running python trigger cobbler.modules.manage_gendersrunning python trigger cobbler.modules.scm_trackrunning shell triggers from /var/lib/cobbler/triggers/change/**** TASK COMPLETE *** 这时候创建一个新虚拟机可以获取到如下信息，没有镜像选择，只能从本地启动 cobbler命令帮助 命令 说明 cobbler check 核对当前设置是否有问题 cobbler list 列出所有的cobbler元素 cobbler report 列出元素的详细信息 cobbler sync 同步配置到数据目录，更改配置最好都执行一下 cobbler reposync 同步yum仓库 cobbler distro 查看导入的发行版系统信息 cobbler system 查看添加的系统信息 cobbler profile 查看配置信息 cobbler配置安装centos6.x 由于我这里实在centos7系统上面配置的cobbler，所以上传了一个centos6的镜像并进行挂载。 1）创建挂载点，并进行挂载12[root@cobbler ~]# mkdir /centos6[root@cobbler ~]# mount -o loop CentOS-6.9-x86_64-bin-DVD1.iso /centos6 2）查看挂载后的目录12345[root@cobbler ~]# ls /centos6/CentOS_BuildTag images repodata RPM-GPG-KEY-CentOS-Testing-6EFI isolinux RPM-GPG-KEY-CentOS-6 TRANS.TBLEULA Packages RPM-GPG-KEY-CentOS-Debug-6GPL RELEASE-NOTES-en-US.html RPM-GPG-KEY-CentOS-Security-6 3）导入镜像12345[root@cobbler ~]# cobbler import --path=/centos6 --name=centos6.9 --arch=x86_64# --path 镜像路径# --name 为安装源定义一个名字# --arch 指定安装源是32位、64位、ia64, 目前支持的选项有: x86│x86_64│ia64# 安装源的唯一标示就是根据name参数来定义，本例导入成功后，安装源的唯一标示就是：centos6.9，如果重复，系统会提示导入失败。 4）查看导入后镜像信息123456789101112131415161718[root@cobbler ~]# cobbler distro report --name=centos6.9-x86_64Name : centos6.9-x86_64Architecture : x86_64TFTP Boot Files : &#123;&#125;Breed : redhatComment : Fetchable Files : &#123;&#125;Initrd : /var/www/cobbler/ks_mirror/centos6.9-x86_64/images/pxeboot/initrd.imgKernel : /var/www/cobbler/ks_mirror/centos6.9-x86_64/images/pxeboot/vmlinuzKernel Options : &#123;&#125;Kernel Options (Post Install) : &#123;&#125;Kickstart Metadata : &#123;'tree': 'http://@@http_server@@/cblr/links/centos6.9-x86_64'&#125;Management Classes : []OS Version : rhel6Owners : ['admin']Red Hat Management Key : &lt;&lt;inherit&gt;&gt;Red Hat Management Server : &lt;&lt;inherit&gt;&gt;Template Files : &#123;&#125; 5）查看profile信息123456789101112131415161718192021222324252627282930313233[root@cobbler ~]# cobbler profile report --name=centos6.9-x86_64Name : centos6.9-x86_64TFTP Boot Files : &#123;&#125;Comment : DHCP Tag : defaultDistribution : centos6.9-x86_64Enable gPXE? : 0Enable PXE Menu? : 1Fetchable Files : &#123;&#125;Kernel Options : &#123;&#125;Kernel Options (Post Install) : &#123;&#125;Kickstart : /var/lib/cobbler/kickstarts/sample_end.ksKickstart Metadata : &#123;&#125;Management Classes : []Management Parameters : &lt;&lt;inherit&gt;&gt;Name Servers : []Name Servers Search Path : []Owners : ['admin']Parent Profile : Internal proxy : Red Hat Management Key : &lt;&lt;inherit&gt;&gt;Red Hat Management Server : &lt;&lt;inherit&gt;&gt;Repos : []Server Override : &lt;&lt;inherit&gt;&gt;Template Files : &#123;&#125;Virt Auto Boot : 1Virt Bridge : xenbr0Virt CPUs : 1Virt Disk Driver Type : rawVirt File Size(GB) : 5Virt Path : Virt RAM (MB) : 512Virt Type : kvm 6）copy一份profile文件(ks)，进行修改123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899[root@cobbler ~]# cd /var/lib/cobbler/kickstarts/[root@cobbler kickstarts]# lsdefault.ks install_profiles sample_autoyast.xml sample_esxi4.ks sample.ksesxi4-ks.cfg legacy.ks sample_end.ks sample_esxi5.ks sample_old.seedesxi5-ks.cfg pxerescue.ks sample_esx4.ks sample_esxi6.ks sample.seed[root@cobbler kickstarts]# cp sample_end.ks centos6.ks# 编辑centos6的kickstart文件[root@cobbler kickstarts]# vim centos6.ks # This kickstart file should only be used with EL &gt; 5 and/or Fedora &gt; 7.# For older versions please use the sample.ks kickstart file.# Install OS instead of upgradeinstall# Use text mode installtext# System keyboardkeyboard us# System languagelang en_US# System timezonetimezone Asia/ShangHai#Root passwordrootpw --iscrypted $default_password_crypted# System authorization informationauth --useshadow --enablemd5# Firewall configurationfirewall --disabled# SELinux configurationselinux --disabled# Use network installationurl --url=$tree# Clear the Master Boot Recordzerombr# System bootloader configurationbootloader --location=mbr# Partition clearing informationclearpart --all --initlabelpart /boot --fstype=ext4 --size=500part swap --fstype=swap --size=2048part / --fstype=ext4 --grow --size=200 # If any cobbler repo definitions were referenced in the kickstart profile, include them here.$yum_repo_stanza# Network information$SNIPPET('network_config')# Do not configure the X Window Systemskipx# Run the Setup Agent on first bootfirstboot --disable# Reboot after installationreboot%pre$SNIPPET('log_ks_pre')$SNIPPET('kickstart_start')$SNIPPET('pre_install_network_config')# Enable installation monitoring$SNIPPET('pre_anamon')%end%packages$SNIPPET('func_install_if_enabled')@core@basetreenmapwgetlftplrzsztelnet%end%post --nochroot$SNIPPET('log_ks_post_nochroot')%end%post$SNIPPET('log_ks_post')# Start yum configuration$yum_config_stanza# End yum configuration$SNIPPET('post_install_kernel_options')$SNIPPET('post_install_network_config')$SNIPPET('func_register_if_enabled')$SNIPPET('download_config_files')$SNIPPET('koan_environment')$SNIPPET('redhat_register')$SNIPPET('cobbler_register')# Enable post-install boot notification$SNIPPET('post_anamon')# Start final steps$SNIPPET('kickstart_done')# End final stepssed -ri "/^#UseDNS/c\UseDNS no" /etc/ssh/sshd_configsed -ri "/^GSSAPIAuthentication/c\GSSAPIAuthentication no" /etc/ssh/sshd_config%end 7）编辑centos6镜像所使用的kickstart文件123456# 动态编辑指定使用新的kickstart文件[root@cobbler ~]# cobbler profile edit --name=centos6.9-x86_64 --kickstart=/var/lib/cobbler/kickstarts/centos6.ks# 验证是否更改成功[root@cobbler ~]# cobbler profile report --name=centos6.9-x86_64 |grep KickstartKickstart : /var/lib/cobbler/kickstarts/centos6.ks 8）再次同步cobbler配置1[root@cobbler ~]# cobbler sync 9）新建虚拟机进行测试选择centos6.9进行安装，会按照kickstart文件安装并启动 cobbler配置安装centos7.x 我这里cobbler服务器就是7的系统，所以直接挂载/dev/cdrom即可 1）创建挂载点，并进行挂载12[root@cobbler ~]# mkdir /centos7[root@cobbler ~]# mount -o loop /dev/cdrom /centos7 2）查看挂载后的目录123[root@cobbler ~]# ls /centos7/CentOS_BuildTag EULA images LiveOS repodata RPM-GPG-KEY-CentOS-Testing-7EFI GPL isolinux Packages RPM-GPG-KEY-CentOS-7 TRANS.TBL 3）导入镜像1234567891011121314151617181920[root@cobbler ~]# cobbler import --path=/centos7 --name=centos7.4 --arch=x86_64task started: 2019-06-12_165812_importtask started (id=Media import, time=Wed Jun 12 16:58:12 2019)Found a candidate signature: breed=redhat, version=rhel6Found a candidate signature: breed=redhat, version=rhel7Found a matching signature: breed=redhat, version=rhel7Adding distros from path /var/www/cobbler/ks_mirror/centos7.4-x86_64:creating new distro: centos7.4-x86_64trying symlink: /var/www/cobbler/ks_mirror/centos7.4-x86_64 -&gt; /var/www/cobbler/links/centos7.4-x86_64creating new profile: centos7.4-x86_64associating reposchecking for rsync repo(s)checking for rhn repo(s)checking for yum repo(s)starting descent into /var/www/cobbler/ks_mirror/centos7.4-x86_64 for centos7.4-x86_64processing repo at : /var/www/cobbler/ks_mirror/centos7.4-x86_64need to process repo/comps: /var/www/cobbler/ks_mirror/centos7.4-x86_64looking for /var/www/cobbler/ks_mirror/centos7.4-x86_64/repodata/*comps*.xmlKeeping repodata as-is :/var/www/cobbler/ks_mirror/centos7.4-x86_64/repodata*** TASK COMPLETE *** 4）查看导入后镜像信息123456789101112131415161718[root@cobbler ~]# cobbler distro report --name=centos7.4-x86_64Name : centos7.4-x86_64Architecture : x86_64TFTP Boot Files : &#123;&#125;Breed : redhatComment : Fetchable Files : &#123;&#125;Initrd : /var/www/cobbler/ks_mirror/centos7.4-x86_64/images/pxeboot/initrd.imgKernel : /var/www/cobbler/ks_mirror/centos7.4-x86_64/images/pxeboot/vmlinuzKernel Options : &#123;&#125;Kernel Options (Post Install) : &#123;&#125;Kickstart Metadata : &#123;'tree': 'http://@@http_server@@/cblr/links/centos7.4-x86_64'&#125;Management Classes : []OS Version : rhel7Owners : ['admin']Red Hat Management Key : &lt;&lt;inherit&gt;&gt;Red Hat Management Server : &lt;&lt;inherit&gt;&gt;Template Files : &#123;&#125; 5）查看profile信息123456789101112131415161718192021222324252627282930313233[root@cobbler ~]# cobbler profile report --name=centos7.4-x86_64Name : centos7.4-x86_64TFTP Boot Files : &#123;&#125;Comment : DHCP Tag : defaultDistribution : centos7.4-x86_64Enable gPXE? : 0Enable PXE Menu? : 1Fetchable Files : &#123;&#125;Kernel Options : &#123;&#125;Kernel Options (Post Install) : &#123;&#125;Kickstart : /var/lib/cobbler/kickstarts/sample_end.ksKickstart Metadata : &#123;&#125;Management Classes : []Management Parameters : &lt;&lt;inherit&gt;&gt;Name Servers : []Name Servers Search Path : []Owners : ['admin']Parent Profile : Internal proxy : Red Hat Management Key : &lt;&lt;inherit&gt;&gt;Red Hat Management Server : &lt;&lt;inherit&gt;&gt;Repos : []Server Override : &lt;&lt;inherit&gt;&gt;Template Files : &#123;&#125;Virt Auto Boot : 1Virt Bridge : xenbr0Virt CPUs : 1Virt Disk Driver Type : rawVirt File Size(GB) : 5Virt Path : Virt RAM (MB) : 512Virt Type : kvm 7）copy一份profile文件( ks )，进行修改,这里直接copy上面6的了，然后修改修改即可1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495[root@cobbler ~]# cd /var/lib/cobbler/kickstarts/[root@cobbler kickstarts]# cp centos6.ks centos7.ks# 编辑centos7的kickstart文件[root@cobbler kickstarts]# vim centos7.ks# This kickstart file should only be used with EL &gt; 5 and/or Fedora &gt; 7.# For older versions please use the sample.ks kickstart file.# Install OS instead of upgradeinstall# Use text mode installtext# System keyboardkeyboard us# System languagelang en_US# System timezonetimezone Asia/ShangHai#Root passwordrootpw --iscrypted $default_password_crypted# System authorization informationauth --useshadow --enablemd5# Firewall configurationfirewall --disabled# SELinux configurationselinux --disabled# Use network installationurl --url=$tree# Clear the Master Boot Recordzerombr# System bootloader configurationbootloader --location=mbr# Partition clearing informationclearpart --all --initlabelpart /boot --fstype=xfs --size=500part swap --fstype=swap --size=2048part / --fstype=xfs --grow --size=200 # If any cobbler repo definitions were referenced in the kickstart profile, include them here.$yum_repo_stanza# Network information$SNIPPET('network_config')# Do not configure the X Window Systemskipx# Run the Setup Agent on first bootfirstboot --disable# Reboot after installationreboot%pre$SNIPPET('log_ks_pre')$SNIPPET('kickstart_start')$SNIPPET('pre_install_network_config')# Enable installation monitoring$SNIPPET('pre_anamon')%end%packages$SNIPPET('func_install_if_enabled')@core@basetreenmapwgetlftplrzsztelnet%end%post --nochroot$SNIPPET('log_ks_post_nochroot')%end%post$SNIPPET('log_ks_post')# Start yum configuration$yum_config_stanza# End yum configuration$SNIPPET('post_install_kernel_options')$SNIPPET('post_install_network_config')$SNIPPET('func_register_if_enabled')$SNIPPET('download_config_files')$SNIPPET('koan_environment')$SNIPPET('redhat_register')$SNIPPET('cobbler_register')# Enable post-install boot notification$SNIPPET('post_anamon')# Start final steps$SNIPPET('kickstart_done')# End final stepssed -ri "/^#UseDNS/c\UseDNS no" /etc/ssh/sshd_configsed -ri "/^GSSAPIAuthentication/c\GSSAPIAuthentication no" /etc/ssh/sshd_config%end 7）编辑centos7镜像所使用的kickstart文件123456# 动态编辑指定使用新的kickstart文件[root@cobbler ~]# cobbler profile edit --name=centos7.4-x86_64 --kickstart=/var/lib/cobbler/kickstarts/centos7.ks# 验证是否更改成功[root@cobbler ~]# cobbler profile report --name=centos7.4-x86_64 |grep KickstartKickstart : /var/lib/cobbler/kickstarts/centos7.ks 8）再次同步cobbler配置1[root@cobbler ~]# cobbler sync 9）新建虚拟机进行测试选择centos7.4进行安装即可 说明：在client端系统安装时，可以在cobbler服务端上查看日志/var/log/messages，观察安装的每一个cobbler流程 cobbler Web管理界面配置 web界面有很多功能，包括上传镜像、编辑kickstart、等等很多在命令行操作的都可以在web界面直接操作。在上面已经安装了cobbler-web软件，访问地址：https://IP/cobbler_web 即可。默认账号为cobbler，密码也为cobbler 修改密码12345678910111213141516/etc/cobbler/users.conf #Web服务授权配置文件/etc/cobbler/users.digest #用于web访问的用户名密码[root@cobbler ~]# cat /etc/cobbler/users.digest cobbler:Cobbler:a2d6bae81669d707b72c0bd9806e01f3# 设置密码，在Cobbler组添加cobbler用户，输入2遍密码确[root@cobbler ~]# htdigest /etc/cobbler/users.digest "Cobbler" cobblerChanging password for user cobbler in realm CobblerNew password: supermanRe-type new password: superman# 同步配置并重启httpd、cobbler[root@cobbler ~]# cobbler sync[root@cobbler ~]# systemctl restart httpd[root@cobbler ~]# systemctl restart cobblerd 再次登录即使用新设置的密码登录即可。]]></content>
      <categories>
        <category>Cobbler</category>
      </categories>
      <tags>
        <tag>Cobbler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible项目实战lnmp]]></title>
    <url>%2F2019%2F06%2F05%2FAnsible%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98lnmp%2F</url>
    <content type="text"><![CDATA[项目规划 通过ansible roles配置lnmp环境，nginx通过源码编译安装，php通过源码编译安装，mysql通过yum安装（mysql源码编译超级慢）支持系统（centos6.x和centos7.x系列） 说明:将nginx和php源码包放到对应的角色文件下的files目录下，通过vars/main.yml控制安装的版本和路径。如下： 123456[root@ansible roles]# cat nginx/vars/main.yml DOWNLOAD_DIR: "/usr/local/src/" #软件包拷贝到目标主机的存放路径INSTALL_DIR: "/usr/local/" #安装路径NGINX_VERSION: "1.12.2" #软件包版本USER: "nginx" #运行的用户GROUP: "nginx" #运行的组 环境配置参考 角色编写 这里角色都统一放在了/etc/ansible/roles下 安装编译时所需要用到的依赖包1234567891011121314151617181920212223242526272829303132333435363738394041[root@ansible roles]# cat init_pkg.yml #安装源码编译php、nginx时所需要用到的依赖包---- hosts: all remote_user: root tasks: - name: Install Package yum: name=&#123;&#123; item &#125;&#125; state=installed with_items: - gcc-c++ - glibc - glibc-devel - glib2 - glib2-devel - pcre - pcre-devel - zlib - zlib-devel - openssl - openssl-devel - libpng - libpng-devel - freetype - freetype-devel - libxml2 - libxml2-devel - bzip2 - bzip2-devel - ncurses - curl - gdbm-devel - libXpm-devel - libX11-devel - gd-devel - gmp-devel - readline-devel - libxslt-devel - expat-devel - xmlrpc-c - libcurl-devel 1[root@ansible ~]# cd /etc/ansible/roles/ nginx roles1）创建相应文件夹1[root@ansible roles]# mkdir -p nginx/&#123;files,handlers,tasks,templates,vars&#125; 2）最终编写效果1234567891011121314151617181920212223[root@ansible roles]# tree nginxnginx├── files│ ├── nginx-1.12.2.tar.gz│ └── nginx-1.16.0.tar.gz├── handlers│ └── main.yml├── tasks│ ├── config.yml│ ├── copypkg.yml│ ├── group.yml│ ├── install.yml│ ├── main.yml│ ├── service.yml│ └── user.yml├── templates│ ├── nginx.conf.j2│ ├── nginx_init.j2│ └── nginx.service.j2└── vars └── main.yml5 directories, 14 files php roles1）创建相应文件夹1[root@ansible roles]# mkdir -p php/&#123;files,handlers,tasks,templates,vars&#125; 2）最终编写效果1234567891011121314151617181920212223[root@ansible roles]# tree phpphp├── files│ └── php-5.6.40.tar.gz├── handlers│ └── main.yml├── tasks│ ├── config.yml│ ├── copypkg.yml│ ├── group.yml│ ├── install.yml│ ├── main.yml│ ├── service.yml│ └── user.yml├── templates│ ├── php-fpm.conf.j2│ ├── php-fpm.init.j2│ ├── php-fpm.service.j2│ └── php.ini.j2└── vars └── main.yml5 directories, 14 files mysql roles1）创建相应文件夹1[root@ansible roles]# mkdir -p mysql/&#123;files,handlers,tasks,templates,vars&#125; 2）最终编写效果12345678910111213141516[root@ansible roles]# tree mysqlmysql├── files├── handlers│ └── main.yml├── tasks│ ├── config.yml│ ├── install.yml│ ├── main.yml│ └── service.yml├── templates│ ├── my.cnf6.j2│ └── my.cnf7.j2└── vars5 directories, 7 files 角色执行playbook文件编写12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879[root@ansible roles]# cat nginx_roles.yml #源码编译安装nginx---- hosts: all remote_user: root roles: - role: nginx[root@ansible roles]# cat php_roles.yml #源码编译安装nginx---- hosts: all remote_user: root roles: - role: php[root@ansible roles]# cat mysql_roles.yml #yum安装MySQL---- hosts: all remote_user: root roles: - role: mysql [root@ansible roles]# cat lnmp.yml #配置lnmp,创建虚拟主机---- hosts: all remote_user: root roles: - role: nginx - role: php - role: mysql vars: PORT: 8081 WEBDIR: "/opt/www" CONFIGDIR: "/usr/local/nginx/conf/conf.d" tasks: - name: create vhost dir file: name=&#123;&#123; WEBDIR &#125;&#125; state=directory owner=www group=www mode=755 - name: create vhost conf template: src=vhost.conf.j2 dest=&#123;&#123; CONFIGDIR &#125;&#125;/vhost.conf notify: Restart Nginx - name: create index.php shell: "echo '&lt;?php phpinfo(); ?&gt;' &gt; &#123;&#123; WEBDIR &#125;&#125;/index.php" handlers: - name: Restart Nginx service: name=nginx state=restarted# hostslist文件准备，这样方便执行，可以在执行playbook时指定某台机器上运行[root@ansible roles]# cat hostlist 192.168.1.31192.168.1.32192.168.1.33192.168.1.36#所有文件查看[root@ansible roles]# ll 总用量 28-rw-r--r--. 1 root root 53 6月 4 22:37 hostlist-rw-r--r--. 1 root root 824 6月 5 10:53 init_pkg.yml-rw-r--r--. 1 root root 646 6月 5 12:05 lnmp.ymldrwxr-xr-x. 7 root root 77 6月 5 10:44 mysql-rw-r--r--. 1 root root 81 6月 5 10:06 mysql_roles.ymldrwxr-xr-x. 7 root root 77 6月 4 15:37 nginx-rw-r--r--. 1 root root 89 6月 4 17:10 nginx_roles.ymldrwxr-xr-x. 7 root root 77 6月 4 17:18 php-rw-r--r--. 1 root root 87 6月 4 17:37 php_roles.yml-rw-r--r--. 1 root root 811 6月 5 11:53 vhost.conf.j2 所有文件查看12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364[root@ansible roles]# tree .├── hostlist├── init_pkg.yml├── lnmp.yml├── mysql│ ├── files│ ├── handlers│ │ └── main.yml│ ├── tasks│ │ ├── config.yml│ │ ├── install.yml│ │ ├── main.yml│ │ └── service.yml│ ├── templates│ │ ├── my.cnf6.j2│ │ └── my.cnf7.j2│ └── vars├── mysql_roles.yml├── nginx│ ├── files│ │ ├── nginx-1.12.2.tar.gz│ │ └── nginx-1.16.0.tar.gz│ ├── handlers│ │ └── main.yml│ ├── tasks│ │ ├── config.yml│ │ ├── copypkg.yml│ │ ├── group.yml│ │ ├── install.yml│ │ ├── main.yml│ │ ├── service.yml│ │ └── user.yml│ ├── templates│ │ ├── nginx.conf.j2│ │ ├── nginx_init.j2│ │ └── nginx.service.j2│ └── vars│ └── main.yml├── nginx_roles.yml├── php│ ├── files│ │ └── php-5.6.40.tar.gz│ ├── handlers│ │ └── main.yml│ ├── tasks│ │ ├── config.yml│ │ ├── copypkg.yml│ │ ├── group.yml│ │ ├── install.yml│ │ ├── main.yml│ │ ├── service.yml│ │ └── user.yml│ ├── templates│ │ ├── php-fpm.conf.j2│ │ ├── php-fpm.init.j2│ │ ├── php-fpm.service.j2│ │ └── php.ini.j2│ └── vars│ └── main.yml├── php_roles.yml└── vhost.conf.j218 directories, 42 files 执行说明1）单独某一台机器安装nginx1[root@ansible roles]# ansible-playbook -i hostlist nginx_roles.yml --limit 192.168.1.31 2）单独某一台机器安装php1[root@ansible roles]# ansible-playbook -i hostlist php_roles.yml --limit 192.168.1.31 3）单独某一台机器安装mysql1[root@ansible roles]# ansible-playbook -i hostlist mysql_roles.yml --limit 192.168.1.31 4）单独某一台机器部署lnmp1[root@ansible roles]# ansible-playbook -i hostlist lnmp.yml --limit 192.168.1.31 5）所有机器部署php1[root@ansible roles]# ansible-playbook php_roles.yml 6）所有机器部署nginx1[root@ansible roles]# ansible-playbook nginx_roles.yml 7）所有机器部署mysql1[root@ansible roles]# ansible-playbook mysql_roles.yml 8）所有机器部署lnmp1[root@ansible roles]# ansible-playbook lnmp.yml]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible之Roles]]></title>
    <url>%2F2019%2F06%2F03%2FAnsible%E4%B9%8BRoles%2F</url>
    <content type="text"><![CDATA[Roles介绍 ansible自1.2版本引入的新特性，用于层次性、结构化地组织playbook。roles能够根据层次型结构自动装载变量文件、tasks以及handlers等。要使用roles只需要在playbook中使用include指令引入即可。简单来讲，roles就是通过分别将变量、文件、任务、模板及处理器放置于单独的目录中，并可以便捷的include它们的一种机制。角色一般用于基于主机构建服务的场景中，但也可以是用于构建守护进程等场景中。主要使用场景代码复用度较高的情况下。 Roles目录结构 各目录含义解释1234567891011121314roles: &lt;--所有的角色必须放在roles目录下，这个目录可以自定义位置，默认的位置在/etc/ansible/roles project: &lt;---具体的角色项目名称，比如nginx、tomcat、php files： &lt;--用来存放由copy模块或script模块调用的文件。 templates： &lt;--用来存放jinjia2模板，template模块会自动在此目录中寻找jinjia2模板文件。 tasks： &lt;--此目录应当包含一个main.yml文件，用于定义此角色的任务列表，此文件可以使用include包含其它的位于此目录的task文件。 main.yml handlers： &lt;--此目录应当包含一个main.yml文件，用于定义此角色中触发条件时执行的动作。 main.yml vars： &lt;--此目录应当包含一个main.yml文件，用于定义此角色用到的变量。 main.yml defaults： &lt;--此目录应当包含一个main.yml文件，用于为当前角色设定默认变量。 main.yml meta： &lt;--此目录应当包含一个main.yml文件，用于定义此角色的特殊设定及其依赖关系。 main.yml Roles示例 通过ansible roles安装配置httpd服务，此处的roles使用默认的路径/etc/ansible/roles 1）创建目录123456789101112[root@ansible ~]# cd /etc/ansible/roles/# 创建需要用到的目录[root@ansible roles]# mkdir -p httpd/&#123;handlers,tasks,templates,vars&#125;[root@ansible roles]# cd httpd/[root@ansible httpd]# tree ..├── handlers├── tasks├── templates└── vars4 directories, 0 file 2）变量文件准备vars/main.yml1234[root@ansible httpd]# vim vars/main.ymlPORT: 8088 #指定httpd监听的端口USERNAME: www #指定httpd运行用户GROUPNAME: www #指定httpd运行组 3）配置文件模板准备templates/httpd.conf.j212345678# copy一个本地的配置文件放在templates/下并已j2为后缀[root@ansible httpd]# cp /etc/httpd/conf/httpd.conf templates/httpd.conf.j2# 进行一些修改，调用上面定义的变量[root@ansible httpd]# vim templates/httpd.conf.j2Listen &#123;&#123; PORT &#125;&#125; User &#123;&#123; USERNAME &#125;&#125;Group &#123;&#123; GROUPNAME &#125;&#125; 4）任务剧本编写，创建用户、创建组、安装软件、配置、启动等123456789101112131415161718192021222324252627282930313233# 创建组的task[root@ansible httpd]# vim tasks/group.yml- name: Create a Startup Group group: name=www gid=60 system=yes# 创建用户的task[root@ansible httpd]# vim tasks/user.yml- name: Create Startup Users user: name=www uid=60 system=yes shell=/sbin/nologin# 安装软件的task[root@ansible httpd]# vim tasks/install.yml- name: Install Package Httpd yum: name=httpd state=installed# 配置软件的task[root@ansible httpd]# vim tasks/config.yml- name: Copy Httpd Template File template: src=httpd.conf.j2 dest=/etc/httpd/conf/httpd.conf notify: Restart Httpd# 启动软件的task[root@ansible httpd]# vim tasks/start.yml- name: Start Httpd Service service: name=httpd state=started enabled=yes# 编写main.yml，将上面的这些task引入进来[root@ansible httpd]# vim tasks/main.yml- include: group.yml- include: user.yml- include: install.yml- include: config.yml- include: start.ym 5）编写重启httpd的handlers，handlers/main.yml1234[root@ansible httpd]# vim handlers/main.yml# 这里的名字需要和task中的notify保持一致- name: Restart Httpd service: name=httpd state=restarted 6）编写主的httpd_roles.yml文件调用httpd角色1234567[root@ansible httpd]# cd ..[root@ansible roles]# vim httpd_roles.yml---- hosts: all remote_user: root roles: - role: httpd #指定角色名称 7）整体的一个目录结构查看12345678910111213141516171819[root@ansible roles]# tree ..├── httpd│ ├── handlers│ │ └── main.yml│ ├── tasks│ │ ├── config.yml│ │ ├── group.yml│ │ ├── install.yml│ │ ├── main.yml│ │ ├── start.yml│ │ └── user.yml│ ├── templates│ │ └── httpd.conf.j2│ └── vars│ └── main.yml└── httpd_roles.yml5 directories, 10 files 8）测试playbook语法是否正确123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[root@ansible roles]# ansible-playbook -C httpd_roles.yml PLAY [all] **************************************************************************************************TASK [Gathering Facts] **************************************************************************************ok: [192.168.1.33]ok: [192.168.1.32]ok: [192.168.1.31]ok: [192.168.1.36]TASK [httpd : Create a Startup Group] ***********************************************************************changed: [192.168.1.31]changed: [192.168.1.33]changed: [192.168.1.36]changed: [192.168.1.32]TASK [httpd : Create Startup Users] *************************************************************************changed: [192.168.1.33]changed: [192.168.1.32]changed: [192.168.1.31]changed: [192.168.1.36]TASK [httpd : Install Package Httpd] ************************************************************************changed: [192.168.1.33]changed: [192.168.1.32]changed: [192.168.1.31]changed: [192.168.1.36]TASK [httpd : Copy Httpd Template File] *********************************************************************changed: [192.168.1.33]changed: [192.168.1.36]changed: [192.168.1.32]changed: [192.168.1.31]TASK [httpd : Start Httpd Service] **************************************************************************changed: [192.168.1.36]changed: [192.168.1.31]changed: [192.168.1.32]changed: [192.168.1.33]RUNNING HANDLER [httpd : Restart Httpd] *********************************************************************changed: [192.168.1.36]changed: [192.168.1.33]changed: [192.168.1.32]changed: [192.168.1.31]PLAY RECAP **************************************************************************************************192.168.1.31 : ok=7 changed=6 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 192.168.1.32 : ok=7 changed=6 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 192.168.1.33 : ok=7 changed=6 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 192.168.1.36 : ok=7 changed=6 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 9）上面的测试没有问题，正式执行playbook123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102[root@ansible roles]# ansible-playbook -C httpd_roles.yml PLAY [all] **************************************************************************************************TASK [Gathering Facts] **************************************************************************************ok: [192.168.1.33]ok: [192.168.1.32]ok: [192.168.1.31]ok: [192.168.1.36]TASK [httpd : Create a Startup Group] ***********************************************************************changed: [192.168.1.31]changed: [192.168.1.33]changed: [192.168.1.36]changed: [192.168.1.32]TASK [httpd : Create Startup Users] *************************************************************************changed: [192.168.1.33]changed: [192.168.1.32]changed: [192.168.1.31]changed: [192.168.1.36]TASK [httpd : Install Package Httpd] ************************************************************************changed: [192.168.1.33]changed: [192.168.1.32]changed: [192.168.1.31]changed: [192.168.1.36]TASK [httpd : Copy Httpd Template File] *********************************************************************changed: [192.168.1.33]changed: [192.168.1.36]changed: [192.168.1.32]changed: [192.168.1.31]TASK [httpd : Start Httpd Service] **************************************************************************changed: [192.168.1.36]changed: [192.168.1.31]changed: [192.168.1.32]changed: [192.168.1.33]RUNNING HANDLER [httpd : Restart Httpd] *********************************************************************changed: [192.168.1.36]changed: [192.168.1.33]changed: [192.168.1.32]changed: [192.168.1.31]PLAY RECAP **************************************************************************************************192.168.1.31 : ok=7 changed=6 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 192.168.1.32 : ok=7 changed=6 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 192.168.1.33 : ok=7 changed=6 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 192.168.1.36 : ok=7 changed=6 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 [root@ansible roles]# ansible-playbook httpd_roles.yml PLAY [all] **************************************************************************************************TASK [Gathering Facts] **************************************************************************************ok: [192.168.1.32]ok: [192.168.1.33]ok: [192.168.1.31]ok: [192.168.1.36]TASK [httpd : Create a Startup Group] ***********************************************************************changed: [192.168.1.32]changed: [192.168.1.31]changed: [192.168.1.33]changed: [192.168.1.36]TASK [httpd : Create Startup Users] *************************************************************************changed: [192.168.1.31]changed: [192.168.1.33]changed: [192.168.1.32]changed: [192.168.1.36]TASK [httpd : Install Package Httpd] ************************************************************************changed: [192.168.1.31]changed: [192.168.1.33]changed: [192.168.1.32]changed: [192.168.1.36]TASK [httpd : Copy Httpd Template File] *********************************************************************changed: [192.168.1.33]changed: [192.168.1.32]changed: [192.168.1.31]changed: [192.168.1.36]TASK [httpd : Start Httpd Service] **************************************************************************fatal: [192.168.1.36]: FAILED! =&gt; &#123;"changed": false, "msg": "httpd: Syntax error on line 56 of /etc/httpd/conf/httpd.conf: Include directory '/etc/httpd/conf.modules.d' not found\n"&#125;changed: [192.168.1.33]changed: [192.168.1.32]changed: [192.168.1.31]RUNNING HANDLER [httpd : Restart Httpd] *********************************************************************changed: [192.168.1.33]changed: [192.168.1.32]changed: [192.168.1.31]PLAY RECAP **************************************************************************************************192.168.1.31 : ok=7 changed=6 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 192.168.1.32 : ok=7 changed=6 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 192.168.1.33 : ok=7 changed=6 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 192.168.1.36 : ok=5 changed=4 unreachable=0 failed=1 skipped=0 rescued=0 ignored=0 这里有看到报错，其原因是因为192.168.1.36这台机器是centos6系统，别的都是centos7系统，两个系统安装的httpd默认版本是不一样的，所以报错。如果需要优化。参考这里 ansible roles总结 1、编写任务(task)的时候，里面不需要写需要执行的主机，单纯的写某个任务是干什么的即可，装软件的就是装软件的，启动的就是启动的。单独做某一件事即可，最后通过main.yml将这些单独的任务安装执行顺序include进来即可，这样方便维护且一目了然。2、定义变量时候直接安装k:v格式将变量写在vars/main.yml文件即可，然后task或者template直接调用即可，会自动去vars/main.yml文件里面去找。3、定义handlers时候，直接在handlers/main.yml文件中写需要做什么事情即可，多可的话可以全部写在该文件里面，也可以像task那样分开来写，通过include引入一样的可以。在task调用notify时直接写与handlers名字对应即可(二者必须高度一直)。4、模板文件一样放在templates目录下即可，task调用的时后直接写文件名字即可，会自动去到templates里面找。注意：如果是一个角色调用另外一个角色的单个task时后，那么task中如果有些模板或者文件，就得写绝对路径了。]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible之Playbook]]></title>
    <url>%2F2019%2F05%2F29%2FAnsible%E4%B9%8BPlaybook%2F</url>
    <content type="text"><![CDATA[Playbook介绍playbook参考文档 Playbook与ad-hoc相比,是一种完全不同的运用ansible的方式，类似与saltstack的state状态文件。ad-hoc无法持久使用，playbook可以持久使用。playbook是由一个或多个play组成的列表，play的主要功能在于将事先归并为一组的主机装扮成事先通过ansible中的task定义好的角色。从根本上来讲，所谓的task无非是调用ansible的一个module。将多个play组织在一个playbook中，即可以让它们联合起来按事先编排的机制完成某一任务 Playbook核心元素 Hosts 执行的远程主机列表 Tasks 任务集 Varniables 内置变量或自定义变量在playbook中调用 Templates 模板，即使用模板语法的文件，比如配置文件等 Handlers 和notity结合使用，由特定条件触发的操作，满足条件方才执行，否则不执行 tags 标签，指定某条任务执行，用于选择运行playbook中的部分代码。 Playbook语法 playbook使用yaml语法格式，后缀可以是yaml,也可以是yml。 在单一一个playbook文件中，可以连续三个连子号(---)区分多个play。还有选择性的连续三个点好(...)用来表示play的结尾，也可省略。 次行开始正常写playbook的内容，一般都会写上描述该playbook的功能。 使用#号注释代码。 缩进必须统一，不能空格和tab混用。 缩进的级别也必须是一致的，同样的缩进代表同样的级别，程序判别配置的级别是通过缩进结合换行实现的。 YAML文件内容和Linux系统大小写判断方式保持一致，是区分大小写的，k/v的值均需大小写敏感 k/v的值可同行写也可以换行写。同行使用:分隔。 v可以是个字符串，也可以是一个列表 一个完整的代码块功能需要最少元素包括 name: task 一个简单的示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# 创建playbook文件[root@ansible ~]# cat playbook01.yml--- #固定格式- hosts: 192.168.1.31 #定义需要执行主机 remote_user: root #远程用户 vars: #定义变量 http_port: 8088 #变量 tasks: #定义一个任务的开始 - name: create new file #定义任务的名称 file: name=/tmp/playtest.txt state=touch #调用模块，具体要做的事情 - name: create new user user: name=test02 system=yes shell=/sbin/nologin - name: install package yum: name=httpd - name: config httpd template: src=./httpd.conf dest=/etc/httpd/conf/httpd.conf notify: #定义执行一个动作(action)让handlers来引用执行，与handlers配合使用 - restart apache #notify要执行的动作，这里必须与handlers中的name定义内容一致 - name: copy index.html copy: src=/var/www/html/index.html dest=/var/www/html/index.html - name: start httpd service: name=httpd state=started handlers: #处理器：更加tasks中notify定义的action触发执行相应的处理动作 - name: restart apache #要与notify定义的内容相同 service: name=httpd state=restarted #触发要执行的动作#测试页面准备[root@ansible ~]# echo "&lt;h1&gt;playbook test file&lt;/h1&gt;" &gt;&gt;/var/www/html/index.html#配置文件准备[root@ansible ~]# cat httpd.conf |grep ^ListenListen &#123;&#123; http_port &#125;&#125;#执行playbook， 第一次执行可以加-C选项，检查写的playbook是否ok[root@ansible ~]# ansible-playbook playbook01.ymlPLAY [192.168.1.31] *********************************************************************************************TASK [Gathering Facts] ******************************************************************************************ok: [192.168.1.31]TASK [create new file] ******************************************************************************************changed: [192.168.1.31]TASK [create new user] ******************************************************************************************changed: [192.168.1.31]TASK [install package] ******************************************************************************************changed: [192.168.1.31]TASK [config httpd] *********************************************************************************************changed: [192.168.1.31]TASK [copy index.html] ******************************************************************************************changed: [192.168.1.31]TASK [start httpd] **********************************************************************************************changed: [192.168.1.31]PLAY RECAP ******************************************************************************************************192.168.1.31 : ok=7 changed=6 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 # 验证上面playbook执行的结果[root@ansible ~]# ansible 192.168.1.31 -m shell -a 'ls /tmp/playtest.txt &amp;&amp; id test02'192.168.1.31 | CHANGED | rc=0 &gt;&gt;/tmp/playtest.txtuid=990(test02) gid=985(test02) 组=985(test02)[root@ansible ~]# curl 192.168.1.31:8088&lt;h1&gt;playbook test file&lt;/h1&gt; Playbook的运行方式 通过ansible-playbook命令运行格式：ansible-playbook &lt;filename.yml&gt; ... [options]12345678910[root@ansible PlayBook]# ansible-playbook -h#ansible-playbook常用选项：--check or -C #只检测可能会发生的改变，但不真正执行操作--list-hosts #列出运行任务的主机--list-tags #列出playbook文件中定义所有的tags--list-tasks #列出playbook文件中定义的所以任务集--limit #主机列表 只针对主机列表中的某个主机或者某个组执行-f #指定并发数，默认为5个-t #指定tags运行，运行某一个或者多个tags。（前提playbook中有定义tags）-v #显示过程 -vv -vvv更详细 Playbook中元素属性主机与用户 在一个playbook开始时，最先定义的是要操作的主机和用户 123---- hosts: 192.168.1.31 remote_user: root 除了上面的定义外，还可以在某一个tasks中定义要执行该任务的远程用户 1234tasks: - name: run df -h remote_user: test shell: name=df -h 还可以定义使用sudo授权用户执行该任务 12345tasks: - name: run df -h sudo_user: test sudo: yes shell: name=df -h tasks任务列表 每一个task必须有一个名称name,这样在运行playbook时，从其输出的任务执行信息中可以很清楚的辨别是属于哪一个task的，如果没有定义 name，action的值将会用作输出信息中标记特定的task。每一个playbook中可以包含一个或者多个tasks任务列表，每一个tasks完成具体的一件事，（任务模块）比如创建一个用户或者安装一个软件等，在hosts中定义的主机或者主机组都将会执行这个被定义的tasks。 12345tasks: - name: create new file file: path=/tmp/test01.txt state=touch - name: create new user user: name=test001 state=present Handlers与Notify 很多时候当我们某一个配置发生改变，我们需要重启服务，（比如httpd配置文件文件发生改变了）这时候就可以用到handlers和notify了；(当发生改动时)notify actions会在playbook的每一个task结束时被触发，而且即使有多个不同task通知改动的发生，notify actions知会被触发一次；比如多个resources指出因为一个配置文件被改动，所以apache需要重启，但是重新启动的操作知会被执行一次。 123456789101112131415161718192021[root@ansible ~]# cat httpd.yml #用于安装httpd并配置启动---- hosts: 192.168.1.31 remote_user: root tasks: - name: install httpd yum: name=httpd state=installed - name: config httpd template: src=/root/httpd.conf dest=/etc/httpd/conf/httpd.conf notify: - restart httpd - name: start httpd service: name=httpd state=started handlers: - name: restart httpd service: name=httpd state=restarted#这里只要对httpd.conf配置文件作出了修改，修改后需要重启生效，在tasks中定义了restart httpd这个action，然后在handlers中引用上面tasks中定义的notify。 Playbook中变量的使用环境说明：这里配置了两个组，一个apache组和一个nginx组1234567[root@ansible PlayBook]# cat /etc/ansible/hosts[apache]192.168.1.36192.168.1.33[nginx]192.168.1.3[1:2] 命令行指定变量 执行playbook时候通过参数-e传入变量，这样传入的变量在整个playbook中都可以被调用，属于全局变量 1234567891011[root@ansible PlayBook]# cat variables.yml ---- hosts: all remote_user: root tasks: - name: install pkg yum: name=&#123;&#123; pkg &#125;&#125;#执行playbook 指定pkg[root@ansible PlayBook]# ansible-playbook -e "pkg=httpd" variables.yml hosts文件中定义变量 在/etc/ansible/hosts文件中定义变量，可以针对每个主机定义不同的变量，也可以定义一个组的变量，然后直接在playbook中直接调用。注意，组中定义的变量没有单个主机中的优先级高。 123456789101112131415161718192021222324252627# 编辑hosts文件定义变量[root@ansible PlayBook]# vim /etc/ansible/hosts[apache]192.168.1.36 webdir=/opt/test #定义单个主机的变量192.168.1.33[apache:vars] #定义整个组的统一变量webdir=/web/test[nginx]192.168.1.3[1:2][nginx:vars]webdir=/opt/web# 编辑playbook文件[root@ansible PlayBook]# cat variables.yml ---- hosts: all remote_user: root tasks: - name: create webdir file: name=&#123;&#123; webdir &#125;&#125; state=directory #引用变量# 执行playbook[root@ansible PlayBook]# ansible-playbook variables.yml playbook文件中定义变量 编写playbook时，直接在里面定义变量，然后直接引用，可以定义多个变量；注意：如果在执行playbook时，又通过-e参数指定变量的值，那么会以-e参数指定的为准。 123456789101112131415161718192021# 编辑playbook[root@ansible PlayBook]# cat variables.yml ---- hosts: all remote_user: root vars: #定义变量 pkg: nginx #变量1 dir: /tmp/test1 #变量2 tasks: - name: install pkg yum: name=&#123;&#123; pkg &#125;&#125; state=installed #引用变量 - name: create new dir file: name=&#123;&#123; dir &#125;&#125; state=directory #引用变量# 执行playbook[root@ansible PlayBook]# ansible-playbook variables.yml# 如果执行时候又重新指定了变量的值，那么会已重新指定的为准[root@ansible PlayBook]# ansible-playbook -e "dir=/tmp/test2" variables.yml 调用setup模块获取变量 setup模块默认是获取主机信息的，有时候在playbook中需要用到，所以可以直接调用。常用的参数参考 12345678910111213# 编辑playbook文件[root@ansible PlayBook]# cat variables.yml ---- hosts: all remote_user: root tasks: - name: create file file: name=&#123;&#123; ansible_fqdn &#125;&#125;.log state=touch #引用setup中的ansible_fqdn# 执行playbook[root@ansible PlayBook]# ansible-playbook variables.yml 独立的变量YAML文件中定义 为了方便管理将所有的变量统一放在一个独立的变量YAML文件中，laybook文件直接引用文件调用变量即可。 12345678910111213141516171819202122# 定义存放变量的文件[root@ansible PlayBook]# cat var.yml var1: vsftpdvar2: httpd# 编写playbook[root@ansible PlayBook]# cat variables.yml ---- hosts: all remote_user: root vars_files: #引用变量文件 - ./var.yml #指定变量文件的path（这里可以是绝对路径，也可以是相对路径） tasks: - name: install package yum: name=&#123;&#123; var1 &#125;&#125; #引用变量 - name: create file file: name=/tmp/&#123;&#123; var2 &#125;&#125;.log state=touch #引用变量# 执行playbook[root@ansible PlayBook]# ansible-playbook variables.yml Playbook中标签的使用 一个playbook文件中，执行时如果想执行某一个任务，那么可以给每个任务集进行打标签，这样在执行的时候可以通过-t选择指定标签执行，还可以通过--skip-tags选择除了某个标签外全部执行等。 12345678910111213141516171819202122232425262728293031323334353637383940# 编辑playbook[root@ansible PlayBook]# cat httpd.yml ---- hosts: 192.168.1.31 remote_user: root tasks: - name: install httpd yum: name=httpd state=installed tags: inhttpd - name: start httpd service: name=httpd state=started tags: sthttpd - name: restart httpd service: name=httpd state=restarted tags: - rshttpd - rs_httpd# 正常执行的结果[root@ansible PlayBook]# ansible-playbook httpd.yml PLAY [192.168.1.31] **************************************************************************************************************************TASK [Gathering Facts] ***********************************************************************************************************************ok: [192.168.1.31]TASK [install httpd] *************************************************************************************************************************ok: [192.168.1.31]TASK [start httpd] ***************************************************************************************************************************ok: [192.168.1.31]TASK [restart httpd] *************************************************************************************************************************changed: [192.168.1.31]PLAY RECAP ***********************************************************************************************************************************192.168.1.31 : ok=4 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 1）通过-t选项指定tags进行执行12345678910111213# 通过-t指定tags名称，多个tags用逗号隔开[root@ansible PlayBook]# ansible-playbook -t rshttpd httpd.yml PLAY [192.168.1.31] **************************************************************************************************************************TASK [Gathering Facts] ***********************************************************************************************************************ok: [192.168.1.31]TASK [restart httpd] *************************************************************************************************************************changed: [192.168.1.31]PLAY RECAP ***********************************************************************************************************************************192.168.1.31 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 2）通过--skip-tags选项排除不执行的tags123456789101112131415[root@ansible PlayBook]# ansible-playbook --skip-tags inhttpd httpd.yml PLAY [192.168.1.31] **************************************************************************************************************************TASK [Gathering Facts] ***********************************************************************************************************************ok: [192.168.1.31]TASK [start httpd] ***************************************************************************************************************************ok: [192.168.1.31]TASK [restart httpd] *************************************************************************************************************************changed: [192.168.1.31]PLAY RECAP ***********************************************************************************************************************************192.168.1.31 : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Playbook中模板的使用 template模板为我们提供了动态配置服务，使用jinja2语言，里面支持多种条件判断、循环、逻辑运算、比较操作等。其实说白了也就是一个文件，和之前配置文件使用copy一样，只是使用copy，不能根据服务器配置不一样进行不同动态的配置。这样就不利于管理。说明：1、多数情况下都将template文件放在和playbook文件同级的templates目录下（手动创建），这样playbook文件中可以直接引用，会自动去找这个文件。如果放在别的地方，也可以通过绝对路径去指定。2、模板文件后缀名为.j2。 循环参考示例：通过template安装httpd1）playbook文件编写1234567891011121314151617181920[root@ansible PlayBook]# cat testtmp.yml #模板示例---- hosts: all remote_user: root vars: - listen_port: 88 #定义变量 tasks: - name: Install Httpd yum: name=httpd state=installed - name: Config Httpd template: src=httpd.conf.j2 dest=/etc/httpd/conf/httpd.conf #使用模板 notify: Restart Httpd - name: Start Httpd service: name=httpd state=started handlers: - name: Restart Httpd service: name=httpd state=restarted 2）模板文件准备，httpd配置文件准备，这里配置文件端口使用了变量12[root@ansible PlayBook]# cat templates/httpd.conf.j2 |grep ^ListenListen &#123;&#123; listen_port &#125;&#125; 3）查看目录结构12345678# 目录结构[root@ansible PlayBook]# tree ..├── templates│ └── httpd.conf.j2└── testtmp.yml1 directory, 2 files 4）执行playbook，由于192.168.1.36那台机器是6的系统，模板文件里面的配置文件是7上面默认的httpd配置文件，httpd版本不一样(6默认版本为2.2.15，7默认版本为2.4.6)，所以拷贝过去后启动报错。下面使用playbook中的判断语句进行处理；此处先略过1234567891011121314151617181920212223242526272829303132333435363738[root@ansible PlayBook]# ansible-playbook testtmp.yml PLAY [all] ******************************************************************************************TASK [Gathering Facts] ******************************************************************************ok: [192.168.1.36]ok: [192.168.1.32]ok: [192.168.1.33]ok: [192.168.1.31]TASK [Install Httpd] ********************************************************************************ok: [192.168.1.36]ok: [192.168.1.33]ok: [192.168.1.32]ok: [192.168.1.31]TASK [Config Httpd] *********************************************************************************changed: [192.168.1.31]changed: [192.168.1.33]changed: [192.168.1.32]changed: [192.168.1.36]TASK [Start Httpd] **********************************************************************************fatal: [192.168.1.36]: FAILED! =&gt; &#123;"changed": false, "msg": "httpd: Syntax error on line 56 of /etc/httpd/conf/httpd.conf: Include directory '/etc/httpd/conf.modules.d' not found\n"&#125;changed: [192.168.1.32]changed: [192.168.1.33]changed: [192.168.1.31]RUNNING HANDLER [Restart Httpd] *********************************************************************changed: [192.168.1.31]changed: [192.168.1.32]changed: [192.168.1.33]PLAY RECAP ******************************************************************************************192.168.1.31 : ok=5 changed=3 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 192.168.1.32 : ok=5 changed=3 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 192.168.1.33 : ok=5 changed=3 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 192.168.1.36 : ok=3 changed=1 unreachable=0 failed=1 skipped=0 rescued=0 ignored=0 template之whenwhen语句参考 条件测试：如果需要根据变量、facts或此前任务的执行结果来做为某task执行与否的前提时要用到条件测试，通过when语句执行，在task中使用jinja2的语法格式、when语句：在task后添加when子句即可使用条件测试；when语句支持jinja2表达式语法。 类似这样：12345678910tasks: - command: /bin/false register: result ignore_errors: True - command: /bin/something when: result|failed - command: /bin/something_else when: result|success - command: /bin/still/something_else when: result|skipped 示例：通过when语句完善上面的httpd配置1）准备两个配置文件，一个centos6系统httpd配置文件，一个centos7系统httpd配置文件。123456[root@ansible PlayBook]# tree templates/templates/├── httpd6.conf.j2 #6系统2.2.15版本httpd配置文件└── httpd7.conf.j2 #7系统2.4.6版本httpd配置文件0 directories, 2 files 2）修改playbook文件，通过setup模块获取系统版本去判断。setup常用模块12345678910111213141516171819202122232425[root@ansible PlayBook]# cat testtmp.yml #when示例---- hosts: all remote_user: root vars: - listen_port: 88 tasks: - name: Install Httpd yum: name=httpd state=installed - name: Config System6 Httpd template: src=httpd6.conf.j2 dest=/etc/httpd/conf/httpd.conf when: ansible_distribution_major_version == "6" #判断系统版本，为6便执行上面的template配置6的配置文件 notify: Restart Httpd - name: Config System7 Httpd template: src=httpd7.conf.j2 dest=/etc/httpd/conf/httpd.conf when: ansible_distribution_major_version == "7" #判断系统版本，为7便执行上面的template配置7的配置文件 notify: Restart Httpd - name: Start Httpd service: name=httpd state=started handlers: - name: Restart Httpd service: name=httpd state=restarted 3）执行playbook123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@ansible PlayBook]# ansible-playbook testtmp.ymlPLAY [all] ******************************************************************************************TASK [Gathering Facts] ******************************************************************************ok: [192.168.1.31]ok: [192.168.1.32]ok: [192.168.1.33]ok: [192.168.1.36]TASK [Install Httpd] ********************************************************************************ok: [192.168.1.32]ok: [192.168.1.33]ok: [192.168.1.31]ok: [192.168.1.36]TASK [Config System6 Httpd] *************************************************************************skipping: [192.168.1.33]skipping: [192.168.1.31]skipping: [192.168.1.32]changed: [192.168.1.36]TASK [Config System7 Httpd] *************************************************************************skipping: [192.168.1.36]changed: [192.168.1.33]changed: [192.168.1.31]changed: [192.168.1.32]TASK [Start Httpd] **********************************************************************************ok: [192.168.1.36]ok: [192.168.1.31]ok: [192.168.1.32]ok: [192.168.1.33]RUNNING HANDLER [Restart Httpd] *********************************************************************changed: [192.168.1.33]changed: [192.168.1.31]changed: [192.168.1.32]changed: [192.168.1.36]PLAY RECAP ******************************************************************************************192.168.1.31 : ok=5 changed=2 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 192.168.1.32 : ok=5 changed=2 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 192.168.1.33 : ok=5 changed=2 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 192.168.1.36 : ok=5 changed=2 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 template之with_items with_items迭代，当有需要重复性执行的任务时，可以使用迭代机制。对迭代项的引用，固定变量名为“item”，要在task中使用with_items给定要迭代的元素列表。列表格式：&emsp;&ensp;字符串&emsp;&ensp;字典 示例1：通过with_items安装多个不同软件编写playbook12345678910111213[root@ansible PlayBook]# cat testwith.yml # 示例with_items---- hosts: all remote_user: root tasks: - name: Install Package yum: name=&#123;&#123; item &#125;&#125; state=installed #引用item获取值 with_items: #定义with_items - httpd - vsftpd - nginx 上面tasks写法等同于：12345678910---- hosts: all remote_user: root tasks: - name: Install Httpd yum: name=httpd state=installed - name: Install Vsftpd yum: name=vsftpd state=installed - name: Install Nginx yum: name=nginx state=installed 示例2：通过嵌套子变量创建用户并加入不同的组1）编写playbook1234567891011121314151617181920[root@ansible PlayBook]# cat testwith01.yml # 示例with_items嵌套子变量---- hosts: all remote_user: root tasks: - name: Create New Group group: name=&#123;&#123; item &#125;&#125; state=present with_items: - group1 - group2 - group3 - name: Create New User user: name=&#123;&#123; item.name &#125;&#125; group=&#123;&#123; item.group &#125;&#125; state=present with_items: - &#123; name: 'user1', group: 'group1' &#125; - &#123; name: 'user2', group: 'group2' &#125; - &#123; name: 'user3', group: 'group3' &#125; 2）执行playbook并验证123456789101112131415161718192021222324# 执行playbook[root@ansible PlayBook]# ansible-playbook testwith01.yml# 验证是否成功创建用户及组[root@ansible PlayBook]# ansible all -m shell -a 'tail -3 /etc/passwd'192.168.1.36 | CHANGED | rc=0 &gt;&gt;user1:x:500:500::/home/user1:/bin/bashuser2:x:501:501::/home/user2:/bin/bashuser3:x:502:502::/home/user3:/bin/bash192.168.1.32 | CHANGED | rc=0 &gt;&gt;user1:x:1001:1001::/home/user1:/bin/bashuser2:x:1002:1002::/home/user2:/bin/bashuser3:x:1003:1003::/home/user3:/bin/bash192.168.1.31 | CHANGED | rc=0 &gt;&gt;user1:x:1002:1003::/home/user1:/bin/bashuser2:x:1003:1004::/home/user2:/bin/bashuser3:x:1004:1005::/home/user3:/bin/bash192.168.1.33 | CHANGED | rc=0 &gt;&gt;user1:x:1001:1001::/home/user1:/bin/bashuser2:x:1002:1002::/home/user2:/bin/bashuser3:x:1003:1003::/home/user3:/bin/bash template之for if 通过使用for，if可以更加灵活的生成配置文件等需求，还可以在里面根据各种条件进行判断，然后生成不同的配置文件、或者服务器配置相关等。 示例11）编写playbook1234567891011121314[root@ansible PlayBook]# cat testfor01.yml # template for 示例---- hosts: all remote_user: root vars: nginx_vhost_port: - 81 - 82 - 83 tasks: - name: Templage Nginx Config template: src=nginx.conf.j2 dest=/tmp/nginx_test.conf 2）模板文件编写12345678# 循环playbook文件中定义的变量，依次赋值给port[root@ansible PlayBook]# cat templates/nginx.conf.j2 &#123;% for port in nginx_vhost_port %&#125;server&#123; listen: &#123;&#123; port &#125;&#125;; server_name: localhost;&#125;&#123;% endfor %&#125; 3）执行playbook并查看生成结果12345678910111213141516[root@ansible PlayBook]# ansible-playbook testfor01.yml# 去到一个节点看下生成的结果发现自动生成了三个虚拟主机[root@linux ~]# cat /tmp/nginx_test.conf server&#123; listen: 81; server_name: localhost;&#125;server&#123; listen: 82; server_name: localhost;&#125;server&#123; listen: 83; server_name: localhost;&#125; 示例21）编写playbook1234567891011121314151617181920212223[root@ansible PlayBook]# cat testfor02.yml # template for 示例---- hosts: all remote_user: root vars: nginx_vhosts: - web1: listen: 8081 server_name: "web1.example.com" root: "/var/www/nginx/web1" - web2: listen: 8082 server_name: "web2.example.com" root: "/var/www/nginx/web2" - web3: listen: 8083 server_name: "web3.example.com" root: "/var/www/nginx/web3" tasks: - name: Templage Nginx Config template: src=nginx.conf.j2 dest=/tmp/nginx_vhost.conf 2）模板文件编写12345678[root@ansible PlayBook]# cat templates/nginx.conf.j2 &#123;% for vhost in nginx_vhosts %&#125;server&#123; listen: &#123;&#123; vhost.listen &#125;&#125;; server_name: &#123;&#123; vhost.server_name &#125;&#125;; root: &#123;&#123; vhost.root &#125;&#125;; &#125;&#123;% endfor %&#125; 3）执行playbook并查看生成结果12345678910111213141516171819[root@ansible PlayBook]# ansible-playbook testfor02.yml# 去到一个节点看下生成的结果发现自动生成了三个虚拟主机[root@linux ~]# cat /tmp/nginx_vhost.conf server&#123; listen: 8081; server_name: web1.example.com; root: /var/www/nginx/web1; &#125;server&#123; listen: 8082; server_name: web2.example.com; root: /var/www/nginx/web2; &#125;server&#123; listen: 8083; server_name: web3.example.com; root: /var/www/nginx/web3; &#125; 示例3在for循环中再嵌套if判断，让生成的配置文件更加灵活1）编写playbook123456789101112131415161718192021[root@ansible PlayBook]# cat testfor03.yml # template for 示例---- hosts: all remote_user: root vars: nginx_vhosts: - web1: listen: 8081 root: "/var/www/nginx/web1" - web2: server_name: "web2.example.com" root: "/var/www/nginx/web2" - web3: listen: 8083 server_name: "web3.example.com" root: "/var/www/nginx/web3" tasks: - name: Templage Nginx Config template: src=nginx.conf.j2 dest=/tmp/nginx_vhost.conf 2）模板文件编写123456789101112131415# 说明：这里添加了判断，如果listen没有定义的话，默认端口使用8888，如果server_name有定义，那么生成的配置文件中才有这一项。[root@ansible PlayBook]# cat templates/nginx.conf.j2 &#123;% for vhost in nginx_vhosts %&#125;server&#123; &#123;% if vhost.listen is defined %&#125; listen: &#123;&#123; vhost.listen &#125;&#125;; &#123;% else %&#125; listen: 8888; &#123;% endif %&#125; &#123;% if vhost.server_name is defined %&#125; server_name: &#123;&#123; vhost.server_name &#125;&#125;; &#123;% endif %&#125; root: &#123;&#123; vhost.root &#125;&#125;; &#125;&#123;% endfor %&#125; 3）执行playbook并查看生成结果123456789101112131415161718[root@ansible PlayBook]# ansible-playbook testfor03.yml# 去到一个节点看下生成的结果发现自动生成了三个虚拟主机[root@linux ~]# cat /tmp/nginx_vhost.conf server&#123; listen: 8081; root: /var/www/nginx/web1; &#125;server&#123; listen: 8888; server_name: web2.example.com; root: /var/www/nginx/web2; &#125;server&#123; listen: 8083; server_name: web3.example.com; root: /var/www/nginx/web3; &#125; 上面三个示例的图片展示效果示例1示例2示例3]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible Module]]></title>
    <url>%2F2019%2F05%2F27%2FAnsible%20Ad-hoc%E5%B8%B8%E7%94%A8Module%2F</url>
    <content type="text"><![CDATA[Ansible Ad-hoc模式常用模块ansible-doc 常用命令1234567891011# ansible-doc -hUsage: ansible-doc [-l|-F|-s] [options] [-t &lt;plugin type&gt; ] [plugin]-j 以json格式显示所有模块信息-l 列出所有的模块-s 查看模块常用参数# 直接跟模块名，显示模块所有信息[root@ansible ~]# ansible-doc -j[root@ansible ~]# ansible-doc -l[root@ansible ~]# ansible-doc -l |wc -l #统计所有模块个数，ansible2.8共计2834个模块2834 命令相关的模块command ansible默认的模块,执行命令，注意：shell中的&quot;&lt;&quot;, &quot;&gt;&quot;, &quot;|&quot;, &quot;;&quot;, &quot;&amp;&quot;,&quot;$&quot;等特殊字符不能在command模块中使用，如果需要使用，则用shell模块 1234567891011121314151617181920212223242526# 查看模块参数[root@ansible ~]# ansible-doc -s command# 在192.168.1.31服务器上面执行ls命令，默认是在当前用户的家目录/root[root@ansible ~]# ansible 192.168.1.31 -a 'ls'# chdir 先切换工作目录，再执行后面的命令，一般情况下在编译时候使用[root@ansible ~]# ansible 192.168.1.31 -a 'chdir=/tmp pwd'192.168.1.31 | CHANGED | rc=0 &gt;&gt;/tmp# creates 如果creates的文件存在，则执行后面的操作[root@ansible ~]# ansible 192.168.1.31 -a 'creates=/tmp ls /etc/passwd' #tmp目录存在，则不执行后面的ls命令192.168.1.31 | SUCCESS | rc=0 &gt;&gt;skipped, since /tmp exists[root@ansible ~]# ansible 192.168.1.31 -a 'creates=/tmp11 ls /etc/passwd' # tmp11文件不存在，则执行后面的ls命令192.168.1.31 | CHANGED | rc=0 &gt;&gt;/etc/passwd# removes 和creates相反，如果removes的文件存在，才执行后面的操作[root@ansible ~]# ansible 192.168.1.31 -a 'removes=/tmp ls /etc/passwd' #tmp文件存在，则执行了后面的ls命令192.168.1.31 | CHANGED | rc=0 &gt;&gt;/etc/passwd[root@ansible ~]# ansible 192.168.1.31 -a 'removes=/tmp11 ls /etc/passwd' #tmp11文件不存在，则没有执行后面的ls命令192.168.1.31 | SUCCESS | rc=0 &gt;&gt;skipped, since /tmp11 does not exist shell 专门用来执行shell命令的模块，和command模块一样，参数基本一样，都有chdir,creates,removes等参数 123456789101112131415# 查看模块参数[root@ansible ~]# ansible-doc -s shell[root@ansible ~]# ansible 192.168.1.31 -m shell -a 'mkdir /tmp/test'[root@ansible ~]# ansible 192.168.1.31 -m shell -a 'ls /tmp' #执行下面这条命令，每次执行都会更新文件的时间戳[root@ansible ~]# ansible 192.168.1.31 -m shell -a 'cd /tmp/test &amp;&amp; touch 1.txt &amp;&amp; ls' 192.168.1.31 | CHANGED | rc=0 &gt;&gt;1.txt# 由于有时候不想更新文件的创建时间戳，则如果存在就不执行creates[root@ansible ~]# ansible 192.168.1.31 -m shell -a 'creates=/tmp/test/1.txt cd /tmp/test &amp;&amp; touch 1.txt &amp;&amp; ls'192.168.1.31 | SUCCESS | rc=0 &gt;&gt;skipped, since /tmp/test/1.txt exists script 用于在被管理机器上面执行shell脚本的模块，脚本无需在被管理机器上面存在 1234567891011121314151617181920212223# 查看模块参数[root@ansible ~]# ansible-doc -s script# 编写shell脚本[root@ansible ~]# vim ansible_test.sh #!/bin/bashecho `hostname`# 在所有被管理机器上执行该脚本[root@ansible ~]# ansible all -m script -a '/root/ansible_test.sh'192.168.1.32 | CHANGED =&gt; &#123; "changed": true, "rc": 0, "stderr": "Shared connection to 192.168.1.32 closed.\r\n", "stderr_lines": [ "Shared connection to 192.168.1.32 closed." ], "stdout": "linux.node02.com\r\n", "stdout_lines": [ "linux.node02.com" ]&#125;...... 文件相关的模块file 用于对文件的处理，创建，删除，权限控制等 1234567891011121314151617181920212223242526272829# 查看模块参数[root@ansible ~]# ansible-doc -s filepath #要管理的文件路径recurse #递归state： directory #创建目录，如果目标不存在则创建目录及其子目录 touch #创建文件，如果文件存在，则修改文件 属性 absent #删除文件或目录 mode #设置文件或目录权限 owner #设置文件或目录属主信息 group #设置文件或目录属组信息 link #创建软连接，需要和src配合使用 hard #创建硬连接，需要和src配合使用# 创建目录[root@ansible ~]# ansible 192.168.1.31 -m file -a 'path=/tmp/test1 state=directory'# 创建文件[root@ansible ~]# ansible 192.168.1.31 -m file -a 'path=/tmp/test2 state=touch'# 建立软链接（src表示源文件，path表示目标文件）[root@ansible ~]# ansible 192.168.1.31 -m file -a 'src=/tmp/test1 path=/tmp/test3 state=link'# 删除文件[root@ansible ~]# ansible 192.168.1.31 -m file -a 'path=/tmp/test2 state=absent'# 创建文件时同时设置权限等信息[root@ansible ~]# ansible 192.168.1.31 -m file -a 'path=/tmp/test4 state=directory mode=775 owner=root group=root' copy 用于管理端复制文件到远程主机，并可以设置权限，属组，属主等 12345678910111213141516171819202122# 查看模块参数[root@ansible ~]# ansible-doc -s copysrc #需要copy的文件的源路径dest #需要copy的文件的目标路径backup #对copy的文件进行备份content #直接在远程主机被管理文件中添加内容，会覆盖原文件内容mode #对copy到远端的文件设置权限owner #对copy到远端的文件设置属主group #对copy到远端文件设置属组# 复制文件到远程主机并改名[root@ansible ~]# ansible 192.168.1.31 -m copy -a 'dest=/tmp/a.sh src=/root/ansible_test.sh'# 复制文件到远程主机，并备份远程文件,安装时间信息备份文件（当更新文件内容后，重新copy时用到）[root@ansible ~]# ansible 192.168.1.31 -m copy -a 'dest=/tmp/a.sh src=/root/ansible_test.sh backup=yes'# 直接在远程主机a.sh中添加内容[root@ansible ~]# ansible 192.168.1.31 -m copy -a 'dest=/tmp/a.sh content="#!/bin/bash\n echo `uptime`"'# 复制文件到远程主机，并设置权限及属主与属组[root@ansible ~]# ansible 192.168.1.31 -m copy -a 'dest=/tmp/passwd src=/etc/passwd mode=700 owner=root group=root' fetch 用于从被管理机器上面拉取文件，拉取下来的内容会保留目录结构，一般情况用在收集被管理机器的日志文件等 123456789101112131415# 查看模块参数[root@ansible ~]# ansible-doc -s fetchsrc #指定需要从远端机器拉取的文件路径dest #指定从远端机器拉取下来的文件存放路径# 从被管理机器上拉取cron日志文件，默认会已管理节点地址创建一个目录，并存放在内[root@ansible ~]# ansible 192.168.1.31 -m fetch -a 'dest=/tmp src=/var/log/cron'[root@ansible ~]# tree /tmp/192.168.1.31//tmp/192.168.1.31/└── var └── log └── cron2 directories, 1 file 用户相关的模块user 用于对系统用户的管理，用户的创建、删除、家目录、属组等设置 1234567891011121314151617181920212223242526272829303132333435# 查看模块参数[root@ansible ~]# ansible-doc -s username #指定用户的名字home #指定用户的家目录uid #指定用户的uidgroup #指定用户的用户组groups #指定用户的附加组password #指定用户的密码shell #指定用户的登录shellcreate_home #是否创建用户家目录，默认是yesremove #删除用户时，指定是否删除家目录state： absent #删除用户 # 创建用户名指定家目录，指定uid及组[root@ansible ~]# ansible 192.168.1.31 -m user -a 'name=mysql home=/opt/mysql uid=1002 group=root'[root@ansible ~]# ansible 192.168.1.31 -m shell -a 'id mysql &amp;&amp; ls -l /opt'192.168.1.31 | CHANGED | rc=0 &gt;&gt;uid=1002(mysql) gid=0(root) 组=0(root)总用量 0drwx------ 3 mysql root 78 5月 27 18:13 mysql# 创建用户，不创建家目录，并且不能登录[root@ansible ~]# ansible 192.168.1.31 -m user -a 'name=apache shell=/bin/nologin uid=1003 create_home=no'[root@ansible ~]# ansible 192.168.1.31 -m shell -a 'id apache &amp;&amp; tail -1 /etc/passwd'192.168.1.31 | CHANGED | rc=0 &gt;&gt;uid=1003(apache) gid=1003(apache) 组=1003(apache)apache:x:1003:1003::/home/apache:/bin/nologin# 删除用户[root@ansible ~]# ansible 192.168.1.31 -m user -a 'name=apache state=absent'# 删除用户并删除家目录[root@ansible ~]# ansible 192.168.1.31 -m user -a 'name=mysql state=absent remove=yes' group 用于创建组，当创建用户时如果需要指定组，组不存在的话就可以通过group先创建组 12345678910111213141516# 查看模块参数[root@ansible ~]# ansible-doc -s groupname #指定组的名字gid #指定组的gidstate： absent #删除组 present #创建组（默认的状态）# 创建组[root@ansible ~]# ansible 192.168.1.31 -m group -a 'name=www'# 创建组并指定gid[root@ansible ~]# ansible 192.168.1.31 -m group -a 'name=www1 gid=1005'# 删除组[root@ansible ~]# ansible 192.168.1.31 -m group -a 'name=www1 state=absent' 软件包相关的模块yum 用于对软件包的管理，下载、安装、卸载、升级等操作 12345678910111213141516171819202122232425262728293031323334# 查看模块参数[root@ansible ~]# ansible-doc -s yumname #指定要操作的软件包名字download_dir #指定下载软件包的存放路径，需要配合download_only一起使用download_only #只下载软件包，而不进行安装，和yum --downloadonly一样list: installed #列出所有已安装的软件包 updates #列出所有可以更新的软件包 repos #列出所有的yum仓库state: installed, present #安装软件包(两者任选其一都可以) removed, absent #卸载软件包 latest #安装最新软件包 # 列出所有已安装的软件包[root@ansible ~]# ansible 192.168.1.31 -m yum -a 'list=installed'# 列出所有可更新的软件包[root@ansible ~]# ansible 192.168.1.31 -m yum -a 'list=updates'#列出所有的yum仓库[root@ansible ~]# ansible 192.168.1.31 -m yum -a 'list=repos'#只下载软件包并到指定目录下[root@ansible ~]# ansible 192.168.1.31 -m yum -a 'name=httpd download_only=yes download_dir=/tmp'#安装软件包[root@ansible ~]# ansible 192.168.1.31 -m yum -a 'name=httpd state=installed'#卸载软件包[root@ansible ~]# ansible 192.168.1.31 -m yum -a 'name=httpd state=removed'#安装包组，类似yum groupinstall 'Development Tools'[root@ansible ~]# ansible 192.168.1.31 -m yum -a 'name="@Development Tools" state=installed' pip 用于安装python中的包 12345678# 查看模块参数[root@ansible ~]# ansible-doc -s pip# 使用pip时，需要保证被管理机器上有python-pip软件包[root@ansible ~]# ansible 192.168.1.31 -m yum -a 'name=python-pip'# 安装pip包[root@ansible ~]# ansible 192.168.1.31 -m pip -a 'name=flask' service 服务模块，用于对服务进行管理，服务的启动、关闭、开机自启等 123456789101112# 查看模块参数[root@ansible ~]# ansible-doc -s servicename #指定需要管理的服务名enabled #指定是否开机自启动state: #指定服务状态 started #启动服务 stopped #停止服务 restarted #重启服务 reloaded #重载服务# 启动服务，并设置开机自启动 [root@ansible ~]# ansible 192.168.1.31 -m service -a 'name=crond state=started enabled=yes' 计划任务相关的模块cron 用于指定计划任务，和crontab -e一样 12345678910111213141516171819202122232425# 查看模块参数[root@ansible ~]# ansible-doc -s cronjob #指定需要执行的任务minute #分钟hour #小时day #天month #月weekday #周name #对计划任务进行描述state: absetn #删除计划任务# 创建一个计划任务，并描述是干嘛用的[root@ansible ~]# ansible 192.168.1.31 -m cron -a "name='这是一个测试的计划任务' minute=* hour=* day=* month=* weekday=* job='/bin/bash /root/test.sh'"[root@ansible ~]# ansible 192.168.1.31 -m shell -a 'crontab -l'192.168.1.31 | CHANGED | rc=0 &gt;&gt;#Ansible: 这是一个测试的计划任务* * * * * /bin/bash /root/test.sh# 创建一个没有带描述的计划任务[root@ansible ~]# ansible 192.168.1.31 -m cron -a "job='/bin/sh /root/test.sh'"# 删除计划任务[root@ansible ~]# ansible 192.168.1.31 -m cron -a "name='None' job='/bin/sh /root/test.sh' state=absent" 系统信息相关的模块setup 用于获取系统信息的一个模块 12345678910111213141516171819202122232425# 查看模块参数[root@ansible ~]# ansible-doc -s setup# 查看系统所有信息[root@ansible ~]# ansible 192.168.1.31 -m setup# filter 对系统信息进行过滤[root@ansible ~]# ansible 192.168.1.31 -m setup -a 'filter=ansible_all_ipv4_addresses'# 常用的过滤选项ansible_all_ipv4_addresses 所有的ipv4地址ansible_all_ipv6_addresses 所有的ipv6地址ansible_architecture 系统的架构ansible_date_time 系统时间ansible_default_ipv4 系统的默认ipv4地址ansible_distribution 系统名称ansible_distribution_file_variety 系统的家族ansible_distribution_major_version 系统的版本ansible_domain 系统所在的域ansible_fqdn 系统的主机名ansible_hostname 系统的主机名,简写ansible_os_family 系统的家族ansible_processor_cores cpu的核数ansible_processor_count cpu的颗数ansible_processor_vcpus cpu的个数]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible快速入门]]></title>
    <url>%2F2019%2F05%2F26%2FAnsible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[介绍 Ansible是一款简单的运维自动化工具，只需要使用ssh协议连接就可以来进行系统管理，自动化执行命令，部署等任务。 Ansible的特点 1、ansible不需要单独安装客户端，也不需要启动任何服务2、ansible是python中的一套完整的自动化执行任务模块3、ansible playbook 采用yaml配置，对于自动化任务执行过一目了然 Ansible组成结构 Ansible是Ansible的命令工具，核心执行工具；一次性或临时执行的操作都是通过该命令执行。 Ansible Playbook任务剧本（又称任务集），编排定义Ansible任务集的配置文件，由Ansible顺序依次执行，yaml格式。 InventoryAnsible管理主机的清单，默认是/etc/ansible/hosts文件。 ModulesAnsible执行命令的功能模块，Ansible2.3版本为止，共有1039个模块。还可以自定义模块。 Plugins插件，模块功能的补充，常有连接类型插件，循环插件，变量插件，过滤插件，插件功能用的较少。 API提供给第三方程序调用的应用程序编程接口。 环境准备 IP 系统 主机名 描述 192.168.1.30 CentOS7 ansible ansible管理节点 192.168.1.31 CentOS7 linux.node01.com 被管理节点1 192.168.1.32 CentOS7 linux.node02.com 被管理节点2 192.168.1.33 CentOS7 linux.node03.com 被管理节点3 192.168.1.36 CentOS6 linux.node06.com 被管理节点6 Ansible安装1）配置epel源123[root@ansible ~]# wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo[root@ansible ~]# yum clean all[root@ansible ~]# yum makecache 2）安装ansible12345678910[root@ansible ~]# yum -y install ansible# 查看ansible版本[root@ansible ~]# ansible --versionansible 2.8.0 config file = /etc/ansible/ansible.cfg configured module search path = [u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules'] ansible python module location = /usr/lib/python2.7/site-packages/ansible executable location = /usr/bin/ansible python version = 2.7.5 (default, Aug 4 2017, 00:39:18) [GCC 4.8.5 20150623 (Red Hat 4.8.5-16)] Ansible Inventory文件Inventory中文文档 Inventory文件通常用于定义要管理的主机的认证信息，例如ssh登录用户名、密码以及key相关信息。可以同时操作一个组的多台主机，组与主机组之间的关系都是通过inventory文件配置。配置文件路径为：/etc/ansible/hosts 基于密码连接12345678910111213141516171819[root@ansible ~]# vim /etc/ansible/hosts# 方法一 主机+端口+密码[webserver]192.168.1.31 ansible_ssh_port=22 ansible_ssh_user=root ansible_ssh_pass="123456"192.168.1.32 ansible_ssh_port=22 ansible_ssh_user=root ansible_ssh_pass="123456"192.168.1.33 ansible_ssh_port=22 ansible_ssh_user=root ansible_ssh_pass="123456"192.168.1.36 ansible_ssh_port=22 ansible_ssh_user=root ansible_ssh_pass="123456"# 方法二 主机+端口+密码[webserver]192.168.1.3[1:3] ansible_ssh_user=root ansible_ssh_pass="123456"# 方法二 主机+端口+密码[webserver]192.168.1.3[1:3][webserver:vars]ansible_ssh_pass="123456" 基于秘钥连接 基于秘钥连接需要先创建公钥和私钥，并发送给被管理机器 1）生成公私钥12[root@ansible ~]# ssh-keygen[root@ansible ~]# for i in &#123;1,2,3,6&#125;; do ssh-copy-id -i 192.168.1.3$i ; done 2）配置连接1234567891011121314[root@ansible ~]# vim /etc/ansible/hosts# 方法一 主机+端口+密钥[webserver]192.168.1.31:22192.168.1.32192.168.1.33192.168.1.36# 方法一 别名主机+端口+密钥[webserver]node1 ansible_ssh_host=192.168.1.31 ansible_ssh_port=22node2 ansible_ssh_host=192.168.1.32 ansible_ssh_port=22node3 ansible_ssh_host=192.168.1.33 ansible_ssh_port=22node6 ansible_ssh_host=192.168.1.36 ansible_ssh_port=22 主机组的使用123456789101112131415# 主机组变量名+主机+密码[apache]192.168.1.36192.168.1.33[apache.vars]ansible_ssh_pass='123456'# 主机组变量名+主机+密钥[nginx]192.168.1.3[1:2]# 定义多个组，把一个组当另外一个组的组员[webserver:children] #webserver组包括两个子组：apache nginxapachenginx 临时指定inventory1）先编辑一个主机定义清单12345[root@ansible ~]# vim /etc/dockers[dockers]192.168.1.31 ansible_ssh_pass='123456'192.168.1.32192.168.1.33 2）在执行命令是指定inventory1234[root@ansible ~]# ansible dockers -m ping -i /etc/dockers -o 192.168.1.33 | SUCCESS =&gt; &#123;"ansible_facts": &#123;"discovered_interpreter_python": "/usr/bin/python"&#125;, "changed": false, "ping": "pong"&#125;192.168.1.32 | SUCCESS =&gt; &#123;"ansible_facts": &#123;"discovered_interpreter_python": "/usr/bin/python"&#125;, "changed": false, "ping": "pong"&#125;192.168.1.31 | SUCCESS =&gt; &#123;"ansible_facts": &#123;"discovered_interpreter_python": "/usr/bin/python"&#125;, "changed": false, "ping": "pong"&#125; Inventory内置参数 Ansible Ad-HocAd-Hoc中文文档 ad hoc —— 临时的，在ansible中是指需要快速执行，并且不需要保存的命令。说白了就是执行简单的命令——一条命令。对于复杂的命令则为playbook，类似于saltstack的state sls状态文件。 ansible命令格式1）常用命令参数12345678[root@ansible ~]# ansible -hUsage: ansible &lt;host-pattern&gt; [options]-a MODULE_ARGS #模块参数-C, --check #检查语法-f FORKS #并发--list-hosts #列出主机列表-m MODULE_NAME #模块名字-o 使用精简的输出 2）示例12345[root@ansible ~]# ansible webserver -m shell -a 'uptime' -o192.168.1.36 | CHANGED | rc=0 | (stdout) 13:46:14 up 1 day, 9:20, 4 users, load average: 0.00, 0.00, 0.00192.168.1.33 | CHANGED | rc=0 | (stdout) 21:26:33 up 1 day, 8:51, 3 users, load average: 0.00, 0.01, 0.05192.168.1.31 | CHANGED | rc=0 | (stdout) 21:26:33 up 1 day, 8:50, 3 users, load average: 0.00, 0.01, 0.05192.168.1.32 | CHANGED | rc=0 | (stdout) 21:26:33 up 1 day, 8:59, 3 users, load average: 0.00, 0.01, 0.05 3）命令说明 host-pattern格式目标target主机，主机组匹配方式 主机的匹配12345678# 一台目标主机[root@ansible ~]# ansible 192.168.1.31 -m ping# 多台目标主机[root@ansible ~]# ansible 192.168.1.31,192.168.1.32 -m ping# 所有目标主机[root@ansible ~]# ansible all -m ping 组的匹配1234567891011121314151617181920212223242526272829# 组的配置信息如下：这里定义了一个nginx组和一个apache组[root@ansible ~]# ansible nginx --list hosts (2): 192.168.1.31 192.168.1.32[root@ansible ~]# ansible apache --list hosts (3): 192.168.1.36 192.168.1.33 192.168.1.32# 一个组的所有主机匹配[root@ansible ~]# ansible apache -m ping# 匹配apache组中有，但是nginx组中没有的所有主机[root@ansible ~]# ansible 'apache:!nginx' -m ping -o192.168.1.36 | SUCCESS =&gt; &#123;"ansible_facts": &#123;"discovered_interpreter_python": "/usr/bin/python"&#125;, "changed": false, "ping": "pong"&#125;192.168.1.33 | SUCCESS =&gt; &#123;"ansible_facts": &#123;"discovered_interpreter_python": "/usr/bin/python"&#125;, "changed": false, "ping": "pong"&#125;# 匹配apache组和nginx组中都有的机器（并集）[root@ansible ~]# ansible 'apache:&amp;nginx' -m ping -o192.168.1.32 | SUCCESS =&gt; &#123;"ansible_facts": &#123;"discovered_interpreter_python": "/usr/bin/python"&#125;, "changed": false, "ping": "pong"&#125;# 匹配apache组nginx组两个组所有的机器（并集）；等于ansible apache,nginx -m ping[root@ansible ~]# ansible 'apache:nginx' -m ping -o192.168.1.32 | SUCCESS =&gt; &#123;"ansible_facts": &#123;"discovered_interpreter_python": "/usr/bin/python"&#125;, "changed": false, "ping": "pong"&#125;192.168.1.31 | SUCCESS =&gt; &#123;"ansible_facts": &#123;"discovered_interpreter_python": "/usr/bin/python"&#125;, "changed": false, "ping": "pong"&#125;192.168.1.33 | SUCCESS =&gt; &#123;"ansible_facts": &#123;"discovered_interpreter_python": "/usr/bin/python"&#125;, "changed": false, "ping": "pong"&#125;192.168.1.36 | SUCCESS =&gt; &#123;"ansible_facts": &#123;"discovered_interpreter_python": "/usr/bin/python"&#125;, "changed": false, "ping": "pong"&#125;]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux保存history到日志文件]]></title>
    <url>%2F2019%2F05%2F22%2Flinux%E4%BF%9D%E5%AD%98history%E5%88%B0%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[在linux下面，为了保证服务器安全，通常会记录所敲命令的历史记录，但是记录为1000条，并且退出重新登录后，之前的变会没有了，通过编辑/etc/bashrc文件记录历史命令到日志文件下面，并已登录来源IP，登录用户名，登录时间命名日志文件名字 查看默认的history 编辑/etc/bashrc编辑/etc/bashrc文件，加入以下内容，也可以放在/etc/profile文件里123456789101112131415161718192021222324252627# vim /etc/bashrcUSER_IP=$(echo -e "\033[31m\033[1m`who -u am i 2&gt;/dev/null| awk '&#123;print $NF&#125;'|sed -e 's/[()]//g'`\033[0m")IP=$(who -u am i 2&gt;/dev/null| awk '&#123;print $NF&#125;'|sed -e 's/[()]//g')USER=$(whoami)USER_NAME=`echo -e "\033[36m\033[1m$(whoami) \033[0m"`HISTFILESIZE=100000HISTSIZE=4096HISTTIMEFORMAT="%F %T $USER_IP $USER_NAME "if [ "$USER_IP" = "" ]then USER_IP=`hostname`fiif [ ! -d /var/log/history ]then mkdir /var/log/history chmod 777 /var/log/historyfiif [ ! -d /var/log/history/$&#123;LOGNAME&#125; ]then mkdir /var/log/history/$&#123;LOGNAME&#125; chmod 300 /var/log/history/$&#123;LOGNAME&#125;fiDT=`date "+%Y%m%d_%H%M"`export HISTFILE="/var/log/history/$&#123;LOGNAME&#125;/$&#123;DT&#125;_$&#123;USER&#125;_$&#123;IP&#125;"chmod 600 /var/log/history/$&#123;LOGNAME&#125;/* 2&gt;/dev/null 退出重新登录再次查看查看前面保留的历史记录日志12345678910111213141516171819[root@salt-master ~]# ll /var/log/history/总用量 0d-wx------. 2 root root 6 5月 22 14:27 root[root@salt-master ~]# ll /var/log/history/root/总用量 4-rw-------. 1 root root 123 5月 22 14:29 20190522_1427_root_192.168.1.123[root@salt-master ~]# cat /var/log/history/root/20190522_1427_root_192.168.1.123 #1558506461ls#1558506467history#1558506519ll /var/log/history/#1558506571ll /var/log/history/root/#1558506574exit]]></content>
      <categories>
        <category>Linux运维日常</category>
      </categories>
      <tags>
        <tag>Linux运维日常</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[saltstack项目实战]]></title>
    <url>%2F2019%2F05%2F21%2Fsaltstack%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[项目架构规划 后端web服务器使用Nginx+Php作为站点，通过HAproxy做负载均衡，Keepalived做高可用 项目环境准备 说明： 关闭防火墙、selinux、时间同步等host绑定12345678910[root@salt-master ~]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.1.30 salt-master192.168.1.31 salt-minion01192.168.1.32 salt-minion02192.168.1.33 salt-minion03192.168.1.34 salt-minion04[root@salt-master ~]# for i in `seq 4`; do scp /etc/hosts 192.168.1.3$i:/etc/hosts ; done 软件安装参考地址1）Master上软件安装12345[root@salt-master ~]# yum -y install https://mirrors.aliyun.com/saltstack/yum/redhat/salt-repo-latest-2.el7.noarch.rpm[root@salt-master ~]# sed -i "s/repo.saltstack.com/mirrors.aliyun.com\/saltstack/g" /etc/yum.repos.d/salt-latest.repo[root@salt-master ~]# yum -y install salt-master[root@salt-master ~]# systemctl enable salt-master[root@salt-master ~]# systemctl start salt-master 2）Minion上软件安装并配置123456# yum -y install https://mirrors.aliyun.com/saltstack/yum/redhat/salt-repo-latest-2.el7.noarch.rpm# yum -y install salt-minion# cp /etc/salt/minion&#123;,.back&#125;# sed -i '/#master: /c\master: salt-master' /etc/salt/minion# systemctl enable salt-minion# systemctl start salt-minion Master上认证1234567891011121314151617181920212223242526272829303132333435363738394041[root@salt-master ~]# systemctl restart salt-master[root@salt-master ~]# salt-key -L Accepted Keys:Denied Keys:Unaccepted Keys:salt-minion01salt-minion02salt-minion03salt-minion04Rejected Keys:[root@salt-master ~]# salt-key -A -yThe following keys are going to be accepted:Unaccepted Keys:salt-minion01salt-minion02salt-minion03salt-minion04Key for minion salt-minion01 accepted.Key for minion salt-minion02 accepted.Key for minion salt-minion03 accepted.Key for minion salt-minion04 accepted.[root@salt-master ~]# salt-key -L Accepted Keys:salt-minion01salt-minion02salt-minion03salt-minion04Denied Keys:Unaccepted Keys:Rejected Keys:[root@salt-master ~]# salt '*' test.pingsalt-minion01: Truesalt-minion02: Truesalt-minion03: Truesalt-minion04: True Master上state编写state环境设置 说明：该案例在prod环境下配置，在prod下面创建了一个modules的目录，所有的安装配置都放在这个目录下面了，里面分别又对应创建了对应的软件目录，每个软件目录下面的files目录用来存放的是软件包或者配置文件模板1234567891011121314151617181920212223242526272829303132[root@salt-master ~]# vim /etc/salt/masterfile_roots: base: - /srv/salt/base test: - /srv/salt/test prod: - /srv/salt/prod dev: - /srv/salt/dev[root@salt-master ~]# systemctl restart salt-master[root@salt-master ~]# mkdir -p /srv/salt/&#123;base,test,prod,dev&#125;[root@salt-master ~]# mkdir -p /srv/salt/prod/modules/&#123;nginx,php,mysql,haproxy,keepalived,lnmp&#125;/files[root@salt-master ~]# mkdir /srv/salt/prod/modules/user[root@salt-master ~]# tree /srv/salt/prod/modules//srv/salt/prod/modules/├── haproxy│ └── files├── keepalived│ └── files├── lnmp│ └── files├── mysql│ └── files├── nginx│ └── files├── php│ └── files└── user13 directories, 0 files sls文件编写pkg基础包安装源码编译所需要用到的基础软件包1234567891011121314151617181920212223242526272829303132333435363738[root@salt-master ~]# cat /srv/salt/prod/modules/pkg.sls pkg-install: pkg.installed: - pkgs: - gcc - gcc-c++ - make - autoconf - glibc - glibc-devel - glib2 - glib2-devel - pcre - pcre-devel - zlib - zlib-devel - openssl - openssl-devel - libpng - libpng-devel - freetype - freetype-devel - libxml2 - libxml2-devel - bzip2 - bzip2-devel - ncurses - curl - gdbm-devel - libXpm-devel - libX11-devel - gd-devel - gmp-devel - readline-devel - libxslt-devel - expat-devel - xmlrpc-c - xmlrpc-c-devel useradd创建网站运行用户12345678910111213[root@salt-master ~]# cat /srv/salt/prod/modules/user/www.sls www-user-group: group.present: - name: www - gid: 2000 user.present: - name: www - fullname: www - shell: /sbin/nologin - uid: 2000 - gid: 2000 - unless: id www nginx1）软件包准备，及配置文件模板，启动文件模板123456789101112[root@salt-master ~]# cd /srv/salt/prod/modules/nginx/[root@salt-master nginx]# tree .├── files│ ├── nginx-1.12.2.tar.gz│ ├── nginx-1.16.0.tar.gz│ ├── nginx.conf.template│ └── nginx.service.template├── install.sls└── service.sls1 directory, 6 files 2）install.sls1234567891011121314151617181920[root@salt-master nginx]# cat install.sls &#123;% set nginx_version = "1.16.0"%&#125;include: - modules.pkg - modules.user.wwwnginx-install: file.managed: - name: /usr/local/src/nginx-&#123;&#123; nginx_version &#125;&#125;.tar.gz - source: salt://modules/nginx/files/nginx-&#123;&#123; nginx_version &#125;&#125;.tar.gz - user: root - group: root - mode: 644 cmd.run: - name: cd /usr/local/src/ &amp;&amp; tar xf nginx-&#123;&#123; nginx_version &#125;&#125;.tar.gz &amp;&amp; cd nginx-&#123;&#123; nginx_version &#125;&#125; &amp;&amp; ./configure --prefix=/usr/local/nginx-&#123;&#123; nginx_version &#125;&#125; --user=root --group=root --with-http_ssl_module --with-stream --with-http_stub_status_module --with-file-aio --with-http_gzip_static_module &amp;&amp; make &amp;&amp; make install &amp;&amp; ln -s /usr/local/nginx-&#123;&#123; nginx_version &#125;&#125; /usr/local/nginx - unless: test -d /usr/local/nginx-&#123;&#123; nginx_version &#125;&#125; &amp;&amp; test -L /usr/local/nginx - require: - file: nginx-install - pkg: pkg-install 3）service.sls123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@salt-master nginx]# cat service.sls #引入nginx安装slsinclude: - modules.nginx.install#添加systemctlnginx-init: file.managed: - name: /usr/lib/systemd/system/nginx.service - source: salt://modules/nginx/files/nginx.service.template - user: root - group: root - mode: 755 - unless: test -f /usr/lib/systemd/system/nginx.service cmd.run: - name: systemctl daemon-reload - require: - file: nginx-init#配置文件/usr/local/nginx/conf/nginx.conf: file.managed: - source: salt://modules/nginx/files/nginx.conf.template - user: root - group: root - mode: 644#启动nginxnginx-service: file.directory: - name: /usr/local/nginx/conf/conf.d - user: root - group: root - mode: 755 - require: - cmd: nginx-install service.running: - name: nginx - enable: True - reload: True - require: - cmd: nginx-init - watch: - file: /usr/local/nginx/conf/nginx.conf - file: nginx-service php1）软件包准备，及配置文件模板，启动文件模板12345678910111213[root@salt-master ~]# cd /srv/salt/prod/modules/php/[root@salt-master php]# tree.├── files│ ├── php-5.6.40.tar.gz│ ├── php-fpm.conf.template│ ├── php-fpm.service.template│ ├── php-fpm.template│ └── php.ini.template├── install.sls└── service.sls1 directory, 7 files 2）install.sls12345678910111213141516171819[root@salt-master php]# cat install.sls &#123;% set php_version = "5.6.40" %&#125;include: - modules.pkgphp-install: file.managed: - name: /usr/local/src/php-&#123;&#123; php_version &#125;&#125;.tar.gz - source: salt://modules/php/files/php-&#123;&#123; php_version &#125;&#125;.tar.gz - user: root - group: root - mode: 644 cmd.run: - name: cd /usr/local/src/ &amp;&amp; tar xf php-&#123;&#123; php_version &#125;&#125;.tar.gz &amp;&amp; cd php-&#123;&#123; php_version &#125;&#125; &amp;&amp; ./configure --prefix=/usr/local/php-&#123;&#123; php_version &#125;&#125; --with-curl --with-freetype-dir --with-gd --with-gettext --with-iconv-dir --with-jpeg-dir --with-kerberos --with-libdir=lib64 --with-libxml-dir --with-mysql --with-mysqli --with-openssl --with-pcre-regex --with-pdo-mysql --with-dpo-sqlite --with-pear --with-png-dir --with-openssl --with-xmlrpc --with-xsl --with-zlib --enable-fpm --enable-bcmath --enable-libxml --enable-inline-optimization --enable-gd-native-ttf --enable-mbregex --enable-mbstring --enable-opcache --enable-pcntl --enable-shmop --enable-soap --enable-sockets --enable-sysvsem --enable-xml --enable-zip &amp;&amp; make &amp;&amp; make install &amp;&amp; ln -s /usr/local/php-&#123;&#123; php_version &#125;&#125; /usr/local/php - unless: test -d /usr/local/php-&#123;&#123; php_version &#125;&#125; &amp;&amp; test -L /usr/local/php - require: - file: php-install - pkg: pkg-install 3）service.sls123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869[root@salt-master php]# cat service.sls #引入php安装的slsinclude: - modules.php.install#php-ini配置文件配置php-ini: file.managed: - name: /usr/local/php/etc/php.ini - source: salt://modules/php/files/php.ini.template - user: root - group: root - mode: 644 - require: - cmd: php-install cmd.run: - name: ln -s /usr/local/php/etc/php.ini /etc/php.ini - unless: test -L /etc/php.ini - require: - file: php-ini#php-fpm配置文件配置php-fpm: file.managed: - name: /usr/local/php/etc/php-fpm.conf - source: salt://modules/php/files/php-fpm.conf.template - user: root - group: root - mode: 644 - require: - cmd: php-install cmd.run: - name: ln -s /usr/local/php/etc/php-fpm.conf /etc/php-fpm.conf - unless: test -L /etc/php-fpm.conf - require: - file: php-fpm#加入system启动php-systemd: file.managed: - name: /usr/lib/systemd/system/php-fpm.service - source: salt://modules/php/files/php-fpm.service.template - user: root - group: root - mode: 644 - require: - cmd: php-install#加入/etc/init.d/启动php-init: file.managed: - name: /etc/init.d/php-fpm - source: salt://modules/php/files/php-fpm.template - user: root - group: root - mode: 755 - require: - cmd: php-install#启动php-fpmphp-service: service.running: - name: php-fpm - enable: True - require: - file: php-systemd - watch: - file: php-fpm - file: php-ini mysql1）配置文件模板准备123456789[root@salt-master ~]# cd /srv/salt/prod/modules/mysql/[root@salt-master mysql]# tree.├── files│ └── my.cnf├── install.sls└── service.sls1 directory, 3 files 2）install.sls123456[root@salt-master mysql]# cat install.sls mariadb-install: pkg.installed: - pkgs: - mariadb-server - mariadb 3）service.sls1234567891011121314151617181920212223242526[root@salt-master mysql]# cat service.sls #引入mysql安装的slsinclude: - modules.mysql.install#my.cnf配置文件mariadb-config: file.managed: - name: /etc/my.cnf - source: salt://modules/mysql/files/my.cnf - user: root - group: root - mode: 644 - require: - pkg: mariadb-install#启动mariadbmariadb-service: service.running: - name: mariadb - enable: True - watch: - file: mariadb-config - require: - pkg: mariadb-install - file: mariadb-config lnmp1）准备测试文件php info 和nginx虚拟主机配置文件123456789[root@salt-master ~]# cd /srv/salt/prod/modules/lnmp/[root@salt-master lnmp]# tree.├── files│ ├── index.php│ └── www.conf└── www.sls1 directory, 3 files 2）www.sls12345678910111213141516171819202122232425262728293031323334353637383940[root@salt-master lnmp]# cat www.sls #引入nginx、php、mysql的安装include: - modules.nginx.service - modules.php.service - modules.mysql.service#虚拟主机web站点目录创建web-www: file.directory: - name: /opt/www - user: www - group: www - mode: 755#虚拟主机配置文件配置web-www-conf: file.managed: - name: /usr/local/nginx/conf/conf.d/www.conf - source: salt://modules/lnmp/files/www.conf - user: root - group: root - mode: 644 - require: - file: web-www - watch_in: - service: nginx-service - template: jinja - defaults: PORT: 80 IPADDR: &#123;&#123; grains['fqdn_ip4'][0] &#125;&#125;#phpinfo测试文件准备web-index: file.managed: - name: /opt/www/index.php - source: salt://modules/lnmp/files/index.php - user: www - group: www - mode: 644 测试lnmp是否OK1）Top file编写1234[root@salt-master ~]# cat /srv/salt/base/top.sls prod: "salt-minion0[3-4]": - modules.lnmp.www 2）执行高级状态1[root@salt-master ~]# salt '*' state.highstate 3）访问测试 haproxy1）配置文件准备123456789[root@salt-master ~]# cd /srv/salt/prod/modules/haproxy/[root@salt-master haproxy]# tree.├── files│ └── haproxy.cfg├── install.sls└── service.sls1 directory, 3 files 2）install.sls1234[root@salt-master haproxy]# cat install.sls haproxy-install: pkg.installed: - name: haproxy 3）service.sls1234567891011121314151617181920212223242526[root@salt-master haproxy]# cat service.sls #引入haproxy安装的slsinclude: - modules.haproxy.install#配置文件haproxy-config: file.managed: - name: /etc/haproxy/haproxy.cfg - source: salt://modules/haproxy/files/haproxy.cfg - user: root - group: root - mode: 644 - require: - pkg: haproxy-install#启动haproxyhaproxy-service: service.running: - name: haproxy - enable: True - require: - pkg: haproxy-install - file: haproxy-config - watch: - file: haproxy-config keepalived1）配置文件准备123456789[root@salt-master ~]# cd /srv/salt/prod/modules/keepalived/[root@salt-master keepalived]# tree.├── files│ └── keepalived.conf├── install.sls└── service.sls1 directory, 3 files 2）install.sls1234[root@salt-master keepalived]# cat install.sls keepalived-install: pkg.installed: - name: keepalived 3）service.sls12345678910111213141516171819202122232425262728293031323334353637[root@salt-master keepalived]# cat service.sls #引入keepalived安装的slsinclude: - modules.keepalived.install#keepalived配置文件keepalived-config: file.managed: - name: /etc/keepalived/keepalived.conf - source: salt://modules/keepalived/files/keepalived.conf - user: root - group: root - mode: 644 - require: - pkg: keepalived-install - template: jinja - defaults:&#123;% if grains['fqdn'] == "salt-minion01" %&#125; ROUTER_ID: saltstack01 STATE: MASTER PRIORITY: 150&#123;% elif grains['fqdn'] == "salt-minion02" %&#125; ROUTER_ID: saltstack02 STATE: BACKUP PRIORITY: 100&#123;% endif %&#125;#启动keepalivedkeepalived-service: service.running: - name: keepalived - enable: True - require: - pkg: keepalived-install - file: keepalived-config - watch: - file: keepalived-config 整体部署1）top file 编写12345678[root@salt-master ~]# cat /srv/salt/base/top.sls prod: "salt-minion0[3-4]": - modules.lnmp.www "salt-minion0[1-2]": - modules.haproxy.service - modules.keepalived.service 2）高级状态执行1[root@salt-master ~]# salt '*' state.highstate 3）测试访问192.168.1.31和192.168.1.32的状态页访问VIP192.168.1.100 通过上面测试可看到可以成功访问lnmp站点，并且haproxy也ok。访问所有四台服务器都可以得到phpinfo页面，而在生产环境中，我们只是对外提供vip即可。 项目总结1）整体环境查看1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@salt-master ~]# tree /srv/salt/prod/modules//srv/salt/prod/modules/├── haproxy│ ├── files│ │ └── haproxy.cfg│ ├── install.sls│ └── service.sls├── keepalived│ ├── files│ │ └── keepalived.conf│ ├── install.sls│ └── service.sls├── lnmp│ ├── files│ │ ├── index.php│ │ └── www.conf│ └── www.sls├── mysql│ ├── files│ │ └── my.cnf│ ├── install.sls│ └── service.sls├── nginx│ ├── files│ │ ├── nginx-1.12.2.tar.gz│ │ ├── nginx-1.16.0.tar.gz│ │ ├── nginx.conf.template│ │ └── nginx.service.template│ ├── install.sls│ └── service.sls├── php│ ├── files│ │ ├── php-5.6.40.tar.gz│ │ ├── php-fpm.conf.template│ │ ├── php-fpm.service.template│ │ ├── php-fpm.template│ │ └── php.ini.template│ ├── install.sls│ └── service.sls├── pkg.sls└── user └── www.sls13 directories, 27 files 2）如果需要在某台服务器上面单独部署某一部分，参考以下写法：123456789101112131415161718192021[root@salt-master ~]# cat /srv/salt/base/top.sls #部署lnmp及haproxy+keepalivedprod: "salt-minion0[3-4]": - modules.lnmp.www "salt-minion0[1-2]": - modules.haproxy.service - modules.keepalived.service#单实例操作说明：prod: "salt-minion04": - modules.nginx.service #单独安装nginx时 - modules.mysql.service #单独安装mysql时 - modules.php.service #单独安装php时 - modules.keepalived.service #单独安装keepalived时 - modules.haproxy.service #单独安装haproxy时 "salt-minion03": - modules.lnmp.www #单独部署lnmp环境时]]></content>
      <categories>
        <category>Saltstack</category>
      </categories>
      <tags>
        <tag>Saltstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[saltstack接口salt-api]]></title>
    <url>%2F2019%2F05%2F20%2Fsaltstack%E6%8E%A5%E5%8F%A3salt-api%2F</url>
    <content type="text"><![CDATA[介绍参考官档参考官档 SaltStack官方提供有REST API格式的salt-api项目，将使salt与第三方系统集成变得更加简单。 salt-api安装配置1）在salt-master上进行安装1[root@salt-master ~]# yum -y install salt-api 2）自签名证书，生产环境可以购买（说明：如果没有salt-call命令，装上salt-minion即可，依赖于该包）123[root@salt-master ~]# salt-call --local tls.create_self_signed_certlocal: Created Private Key: "/etc/pki/tls/certs/localhost.key." Created Certificate: "/etc/pki/tls/certs/localhost.crt." 3）打开include加载子配置文件，方便管理12[root@salt-master ~]# vim /etc/salt/masterdefault_include: master.d/*.conf 4）配置api配置文件，将上面生成的证书写到配置文件123456[root@salt-master ~]# vim /etc/salt/master.d/api.confrest_cherrypy: host: 192.168.1.30 port: 8000 ssl_crt: /etc/pki/tls/certs/localhost.crt ssl_key: /etc/pki/tls/certs/localhost.key 5）创建认证用户，并设置密码12[root@salt-master ~]# useradd -M -s /sbin/nologin saltapi[root@salt-master ~]# echo 'saltapi' | passwd --stdin saltapi 6）创建认证配置文件12345678[root@salt-master ~]# vim /etc/salt/master.d/auth.confexternal_auth: pam: saltapi: - .* - '@wheel' - '@runner' - '@jobs' 7）重启salt-master和启动salt-api12[root@salt-master ~]# systemctl restart salt-master[root@salt-master ~]# systemctl start salt-api 8）查看salt-api监听端口123[root@salt-master ~]# netstat -anlutp |grep 8000tcp 0 0 192.168.1.30:8000 0.0.0.0:* LISTEN 10904/python tcp 0 0 192.168.1.30:53414 192.168.1.30:8000 TIME_WAIT - 9）验证login登录，获取token字符串12345678910111213141516[root@salt-master ~]# curl -sSk https://192.168.1.30:8000/login \&gt; -H 'Accept: application/x-yaml' \&gt; -d username=saltapi \&gt; -d password=saltapi \&gt; -d eauth=pamreturn:- eauth: pam expire: 1558663247.869537 perms: - .* - '@wheel' - '@runner' - '@jobs' start: 1558620047.869536 token: e8330f642a3addd853c723d63844d29a12de9484 user: saltapi 10）通过api执行test.ping测试连通性1234567891011[root@salt-master ~]# curl -sSk https://192.168.1.30:8000 \&gt; -H 'Accept: application/x-yaml' \&gt; -H 'X-Auth-Token: e8330f642a3addd853c723d63844d29a12de9484'\&gt; -d client=local \&gt; -d tgt='*' \&gt; -d fun=test.pingreturn:- salt-minion01: true salt-minion02: true salt-minion03: true salt-minion04: true 11）通过api执行cmd.run1234567891011[root@salt-master ~]# curl -sSk https://192.168.1.30:8000 \&gt; -H 'Accept: application/x-yaml' \&gt; -H 'X-Auth-Token: e8330f642a3addd853c723d63844d29a12de9484'\&gt; -d client=local \&gt; -d tgt='*' \&gt; -d fun='cmd.run' -d arg='uptime'return:- salt-minion01: ' 22:10:25 up 46 min, 1 user, load average: 0.00, 0.01, 0.05' salt-minion02: ' 22:10:25 up 7 min, 0 users, load average: 0.00, 0.18, 0.15' salt-minion03: ' 22:10:25 up 7 min, 0 users, load average: 0.06, 0.33, 0.26' salt-minion04: ' 22:10:25 up 7 min, 0 users, load average: 0.01, 0.21, 0.16' 12）通过api获取grains信息`123456789101112131415[root@salt-master ~]# curl -sSk https://192.168.1.30:8000/minions/salt-minion01 \&gt; -H 'Accept: application/x-yaml' \&gt; -H 'X-Auth-Token: e8330f642a3addd853c723d63844d29a12de9484'return:- salt-minion01: SSDs: [] biosreleasedate: 05/19/2017 biosversion: '6.00' cpu_flags: - fpu - vme - de - pse - tsc..... 13）使用json格式12345[root@salt-master ~]# curl -sSk https://192.168.1.30:8000/minions/salt-minion01 \&gt; -H 'Accept: application/json' \&gt; -H 'X-Auth-Token: e8330f642a3addd853c723d63844d29a12de9484'&#123;"return": [&#123;"salt-minion01": &#123;"biosversion": "6.00", "kernel": "Linux", "domain": "", "uid": 0, "zmqversion": "4.1.4", "kernelrelease": "3.10.0-693.el7.x86_64", "selinux": &#123;"enforced": "Disabled", "enabled": false&#125;, "serialnumber": "VMware-56 4d 9e a0 21 56 90 87-cd 89 69 32 13 94 17 44", "pid": 1449, "fqdns": [], "ip_interfaces": &#123;"lo": ["127.0.0.1", "::1"], "virbr0": ["192.168.122.1"], "virbr0-nic": [], "ens33": ["192.168.1.31", "192.168.1.100", "fe80::20c:29ff:fe94:1744"]&#125;, "groupname": "root", "fqdn_ip6": ["fe80::20c:29ff:fe94:1744"],....... 总结 salt-api必须使用https，生产环境建议使用可信证书当salt-api服务重启后原token失效]]></content>
      <categories>
        <category>Saltstack</category>
      </categories>
      <tags>
        <tag>Saltstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[saltstack使用salt-ssh]]></title>
    <url>%2F2019%2F05%2F20%2Fsaltstack%E4%BD%BF%E7%94%A8salt-ssh%2F</url>
    <content type="text"><![CDATA[salt-ssh 介绍参考官档 salt-ssh是 0.17.0 新引入的一个功能，不需要minion对客户端进行管理，也可以不需要master；salt-ssh也支持salt大部分的功能：比如grains,modules,state等；salt-ssh没有使用ZeroMQ的通信架构，执行是串行模式 salt-ssh执行原理 salt-ssh是在salt基础上打了一个python包上传到客户端的默认tmp目录下, 在客户端上面解压并执行返回结果,最后删除tmp上传的临时文件。 salt-minion方法是salt-mater先执行语法验证，验证通过后发送到minion，minion收到Msater的状态文件默认保存在/var/cache/salt/minion salt-ssh和salt-minion可以共存，salt-minion不依赖于ssh服务 安装配置1）安装salt-ssh1[root@salt-master ~]# yum -y install salt-ssh 2）修改roster文件，配置需要管理的机器123456789101112[root@salt-master ~]# vim /etc/salt/rostersalt-minion01: host: 192.168.1.31 user: root passwd: 123456 port: 22salt-minion02: host: 192.168.1.32 user: root passwd: 123456 port: 22 3）管理测试 (说明，如果第一次需要输入yes或no进行ssh认证，可以加-i参数自动认证)12345[root@salt-master ~]# salt-ssh '*' test.ping -isalt-minion01: Truesalt-minion02: True 4）salt-ssh命令参数12345678910-r, –raw, –raw-shell # 直接使用shell命令–priv #指定SSH私有密钥文件–roster #定义使用哪个roster系统，如果定义了一个后端数据库，扫描方式，或者用户自定义的的roster系统，默认的就是/etc/salt/roster文件–roster-file #指定roster文件–refresh, –refresh-cache #刷新cache，如果target的grains改变会自动刷新–max-procs #指定进程数，默认为25-i, –ignore-host-keys #当ssh连接时，忽略keys–passwd #指定默认密码–key-deploy #配置keys 设置这个参数对于所有minions用来部署ssh-key认证， 这个参和–passwd结合起来使用会使初始化部署很快很方便。当调用master模块时，并加上参数 –key-deploy 即可在minions生成keys，下次开始就不使用密码 5）salt-ssh执行命令1234567891011121314151617[root@salt-master ~]# salt-ssh '*' -r "uptime"salt-minion01: ---------- retcode: 0 stderr: stdout: root@192.168.1.31's password: 10:06:08 up 1 day, 17:05, 2 users, load average: 0.00, 0.01, 0.05salt-minion02: ---------- retcode: 0 stderr: stdout: root@192.168.1.32's password: 10:06:08 up 1 day, 17:16, 2 users, load average: 0.03, 0.06, 0.06 6）salt-ssh执行状态模块12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273[root@salt-master ~]# salt-ssh '*' state.sls modules.haproxy.service saltenv=prodsalt-minion02:---------- ID: haproxy-install Function: pkg.installed Name: haproxy Result: True Comment: All specified packages are already installed Started: 10:09:48.541019 Duration: 10161.705 ms Changes: ---------- ID: haproxy-config Function: file.managed Name: /etc/haproxy/haproxy.cfg Result: True Comment: File /etc/haproxy/haproxy.cfg is in the correct state Started: 10:09:59.020659 Duration: 54.263 ms Changes: ---------- ID: haproxy-service Function: service.running Name: haproxy Result: True Comment: The service haproxy is already running Started: 10:09:59.079110 Duration: 128.052 ms Changes: Summary for salt-minion02------------Succeeded: 3Failed: 0------------Total states run: 3Total run time: 10.344 ssalt-minion01:---------- ID: haproxy-install Function: pkg.installed Name: haproxy Result: True Comment: All specified packages are already installed Started: 10:10:00.561015 Duration: 2862.018 ms Changes: ---------- ID: haproxy-config Function: file.managed Name: /etc/haproxy/haproxy.cfg Result: True Comment: File /etc/haproxy/haproxy.cfg is in the correct state Started: 10:10:03.905713 Duration: 220.443 ms Changes: ---------- ID: haproxy-service Function: service.running Name: haproxy Result: True Comment: The service haproxy is already running Started: 10:10:04.135607 Duration: 230.231 ms Changes: Summary for salt-minion01------------Succeeded: 3Failed: 0------------Total states run: 3Total run time: 3.313 s Roster说明 salt-ssh需要一个名单系统来确定哪些执行目标，Salt的0.17.0版本中salt-ssh引入roster系统roster系统编译成了一个数据结构，包含了targets，这些targets是一个目标系统主机列表和或如连接到这些targets。 配置文件说明：12345678910111213# target的信息 host: # 远端主机的ip地址或者dns域名 user: # 登录的用户 passwd: # 用户密码,如果不使用此选项，则默认使用秘钥方式# 可选的部分 port: #ssh端口 sudo: #可以通过sudo tty: # 如果设置了sudo，设置这个参数为true priv: # ssh秘钥的文件路径 timeout: # 当建立链接时等待响应时间的秒数 minion_opts: # minion的位置路径 thin_dir: # target系统的存储目录，默认是/tmp/salt-&lt;hash&gt; cmd_umask: # 使用salt-call命令的umask值]]></content>
      <categories>
        <category>Saltstack</category>
      </categories>
      <tags>
        <tag>Saltstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[saltstack数据系统]]></title>
    <url>%2F2019%2F05%2F17%2Fsaltstack%E6%95%B0%E6%8D%AE%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[数据系统Grains 1、Grains是SaltStack收集的有关底层管理系统的静态信息。包括操作系统版本、域名、IP地址、内存、内核、CPU、操作系统类型以及许多其他系统属性。Minion 收集的信息，可以作为Master端匹配目标。2、如果需要自定义grains，需要添加到Salt Minion的/etc/salt/grains文件中（配置文件中定义的默认路径），也可以直接写在配置文件/etc/salt/minion中 Grains官方文档1）资产管理，信息查询12345678#列出所有可用的grains状态模块[root@salt-master ~]# salt '*' grains.ls#打印所有状态信息[root@salt-master ~]# salt '*' grains.items#列出每台minion的本地IP地址[root@salt-master ~]# salt '*' grains.item fqdn_ip4#列出每台minion的操作系统[root@salt-master ~]# salt '*' grains.item os 2）用于匹配12[root@salt-master ~]# salt -G 'os:CentOS' test.ping[root@salt-master ~]# salt -G 'localhost:salt-minion01' test.ping 3）minion自定义grains12345678910111213141516#1.修改配置文件，自定义grains[root@salt-minion01 ~]# vim /etc/salt/miniongrains: roles: - webserver - memcache ipaddr: - 192.168.1.32#2.重启minion[root@salt-minion01 ~]# systemctl restart salt-minion#3.master上测试[root@salt-master ~]# salt -G 'ipaddr:192.168.1.32' test.ping salt-minion01: True 4）Grains优先级问题1231、Grains默认核心信息2、自定义写在/etc/salt/grains文件中的3、自定义写在/etc/salt/minion文件中的 数据系统Pillar Pillar是动态的，Pillar存储在master上，提供给minion。Pillar主要记录一些加密信息，可以确保这些敏感数据不被其他minion看到。比如：软件版本号、用户名密码等。存储格式都是YAML格式 1）在Master端定义Pillar123456789[root@salt-master ~]# vim /etc/salt/masterpillar_roots: base: - /srv/pillar[root@salt-master ~]# mkdir /srv/pillar[root@salt-master ~]# cat /srv/pillar/zabbix.sls Zabbix_Server: 192.168.1.11Zabbix_Name: zabbix.examp.com 2）编写TopFile指定Minion端可以使用1234[root@salt-master ~]# cat /srv/pillar/top.sls base: 'salt-minion01': - zabbix 3）刷新Pillar1[root@salt-master ~]# salt '*' saltutil.refresh_pillar 4）获取对应pillar值123456789101112131415161718[root@salt-master ~]# salt '*' pillar.itemssalt-minion01: ---------- Zabbix_Name: zabbix.examp.com Zabbix_Server: 192.168.1.11salt-minion03: ----------salt-minion02: ----------#获取指定的key[root@salt-master ~]# salt 'salt-minion01' pillar.item Zabbix_Serversalt-minion01: ---------- Zabbix_Server: 192.168.1.11 说明：如果Master更新了新的数值，需要刷新Pillar到Minion才可以获取 Pirrar与Grains对比123类型 数据采集方式 应用场景 定义位置Grains 静态 minion启动时收集 数据查询 目标选择 配置管理 minionPillar 动态 master进行自定义 目标选择 配置管理 敏感数据 master]]></content>
      <categories>
        <category>Saltstack</category>
      </categories>
      <tags>
        <tag>Saltstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[saltstack状态判断unless与onlyif]]></title>
    <url>%2F2019%2F05%2F17%2Fsaltstack%E7%8A%B6%E6%80%81%E5%88%A4%E6%96%ADunless%E4%B8%8Eonlyif%2F</url>
    <content type="text"><![CDATA[很多时候我们在编写state 文件时候需要进行判断，判断该目录或文件是否存在，判断该配置是否已经已添加，然后根据判断结果再决定命令或动作是否执行，这时候就需要用到了状态判断的unless和onlyif。 unlessunless示例：需求创建/tmp/unless.txt文件，存在则不创建，不存在则创建1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[root@salt-master ~]# cat /srv/salt/prod/unless.sls test-unless: cmd.run: - name: touch /tmp/unless.txt - unless: test -f /tmp/unless.txt[root@salt-master ~]# salt 'salt-minion01' state.sls unless saltenv=prodsalt-minion01:---------- ID: test-unless Function: cmd.run Name: touch /tmp/unless.txt Result: True Comment: Command "touch /tmp/unless.txt" run Started: 15:10:51.522319 Duration: 31.822 ms Changes: ---------- pid: 6538 retcode: 0 stderr: stdout:Summary for salt-minion01------------Succeeded: 1 (changed=1)Failed: 0------------Total states run: 1Total run time: 31.822 ms#上面第一次执行，可以看到发生了一次更改，创建了 /tmp/unless.txt文件[root@salt-master ~]# salt 'salt-minion01' state.sls unless saltenv=prodsalt-minion01:---------- ID: test-unless Function: cmd.run Name: touch /tmp/unless.txt Result: True Comment: unless condition is true Started: 15:11:40.819789 Duration: 10.477 ms Changes: Summary for salt-minion01------------Succeeded: 1Failed: 0------------Total states run: 1Total run time: 10.477 ms#第二次执行，可以看到该文件已经存在，并没有再次创建 通过上面的小案例可以看出，当unless返回为真则不执行，当unless返回为假才执行。 onlyif onlyif正好和unless相反，当onlyif返回为真执行，当onlyif返回为假不执行 onlyif示例：需求，当/tmp/onlyif.txt文件存在，则创建/tmp/onlyif目录，不存在，则不创建/tmp/onlyif目录12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758[root@salt-master ~]# cat /srv/salt/prod/onlyif.sls test-onlyif: cmd.run: - name: mkdir /tmp/onlyif - onlyif: test -f /tmp/onlyif.txt[root@salt-master ~]# salt 'salt-minion01' state.sls onlyif saltenv=prodsalt-minion01:---------- ID: test-onlyif Function: cmd.run Name: mkdir /tmp/onlyif Result: True Comment: onlyif condition is false Started: 15:34:56.460583 Duration: 9.612 ms Changes: Summary for salt-minion01------------Succeeded: 1Failed: 0------------Total states run: 1Total run time: 9.612 ms#通过上面可以看到，由于/tmp/onlyif.txt文件不存在，并没有创建；手动创建一个/tmp/onlyif.txt文件再次执行[root@salt-master ~]# salt 'salt-minion01' cmd.run "touch /tmp/onlyif.txt"salt-minion01:[root@salt-master ~]# salt 'salt-minion01' state.sls onlyif saltenv=prodsalt-minion01:---------- ID: test-onlyif Function: cmd.run Name: mkdir /tmp/onlyif Result: True Comment: Command "mkdir /tmp/onlyif" run Started: 15:38:07.712492 Duration: 14.646 ms Changes: ---------- pid: 6871 retcode: 0 stderr: stdout:Summary for salt-minion01------------Succeeded: 1 (changed=1)Failed: 0------------Total states run: 1Total run time: 14.646 ms#可以看到上面我们手动创建了一个/tmp/onlyif.txt文件后再次执行，则发生了改变，在/tmp/创建了onlyif目录 Redis主从架构案例说明：该案例在prod环境配置 1）环境准备，定义file_roots环境12345678[root@salt-master ~]# vim /etc/salt/masterfile_roots: base: - /srv/salt/base dev: - /srv/salt/dev prod: - /srv/salt/prod 2）创建对应环境目录12[root@salt-master ~]# mkdir -p /srv/salt/&#123;base,dev,prod&#125;[root@salt-master ~]# mkdir -p /srv/salt/prod/redis/files/ 3）编写state sls状态文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#初始化redis（安装和基本配置）[root@salt-master ~]# cat /srv/salt/prod/redis/init.sls redis-install: pkg.installed: - name: redisredis-config: file.managed: - name: /etc/redis.conf - source: salt://redis/files/redis.conf - user: root - group: root - mode: 644 - template: jinja - defaults: BIND: &#123;&#123; grains['fqdn_ip4'][0] &#125;&#125; PORT: 6379 DAEMONIZA: 'yes' - require: - pkg: redis-installredis-service: service.running: - name: redis - enable: True - watch: - file: redis-config#master直接引入 init[root@salt-master ~]# cat /srv/salt/prod/redis/master.sls include: - redis.init#slave引入init 并配置主从信息[root@salt-master ~]# cat /srv/salt/prod/redis/slave.sls include: - redis.init#配置主从slave-config: cmd.run: - name: redis-cli -h 192.168.1.34 slaveof 192.168.1.33 6379 - unless: redis-cli -h 192.168.1.34 info |grep role:slave - require: - service: redis-service说明：unless：返回为真则不执行，反之为假则执行 4）配置文件准备123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657[root@salt-master ~]# grep "^[a-Z]" /etc/redis.conf &gt;&gt;/srv/salt/prod/redis/files/redis.conf[root@salt-master ~]# cat /srv/salt/prod/redis/files/redis.conf #这里使用jinjabind &#123;&#123; BIND &#125;&#125;protected-mode yes#这里使用jinjaport &#123;&#123; PORT &#125;&#125;tcp-backlog 511timeout 0tcp-keepalive 300#这里使用jinjadaemonize &#123;&#123; DAEMONIZA &#125;&#125;supervised nopidfile /var/run/redis_6379.pidloglevel noticelogfile /var/log/redis/redis.logdatabases 16save 900 1save 300 10save 60 10000stop-writes-on-bgsave-error yesrdbcompression yesrdbchecksum yesdbfilename dump.rdbdir /var/lib/redisslave-serve-stale-data yesslave-read-only yesrepl-diskless-sync norepl-diskless-sync-delay 5repl-disable-tcp-nodelay noslave-priority 100appendonly noappendfilename "appendonly.aof"appendfsync everysecno-appendfsync-on-rewrite noauto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mbaof-load-truncated yeslua-time-limit 5000slowlog-log-slower-than 10000slowlog-max-len 128latency-monitor-threshold 0notify-keyspace-events ""hash-max-ziplist-entries 512hash-max-ziplist-value 64list-max-ziplist-size -2list-compress-depth 0set-max-intset-entries 512zset-max-ziplist-entries 128zset-max-ziplist-value 64hll-sparse-max-bytes 3000activerehashing yesclient-output-buffer-limit normal 0 0 0client-output-buffer-limit slave 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60hz 10aof-rewrite-incremental-fsync yes 5）top file文件编写123456[root@salt-master ~]# cat /srv/salt/base/top.sls prod: 'salt-minion02': - redis.master 'salt-minion03': - redis.slave 6）整体state文件查看123456789[root@salt-master ~]# tree /srv/salt/prod/redis//srv/salt/prod/redis/├── files│ └── redis.conf├── init.sls├── master.sls└── slave.sls1 directory, 4 files 7）top file高级状态执行123#先测试下看下状态文件是否编写正确，再正式执行[root@salt-master ~]# salt '*' state.highstate test=True[root@salt-master ~]# salt '*' state.highstate]]></content>
      <categories>
        <category>Saltstack</category>
      </categories>
      <tags>
        <tag>Saltstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[saltstack配置管理]]></title>
    <url>%2F2019%2F05%2F15%2Fsaltstack%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[Saltstack状态模块 远程执行模块的执行是过程式，而状态是对minion的一种描述和定义，管理人员不需要关系部署任务如何完成的，只需要描述minion的状态描述。它的和兴是写sls(Salt State file)文件，sls文件默认格式为YAML格式，并默认使用jinja模板，jinja是根据django的模板语言发展而来的语言，简单并强大，支持for if等循环语句。salt state主要用来描述系统，服务，配置文件的状态，常常被称为配置管理。 12345678910mysql-install: #ID声明，必须唯一 pkg.installed: #state状态声明 - pkgs: #选项声明 - mariadb: #选项列表 - mariadb-server说明：一个ID只能出现一次一个ID下相同模块只能使用一次一个ID下不可以使用多个不同模块 模块帮助手册12345678#列出所有状态模块[root@salt-master ~]# salt '*' sys.list_modules#查看指定模块的所有方法[root@salt-master ~]# salt '*' sys.list_state_functions pkg#查看指定模块的使用方法[root@salt-master ~]# salt '*' sys.state_doc pkg#查看指定模块的指定方法的用法[root@salt-master ~]# salt '*' sys.state_doc pkg.installed pkg软件模块pkg模块官档pkg.installed 软件安装1234567php-install: pkg.installed: - pkgs: - php - php-mysql: "&gt;=5.4.16" #指定安装版本 - php-pdo - php-cli 指定安装最新版本的软件1234567php-install: pkg.latest: - pkgs: - php - php-mysql - php-pdo - php-cli file文件模块file模块官档file.managed 下发文件，确保文件存在1234567891011121314151617181920212223[root@salt-master ~]# mkdir /srv/salt/base/files[root@salt-master ~]# cp /etc/httpd/conf/httpd.conf /srv/salt/base/files/[root@salt-master ~]# cat /srv/salt/base/apache_conf.slsapache-config: file.managed: - name: /etc/httpd/conf/httpd.conf - source: salt://files/httpd.conf - user: root - group: root - mode: 644说明：- name: 表示存放目的地址- source: 表示这个文件来自哪里（说明这个文件得提前准备）或者这样写，直接已目的地址命令ID，这样ID也表示目的地址[root@salt-master ~]# cat /srv/salt/base/apache_conf.sls/etc/httpd/conf/httpd.conf: file.managed: - source: salt://files/httpd.conf - user: root - group: root - mode: 644 1234567891011121314151617小示例：[root@salt-master ~]# cat /srv/salt/base/test.sls/tmp/passwd_back: file.managed: - source: salt://files/passwd - user: root - group: root - mode: 644[root@salt-master ~]# cp /etc/passwd /srv/salt/base/files/[root@salt-master ~]# salt '*' state.sls test[root@salt-master ~]# salt '*' cmd.run "ls -l /tmp/passwd_back"salt-minion03: -rw-r--r-- 1 root root 2098 May 15 16:21 /tmp/passwd_backsalt-minion02: -rw-r--r-- 1 root root 2098 May 15 16:21 /tmp/passwd_backsalt-minion01: -rw-r--r-- 1 root root 2098 May 15 16:21 /tmp/passwd_back file.directory 建立目录12345678910111213141516[root@salt-master ~]# cat /srv/salt/base/directory.sls/tmp/saltdir: file.directory: - user: root - group: root - mode: 755 - makedirs: True #如果上一级目录不存在自动创建；类似（mkdir -p）[root@salt-master ~]# salt '*' state.sls directory[root@salt-master ~]# salt '*' cmd.run "ls -d /tmp/saltdir"salt-minion03: /tmp/saltdirsalt-minion02: /tmp/saltdirsalt-minion01: /tmp/saltdir file.recurse 下发整个目录1234567891011[root@salt-master ~]# cat /srv/salt/base/httpd_conf_dir.slshttpd_conf_dir: file.recurse: - name: /etc/httpd/conf.d - source: salt://files/conf.d - file_mode: 600 #文件权限 - dir_mode: 755 #目录权限 - include_empty: True #同步空目录 - clean: True #使用后minion与master保持一致[root@salt-master ~]# rsync -avh /etc/httpd/conf.d /srv/salt/base/files/ file.symlink 建立软链接12345678910111213[root@salt-master ~]# cat /srv/salt/base/target_link.sls/etc/grub.cfg: file.symlink: - target: /etc/grub2.cfg[root@salt-master ~]# salt '*' state.sls target_link[root@salt-master ~]# salt '*' cmd.run "ls -l /etc/grub.cfg"salt-minion03: lrwxrwxrwx 1 root root 14 May 15 16:42 /etc/grub.cfg -&gt; /etc/grub2.cfgsalt-minion01: lrwxrwxrwx 1 root root 14 May 15 16:42 /etc/grub.cfg -&gt; /etc/grub2.cfgsalt-minion02: lrwxrwxrwx 1 root root 14 May 15 16:42 /etc/grub.cfg -&gt; /etc/grub2.cfg service服务模块service模块官档12345678910111213[root@salt-master ~]# cat /srv/salt/base/service_httpd.slshttpd: service.running: - name: httpd #服务名称 - enable: True #开机自启动 - reload: True #允许重载配置文件，不写则是restart或者这样写[root@salt-master ~]# cat /srv/salt/base/service_httpd.slshttpd: #即表示ID，又表示服务名 service.running: - enable: True - reload: True 高级状态模块 当我们想要不同的主机应用不同的配置，那么可以使用高级状态管理 top file来进行管理。可以通过正则，grain模块，或分组名，来进行匹配，再下一级是要执行的state文件 可以将我们的配置需求转换为YAML并在Top file文件中表示： Top file示例1234567891011base: '*': #通过正则去匹配所有minion - app.nginx webserver: #定义的分组名称 - match: nodegroup - app.cron 'os:centos': #通过grains模块匹配 - match: grains - nginx Top file 高级状态的执行1[root@salt-master ~]# salt '*' state.highstate LAMP架构案例说明：该案例在prod环境配置1）环境准备，定义file_roots环境12345678[root@salt-master ~]# vim /etc/salt/masterfile_roots: base: - /srv/salt/base dev: - /srv/salt/dev prod: - /srv/salt/prod 2）创建对应环境目录12[root@salt-master ~]# mkdir -p /srv/salt/&#123;base,dev,prod&#125;[root@salt-master ~]# mkdir /srv/salt/prod/&#123;httpd,php,mysql,files&#125; 3）配置文件准备及测试文件准备12345[root@salt-master ~]# cp /etc/my.cnf /srv/salt/prod/files/[root@salt-master ~]# cp /etc/httpd/conf/httpd.conf /srv/salt/prod/files/[root@salt-master ~]# cp /etc/php.ini /srv/salt/prod/files/[root@salt-master ~]# echo "&lt;h1&gt;LAMP html&lt;/h1&gt;" &gt;&gt;/srv/salt/prod/files/index.html[root@salt-master ~]# echo "&lt;?php phpinfo(); ?&gt;" &gt;&gt; /srv/salt/prod/files/index.php 4）编写state sls状态文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#httpd[root@salt-master ~]# cat /srv/salt/prod/httpd/init.slsapache-install: pkg.installed: - pkgs: - httpd - httpd-toolsapache-config: file.managed: - name: /etc/httpd/conf/httpd.conf - source: salt://files/httpd.conf - user: root - group: root - mode: 644apache-service: service.running: - name: httpd - enable: True#php[root@salt-master ~]# cat /srv/salt/prod/php/init.slsphp-install: pkg.installed: - pkgs: - php - php-mysql - php-pdo - php-cliphp-config: file.managed: - name: /etc/php.ini - source: salt://files/php.ini - user: root - group: root - mode: 644#mysql[root@salt-master ~]# cat /srv/salt/prod/mysql/init.slsmariadb-install: pkg.installed: - pkgs: - mariadb-server - mariadbmariadb-config: file.managed: - name: /etc/my.cnf - source: salt://files/my.cnf - user: root - group: root - mode: 644mariadb-service: service.running: - name: mariadb - enable: True#测试文件[root@salt-master ~]# cat /srv/salt/prod/testfile.sls/var/www/html/index.html: file.managed: - source: salt://files/index.html/var/www/html/index.php: file.managed: - source: salt://files/index.php 6）topfile文件编写1234567[root@salt-master ~]# cat /srv/salt/base/top.slsprod: 'salt-minion*': - httpd.init - php.init - mysql.init - testfile 7）部署LAMP整体state文件查看12345678910111213141516171819[root@salt-master ~]# tree /srv/salt//srv/salt/├── base│ └── top.sls├── dev└── prod ├── files │ ├── httpd.conf │ ├── index.html │ ├── index.php │ ├── my.cnf │ └── php.ini ├── httpd │ └── init.sls ├── mysql │ └── init.sls ├── php │ └── init.sls └── testfile.sls 8）执行topfile1[root@salt-master ~]# salt '*' state.highstate States状态依赖通过上面的lamp可以看出已经可以使用state模块来定义minion的状态了，但是如果一个主机涉及多个状态，并且状态之间相互关联，在执行顺序上有先后之分，那么必须引用requisites来进行控制 关系说明：1、require 我依赖某个状态，我依赖谁2、require_in 我被某个状态依赖，谁依赖我3、watch 我关注某个状态，当状态发生改变，进行restart或者reload操作4、watch_in 我被某个状态关注5、include 我引用谁 1）修改上面lamp状态间依赖关系1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#httpd[root@salt-master ~]# cat /srv/salt/prod/httpd/init.slsapache-install: pkg.installed: - pkgs: - httpd - httpd-toolsapache-config: file.managed: - name: /etc/httpd/conf/httpd.conf - source: salt://files/httpd.conf - user: root - group: root - mode: 644 - require: - pkg: apache-install #表示上面apache-install执行成功，才能执行apache-configapache-service: service.running: - name: httpd - enable: True - require: - file: apache-config - watch: - file: apache-config#php[root@salt-master ~]# cat /srv/salt/prod/php/init.slsphp-install: pkg.installed: - pkgs: - php - php-mysql - php-pdo - php-cli - reqiure_in: - file: php-configphp-config: file.managed: - name: /etc/php.ini - source: salt://files/php.ini - user: root - group: root - mode: 644#mysql[root@salt-master ~]# cat /srv/salt/prod/mysql/init.slsmariadb-install: pkg.installed: - pkgs: - mariadb-server - mariadbmariadb-config: file.managed: - name: /etc/my.cnf - source: salt://files/my.cnf - user: root - group: root - mode: 644 - require: - pkg: mariadb-installmariadb-service: service.running: - name: mariadb - enable: True - reload: True - require: - file: mariadb-config - watch: - file: mariadb-config 2）修改引用关系后include1234567891011121314151617181920212223242526272829303132[root@salt-master ~]# tree /srv/salt//srv/salt/├── base│ └── top.sls├── dev└── prod ├── files │ ├── httpd.conf │ ├── index.html │ ├── index.php │ ├── my.cnf │ └── php.ini ├── httpd │ └── init.sls ├── lamp.sls ├── mysql │ └── init.sls ├── php │ └── init.sls └── testfile.sls[root@salt-master ~]# cat /srv/salt/prod/lamp.slsinclude: - httpd.init - php.init - mysql.init - testfile[root@salt-master ~]# cat /srv/salt/base/top.slsprod: 'salt-minion*': - lamp 3）编写SLS技巧 1、按照状态分类，如果单独使用，清晰明了2、按照服务分类，可以被其它SLS引用 Jinja模板使用 配置文件一般灵活多变，比如配置apache的IP地址或者端口PORT等，则可以动态传值。Jinja官档salt jinja官档 Jinja2 模板包含变量和表达式，变量用 &#123;&#123; … &#125;&#125; 包围，表达式用 &#123;&#37; … &#37;&#125; 包围。变量使用示例：123456789101112131415161718192021222324252627282930313233[root@salt-master ~]# cat /srv/salt/base/var.sls &#123;% set var= 'hello world!' %&#125;test_var: cmd.run: - name: echo "测试变量 &#123;&#123; var &#125;&#125;"[root@salt-master ~]# salt 'salt-minion01' state.sls varsalt-minion01:---------- ID: test_var Function: cmd.run Name: echo "测试变量 hello world!" Result: True Comment: Command "echo "测试变量 hello world!"" run Started: 14:50:58.302424 Duration: 12.358 ms Changes: ---------- pid: 22510 retcode: 0 stderr: stdout: 测试变量 hello world!Summary for salt-minion01------------Succeeded: 1 (changed=1)Failed: 0------------Total states run: 1Total run time: 12.358 ms jinja2 常用变量1、字符串类型12&#123;% set var = 'test' %&#125; #定义变量&#123;&#123; var &#125;&#125; #调用变量 2、列表类型12&#123;% set list = ['one', 'two', 'three'] %&#125;&#123;&#123; list[1] &#125;&#125; #获取变量的第一个值 3、字典类型12&#123;% set dict = &#123;'key1':'value1', 'key2':'value2'&#125; %&#125;&#123;&#123; dict['key1'] &#125;&#125; #获取'key1'的值 示例1：Saltstack使用jinja模块配置apache监听端口12345678910111213141516171819202122#1.告诉file状态模块，需要使用jinja - template: jinja#2.列出参数列表 - defaults: PORT: 8000#3.配置文件引用jinja模板&#123;&#123; PORT &#125;&#125;# 配置示例apache-config: file.managed: - name: /etc/httpd/conf/httpd.conf - source: salt://files/httpd.conf - user: root - group: root - mode: 644 - template: jinja - defaults: PORT: 8000# 修改httpd.conf配置文件引用变量Listen &#123;&#123; PORT &#125;&#125; 示例2：使用grinas 方式进行赋值123456789101112131415#配置示例apache-config: file.managed: - name: /etc/httpd/conf/httpd.conf - source: salt://files/httpd.conf - user: root - group: root - mode: 644 - template: jinja - defaults: PORT: 8000 IPADDR: &#123;&#123; grains['fqdn_ip4'][0] &#125;&#125;# 修改httpd.conf配置文件引用变量Listen &#123;&#123; IPADDR &#125;&#125;:&#123;&#123; PORT &#125;&#125; 示例3：通过jinja+grains根据系统不同安装apache123456789[root@salt-master ~]# cat /srv/salt/base/httpd.sls#根据grains获取的值判别系统后安装软件httpd-install: pkg.installed:&#123;% if grains['os'] == 'CentOS' %&#125; - name: httpd&#123;% elif grains['OS'] == 'Debin' %&#125; - name: apache2&#123;% endif %&#125;]]></content>
      <categories>
        <category>Saltstack</category>
      </categories>
      <tags>
        <tag>Saltstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[saltstack远程执行]]></title>
    <url>%2F2019%2F05%2F14%2Fsaltstack%E8%BF%9C%E7%A8%8B%E6%89%A7%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[安装完Saltstack后可以立即执行shell命令，更新软件包并将文件同时分不到所有受管系统。所有回复都以一致的可配置格式返回。远程执行参考文档：http://docs.saltstack.cn/topics/tutorials/modules.html1234567[root@salt-master ~]# salt '*' cmd.run "uptime"salt-minion01: 15:23:08 up 1 day, 58 min, 2 users, load average: 0.00, 0.03, 0.08salt-minion02: 15:23:08 up 21:38, 2 users, load average: 0.00, 0.04, 0.10salt-minion03: 15:23:08 up 21:36, 2 users, load average: 0.00, 0.04, 0.10 Salt命令的结构语法1salt '&lt;target&gt;' &lt;function&gt; [arguments] 目标主机Target1、通配符匹配123456[root@salt-master ~]# salt '*' test.ping[root@salt-master ~]# salt 'salt-minion01' test.ping[root@salt-master ~]# salt '*01' test.ping[root@salt-master ~]# salt 'salt-minion0[1|2]' test.ping[root@salt-master ~]# salt 'salt-minion0[!1|2]' test.ping[root@salt-master ~]# salt 'salt-minion0?' test.ping 2、列表匹配1[root@salt-master ~]# salt -L 'salt-minion01,salt-minion02' test.ping 3、正则匹配12[root@salt-master ~]# salt -E '^salt' test.ping[root@salt-master ~]# salt -E '^salt.*2$' test.ping 4、IP匹配12[root@salt-master ~]# salt -S '192.168.1.32' test.ping[root@salt-master ~]# salt -S '192.168.1.0/24' test.ping 5、复合匹配1[root@salt-master ~]# salt -C 'G@os:CentOS and S@192.168.1.32' test.ping 6、分组匹配1234567[root@salt-master ~]# vim /etc/salt/masternodegroups: webserver: 'salt-minion01,salt-minion02' dbserver: 'salt-minion03[root@salt-master ~]# systemctl restart salt-master[root@salt-master ~]# salt -N 'webserver' test.ping[root@salt-master ~]# salt -N 'dbserver' test.ping 7、Grains匹配12[root@salt-master ~]# salt -G 'os:CentOS' test.ping[root@salt-master ~]# salt -G 'localhost:salt-minion02' test.ping 说明：上面这些匹配方式在top.sls文件中同样适用。 模块Module test模块多用于测试user模块用于用户管理cmd模块可以执行任意shell命令pkg模块用于软件包管理file模块多用于配置service模块用于服务管理 所有模块列表 test模块123模块名：test功能：用于测试[root@salt-master ~]# salt '*' test.ping user模块123参考：http://docs.saltstack.cn/ref/modules/all/salt.modules.useradd.html#module-salt.modules.useradd# salt '*' user.add name &lt;uid&gt; &lt;gid&gt; &lt;groups&gt; &lt;home&gt; &lt;shell&gt;[root@salt-master ~]# salt '*' user.add testuser cmd模块12345模块名：cmd功能：实现远程的命令行调用执行（默认具备root操作权限，使用时需评估风险）#查看所有minion内存和磁盘使用情况[root@salt-master ~]# salt '*' cmd.run "free -m"[root@salt-master ~]# salt '*' cmd.run "df -h" pkg模块1234567891011121314模块名：pkg功能：软件包状态管理，会根据操作系统不同，选择对应的安装方式（如CentOS系统默认使用yum，Debian系统默认使用apt-get）#安装[root@salt-master ~]# salt '*' pkg.install "vsftpd" #卸载[root@salt-master ~]# salt '*' pkg.remove "vsftpd"#安装最新版本[root@salt-master ~]# salt '*' pkg.latest_version "vsftpd"#更新软件包[root@salt-master ~]# salt '*' pkg.upgrade "vsftpd"#查看帮助手册[root@salt-master ~]# salt '*' pkg file模块1234567891011121314151617181920212223242526272829模块名：file功能：被控主机常见的文件操作，包括文件读写、权限、查找、校验#校验所有minion主机文件的加密信息，支持md5、sha1、sha224、shs256、sha384、sha512加密算法[root@salt-master ~]# salt '*' file.get_sum /etc/passwd md5#修改所有minion主机/etc/passwd文件的属组、用户权限、等价于chown root:root /etc/passwd[root@salt-master ~]# salt '*' file.chown /etc/passwd root root#获取所有minion主机/etc/passwd的stats信息[root@salt-master ~]# salt '*' file.stats /etc/passwd#获取所有minion主机/etc/passwd的权限mode,如755,644[root@salt-master ~]# salt '*' file.get_mode /etc/passwd#修改所有minion主机/etc/passwd的权限mode为0644[root@salt-master ~]# salt '*' file.set_mode /etc/passwd 0644#在所有minion主机创建/opt/test目录[root@salt-master ~]# salt '*' file.mkdir /opt/test#在所有minion主机穿件/tmp/test.conf文件[root@salt-master ~]# salt '*' file.touch /tmp/test.conf#将所有minion主机/tmp/test.conf文件追加内容'maxclient 100'[root@salt-master ~]# salt '*' file.append /tmp/test.conf 'maxclient 100'#删除所有minion主机的/tmp/test.conf文件[root@salt-master ~]# salt '*' file.remove /tmp/test.conf service模块12345678910111213模块名：service功能：被控主机程序包服务管理#开启（enable）禁用（disable）salt '*' service.enable &lt;service name&gt;salt '*' service.disabled &lt;service name&gt;#reload、restart、start、stop、status操作salt '*' service.reload &lt;service name&gt;salt '*' service.restart &lt;service name&gt;salt '*' service.start &lt;service name&gt;salt '*' service.stop &lt;service name&gt;salt '*' service.status &lt;service name&gt;]]></content>
      <categories>
        <category>Saltstack</category>
      </categories>
      <tags>
        <tag>Saltstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[saltstack基本入门]]></title>
    <url>%2F2019%2F05%2F13%2Fsaltstack%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[saltstack介绍Salt，,一种全新的基础设施管理方式，部署轻松，在几分钟内可运行起来，扩展性好，很容易管理上万台服务器，速度够快，服务器之间秒级通讯 主要功能远程执行配置管理Stalstack官方文档 Saltstack原理 Salt使用server-agent通信模型，服务端组件被称为Salt master，agent被称为Salt minionSalt master主要负责向Salt minions发送命令，然后聚合并显示这些命令的结果。一个Salt master可以管理多个minion系统Salt server与Salt minion通信的连接由Salt minion发起，这也意味着Salt minion上不需要打开任何传入端口（从而减少攻击）。Salt server使用端口4505和4506，必须打开端口才能接收到访问连接 Publisher （端口4505）所有Salt minions都需要建立一个持续连接到他们收听消息的发布者端口。命令是通过此端口异步发送给所有连接，这使命令可以在大量系统上同时执行。 Request Server （端口4506）Salt minios根据需要连接到请求服务器，将结果发送给Salt master，并安全地获取请求的文件或特定minion相关的数据值（称为Salt pillar）。连接到这个端口的连接在Salt master和Salt minion之间是1:1（不是异步）。 123456[root@salt-master ~]# lsof -i:4505COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsalt-mast 81121 root 16u IPv4 304019 0t0 TCP *:4505 (LISTEN)salt-mast 81121 root 18u IPv4 304082 0t0 TCP salt-master:4505-&gt;salt-minion03:37240 (ESTABLISHED)salt-mast 81121 root 19u IPv4 307610 0t0 TCP salt-master:4505-&gt;salt-minion01:47804 (ESTABLISHED)salt-mast 81121 root 20u IPv4 307611 0t0 TCP salt-master:4505-&gt;salt-minion02:58594 (ESTABLISHED) 快速安装1.1 配置 yum 仓库1234567# 使用官方自带yum[root@salt-master ~]# yum install https://repo.saltstack.com/yum/redhat/salt-repo-latest.el7.noarch.rpm# 或者使用阿里云的yum（建议使用阿里云的，速度快一点）[root@salt-master ~]# yum -y install https://mirrors.aliyun.com/saltstack/yum/redhat/salt-repo-latest-2.el7.noarch.rpm[root@salt-master ~]# sed -i "s/repo.saltstack.com/mirrors.aliyun.com\/saltstack/g" /etc/yum.repos.d/salt-latest.repo[root@salt-master ~]# yum clean all[root@salt-master ~]# yum makecache 1.2 安装Master，并启动服务123[root@salt-master ~]# yum -y install salt-master[root@salt-master ~]# systemctl enable salt-master[root@salt-master ~]# systemctl start salt-master 1.3 安装 Salt-Minion 指向 Salt-Master 网络地址123456[root@salt-minion01 ~]# yum -y install salt-minion# 可以使用主机名，也可以使用IP地址[root@salt-minion01 ~]# cp /etc/salt/minion&#123;,.back&#125;[root@salt-minion01 ~]# sed -i '/#master: /c\master: salt-master' /etc/salt/minion[root@salt-minion01 ~]# systemctl enable salt-minion[root@salt-minion01 ~]# systemctl start salt-minion SaltStack认证方式 Salt 的数据传输是通过 AES 加密，Master 和 Minion 之前在通信之前，需要进行认证。Salt 通过认证的方式保证安全性，完成一次认证后，Master 就可以控制 Minion 来完成各项工作了。1. minion 在第一次启动时候，会在 /etc/salt/pki/minion/ 下自动生成 minion.pem(private key) 和 minion.pub(public key), 然后将 minion.pub 发送给 master2. master 在第一次启动时，会在 /etc/salt/pki/master/ 下自动生成 master.pem 和 master.pub ；并且会接收到 minion 的 public key , 通过 salt-key 命令接收 minion public key， 会在 master 的 /etc/salt/pki/master/minions 目录下存放以 minion id 命令的 public key ；验证成功后同时 minion 会保存一份 master public key 在 minion的 /etc/salt/pki/minion/minion_master.pub里。 Salt认证原理总结 minion将自己的公钥发送给mastermaster认证后再将自己的公钥也发送给minion端 Master端认证示例根据上面提到的认证原理，先看下未认证前的master和minion的pki目录123456789101112131415161718192021# master上查看[root@salt-master ~]# tree /etc/salt/pki//etc/salt/pki/├── master│ ├── master.pem│ ├── master.pub│ ├── minions│ ├── minions_autosign│ ├── minions_denied│ ├── minions_pre│ │ └── salt-minion01│ └── minions_rejected└── minion# minion上查看[root@salt-minion01 ~]# tree /etc/salt/pki//etc/salt/pki/├── master└── minion ├── minion.pem └── minion.pub salt-key命令解释：12345678910111213141516[root@salt-master ~]# salt-key -L Accepted Keys: #已经接受的keyDenied Keys: #拒绝的keyUnaccepted Keys: #未加入的keyRejected Keys: #吊销的key#常用参数-L #查看KEY状态-A #允许所有-D #删除所有-a #认证指定的key-d #删除指定的key-r #注销掉指定key（该状态为未被认证）#配置master自动接受请求认证(master上配置 /etc/salt/master)auto_accept: True salt-key认证1234567891011121314151617181920212223#列出当前所有的key[root@salt-master ~]# salt-key -L Accepted Keys:Denied Keys:Unaccepted Keys:salt-minion01Rejected Keys:#添加指定minion的key[root@salt-master ~]# salt-key -a salt-minion01 -yThe following keys are going to be accepted:Unaccepted Keys:salt-minion01Key for minion salt-minion01 accepted.#添加所有minion的key[root@salt-master ~]# salt-key -A -y[root@salt-master ~]# salt-key -L Accepted Keys:salt-minion01Denied Keys:Unaccepted Keys:Rejected Keys: 上面认证完成后再次查看master和minion的pki目录12345678910111213141516171819202122# master上[root@salt-master ~]# tree /etc/salt/pki//etc/salt/pki/├── master│ ├── master.pem│ ├── master.pub│ ├── minions│ │ └── salt-minion01│ ├── minions_autosign│ ├── minions_denied│ ├── minions_pre│ └── minions_rejected└── minion# minion上[root@salt-minion01 ~]# tree /etc/salt/pki//etc/salt/pki/├── master└── minion ├── minion_master.pub ├── minion.pem └── minion.pub Saltstack远程执行 远程执行是 Saltstack 的核心功能之一。主要使用 salt 模块批量给选定的 minion 端执行相应的命令，并获得返回结果。 1、判断 salt 的 minion 主机是否存活1234567891011[root@salt-master ~]# salt '*' test.pingsalt-minion02: Truesalt-minion03: Truesalt-minion01: True# salt saltstack自带的一个命令# * 表示目标主机，这里表示所有目标主机# test.ping test是saltstack中的一个模块，ping则是这个模块下面的一个方法 2、saltstack使用 cmd.run模块远程执行shell命令1234#在指定目标minion节点运行uptime命令[root@salt-master ~]# salt 'salt-minion02' cmd.run 'uptime'salt-minion02: 18:13:08 up 28 min, 2 users, load average: 0.00, 0.04, 0.13 Saltstack配置管理Salt 通过State模块来进行文件的管理；通过YAML语法来描述，后缀是.sls的文件1、了解 YAML 参考：http://docs.saltstack.cn/topics/yaml/index.html123remove vim: pkg.removed: - name: vim 带有ID和每个函数调用的行都以冒号（:)结束。 每个函数调用在ID下面缩进两个空格。 参数作为列表传递给每个函数。 每行包含函数参数的行都以两个空格缩进开头，然后是连字符，然后是一个额外的空格。 如果参数采用单个值，则名称和值位于由冒号和空格分隔的同一行中。 如果一个参数需要一个列表，则列表从下一行开始，并缩进两个空格 2、配置sals ,定义环境 参考文档12345678910111213# 定义环境目录[root@salt-master ~]# vim /etc/salt/masterfile_roots: base: - /srv/salt/base dev: - /srv/salt/dev prod: - /srv/salt/prod# 创建上面定义的目录[root@salt-master ~]# mkdir -p /srv/salt/&#123;base,dev,prod&#125;# 重启服务[root@salt-master ~]# systemctl restart salt-master 3、编写第一个sls文件1234567891011121314151617181920212223# 在base环境下编写第一个安装apache的sls文件[root@salt-master ~]# cd /srv/salt/base/[root@salt-master base]# cat apache.sls apache-install: pkg.installed: - name: httpdapache-service: service.running: - name: httpd - enable: True# 在dev环境下编写一个安装ftp的sls文件[root@salt-master base]# cd /srv/salt/dev/[root@salt-master dev]# cat vsftpd.sls vsftpd-install: pkg.installed: - name: vsftpdvsftpd-service: service.running: - name: vsftpd - enable: True 4、使用salt命令的state状态模块让minion应用配置12345# 让所有的minion都安装apache（由于salt默认的环境就是base，所以可以直接在后面指定调用的apache.sls文件，不要后缀sls）[root@salt-master ~]# salt '*' state.sls apache# 让所有的minion都安装vsftpd（saltenv指定环境）[root@salt-master ~]# salt '*' state.sls vsftpd saltenv=dev 5、使用salt的高级状态使不同主机应用不同的配置123456789101112# topfile入口文件只能放在base环境[root@salt-master ~]# cat /srv/salt/base/top.sls base: 'salt-minion01': - apache 'salt-minion03': - apachedev: 'salt-minion02': - vsftpd 'salt-minion03': - vsftpd 6、使用salt命令执行高级状态，会将top.sls当做入口文件，进行调用12# 将高级状态应用到所有主机[root@salt-master ~]# salt '*' state.highstate Saltstack常用配置1、Salt Master配置Salt Master端的配置文件/etc/salt/master，常用配置如下：123456789101112131415161718interface: //指定bind 的地址(默认为0.0.0.0)publish_port: //指定发布端口(默认为4505)ret_port: //指定结果返回端口, 与minion配置文件中的master_port对应(默认为4506)user: //指定master进程的运行用户,如果调整, 则需要调整部分目录的权限(默认为root)timeout: //指定timeout时间, 如果minion规模庞大或网络状况不好,建议增大该值(默认5s)keep_jobs: //minion执行结果返回master, master会缓存到本地的cachedir目录,该参数指定缓存多长时间,可查看之间执行结果会占用磁盘空间(默认为24h)job_cache: //master是否缓存执行结果,如果规模庞大(超过5000台),建议使用其他方式来存储jobs,关闭本选项(默认为True)file_recv : //是否允许minion传送文件到master 上(默认是Flase)file_roots: //指定file server目录, 默认为: file_roots: base: - /srv/salt pillar_roots : //指定pillar 目录, 默认为: pillar_roots: base: - /srv/pillar log_level: //日志级别支持的日志级别有'garbage', 'trace', 'debug', info', 'warning', 'error', ‘critical ’ ( 默认为’warning’) 2、Salt Minion端的配置文件/etc/salt/minion，常用配置如下：1234567891011master: //指定master 主机(默认为salt)master_port: //指定认证和执行结果发送到master的哪个端口, 与master配置文件中的ret_port对应(默认为4506)id: //指定本minion的标识, salt内部使用id作为标识(默认为主机名)user: //指定运行minion的用户.由于安装包,启动服务等操作需要特权用户, 推荐使用root( 默认为root)cache_jobs : //minion是否缓存执行结果(默认为False)backup_mode: //在文件操作(file.managed 或file.recurse) 时, 如果文件发送变更,指定备份目录.当前有效providers : //指定模块对应的providers, 如在RHEL系列中, pkg对应的providers 是yumpkg5renderer: //指定配置管理系统中的渲染器(默认值为:yaml_jinja )file_client : //指定file clinet 默认去哪里(remote 或local) 寻找文件(默认值为remote)loglevel: //指定日志级别(默认为warning)tcp_keepalive : //minion 是否与master 保持keepalive 检查, zeromq3(默认为True)]]></content>
      <categories>
        <category>Saltstack</category>
      </categories>
      <tags>
        <tag>Saltstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下解压windows上压缩rar包]]></title>
    <url>%2F2019%2F04%2F30%2Flinux%E4%B8%8B%E8%A7%A3%E5%8E%8Bwindows%E4%B8%8A%E5%8E%8B%E7%BC%A9rar%E5%8C%85%2F</url>
    <content type="text"><![CDATA[下载# wget http://www.rarlab.com/rar/rarlinux-3.8.0.tar.gz 安装1234# tar xvzf rarlinux-3.8.0.tar.gz# cd rar# make # make install rar命令语法将 /etc 目录压缩为 etc.rar 命令为：\# rar a etc.rar /etc\将 etc.rar 解压 命令为：\12rar x etc.rarunrar -e etc.rar rar帮助手册123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108# rarRAR 3.80 Copyright (c) 1993-2008 Alexander Roshal 16 Sep 2008Shareware version Type RAR -? for helpUsage: rar &lt;command&gt; -&lt;switch 1&gt; -&lt;switch N&gt; &lt;archive&gt; &lt;files...&gt; &lt;@listfiles...&gt; &lt;path_to_extract\&gt;&lt;Commands&gt; a Add files to archive c Add archive comment cf Add files comment ch Change archive parameters cw Write archive comment to file d Delete files from archive e Extract files to current directory f Freshen files in archive i[par]=&lt;str&gt; Find string in archives k Lock archive l[t,b] List archive [technical, bare] m[f] Move to archive [files only] p Print file to stdout r Repair archive rc Reconstruct missing volumes rn Rename archived files rr[N] Add data recovery record rv[N] Create recovery volumes s[name|-] Convert archive to or from SFX t Test archive files u Update files in archive v[t,b] Verbosely list archive [technical,bare] x Extract files with full path&lt;Switches&gt; - Stop switches scanning ad Append archive name to destination path ag[format] Generate archive name using the current date ap&lt;path&gt; Set path inside archive as Synchronize archive contents av Put authenticity verification (registered versions only) av- Disable authenticity verification check c- Disable comments show cfg- Disable read configuration cl Convert names to lower case cu Convert names to upper case df Delete files after archiving dh Open shared files ds Disable name sort for solid archive dw Wipe files after archiving e[+]&lt;attr&gt; Set file exclude and include attributes ed Do not add empty directories en Do not put 'end of archive' block ep Exclude paths from names ep1 Exclude base directory from names ep3 Expand paths to full including the drive letter f Freshen files hp[password] Encrypt both file data and headers id[c,d,p,q] Disable messages ierr Send all messages to stderr ilog[name] Log errors to file (registered versions only) inul Disable all messages isnd Enable sound k Lock archive kb Keep broken extracted files m&lt;0..5&gt; Set compression level (0-store...3-default...5-maximal) mc&lt;par&gt; Set advanced compression parameters md&lt;size&gt; Dictionary size in KB (64,128,256,512,1024,2048,4096 or A-G) ms[ext;ext] Specify file types to store n&lt;file&gt; Include only specified file n@ Read file names to include from stdin n@&lt;list&gt; Include files in specified list file o[+|-] Set the overwrite mode ol Save symbolic links as the link instead of the file or Rename files automatically ow Save or restore file owner and group p[password] Set password p- Do not query password r Recurse subdirectories r0 Recurse subdirectories for wildcard names only rr[N] Add data recovery record rv[N] Create recovery volumes s[&lt;N&gt;,v[-],e] Create solid archive s- Disable solid archiving sc&lt;chr&gt;[obj] Specify the character set sfx[name] Create SFX archive si[name] Read data from standard input (stdin) sl&lt;size&gt; Process files with size less than specified sm&lt;size&gt; Process files with size more than specified t Test files after archiving ta&lt;date&gt; Process files modified after &lt;date&gt; in YYYYMMDDHHMMSS format tb&lt;date&gt; Process files modified before &lt;date&gt; in YYYYMMDDHHMMSS format tk Keep original archive time tl Set archive time to latest file tn&lt;time&gt; Process files newer than &lt;time&gt; to&lt;time&gt; Process files older than &lt;time&gt; ts&lt;m,c,a&gt;[N] Save or restore file time (modification, creation, access) u Update files v Create volumes with size autodetection or list all volumes v&lt;size&gt;[k,b] Create volumes with size=&lt;size&gt;*1000 [*1024, *1] ver[n] File version control vn Use the old style volume naming scheme vp Pause before each volume w&lt;path&gt; Assign work directory x&lt;file&gt; Exclude specified file x@ Read file names to exclude from stdin x@&lt;list&gt; Exclude files in specified list file y Assume Yes on all queries z[file] Read archive comment from file]]></content>
      <categories>
        <category>Linux运维日常</category>
      </categories>
      <tags>
        <tag>Linux运维日常</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[phpRedisAdmin搭建]]></title>
    <url>%2F2019%2F04%2F26%2FphpRedisAdmin%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[在已有的lnmp 环境下搭建 phpRedisAdmin 由于服务器做了限制，隧道端口转发等都不能使用，导致 RedisDesktopManager 管理工具不能使用，只能通过 phpRedisAdmin来进行管理了。 1）下载软件123# git clone https://github.com/ErikDubbelboer/phpRedisAdmin.git# cd phpRedisAdmin# git clone https://github.com/nrk/predis.git vendor 2）放置到web站点目录123# cd ..# mv phpRedisAdmin /opt/# chown apache.apache phpRedisAdmin/ -R 3）编辑nginx配置文件1234567891011121314151617181920212223242526272829# vim phpRedisAdmin.confserver &#123; listen 80; server_name www.redis.com; autoindex off; error_log logs/redis_error.log error; access_log logs/redis_access.log main; location / &#123; root /opt/phpRedisAdmin; index index.php index.html index.htm; &#125; fastcgi_intercept_errors on; error_page 404 /404.html; location ~ \.php$ &#123; root html; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /opt/phpRedisAdmin$fastcgi_script_name; include fastcgi_params; &#125; location ~ /\. &#123; deny all; error_page 403 /404.html; &#125;&#125; 4）配置连接管理，默认连接的是6379端口123456789101112# vim /opt/phpRedisAdmin/includes/config.sample.inc.php$config = array( 'servers' =&gt; array( array( 'name' =&gt; 'local server', // Optional name. 'host' =&gt; '127.0.0.1', 'port' =&gt; 6395, 'filter' =&gt; '*', 'scheme' =&gt; 'tcp', // Optional. Connection scheme. 'tcp' - for TCP connection, 'unix' - for connection by unix domain socket 'path' =&gt; '', // Optional. Path to unix domain socket. Uses only if 'scheme' =&gt; 'unix'. Example: '/var/run/redis/redis.sock' 'auth' =&gt; 'jieg6uy5Dach0yooxo1l' // Warning: The password is sent in plain-text to the Redis server. ), 5）windows添加host然后访问 http://www.redis.com]]></content>
      <categories>
        <category>Linux运维日常</category>
      </categories>
      <tags>
        <tag>Linux运维日常</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[phpMyAdmin搭建]]></title>
    <url>%2F2019%2F04%2F26%2FphpMyAdmin%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[在已有的 lnmp 环境下搭建 phpMyAdmin。 由于服务器做了限制，隧道端口转发等都不能使用，导致 Navicat 管理工具不能使用，只能通过 phpMyAdmin 来进行管理了。 下载phpMyAdmin官网地址：https://www.phpmyadmin.net/ 上传到服务器进行安装 123# unzip phpMyAdmin-4.8.5-all-languages.zip# mv phpMyAdmin-4.8.5-all-languages /opt/phpMyAdmin# chown apache. /opt/phpMyAdmin 编辑配置文件 config.default.php 123456# vim /opt/phpMyAdmin/libraries/config.default.php$cfg['Servers'][$i]['host'] = '130.39.113.45';$cfg['Servers'][$i]['port'] = '3306';$cfg['Servers'][$i]['socket'] = 'socket';$cfg['Servers'][$i]['user'] = 'tj_court';$cfg['Servers'][$i]['password'] = 'AigheiguSh4eesh0eey8'; 配置nginx站点 1234567891011121314151617181920212223242526272829# vim phpMyAdmin.confserver &#123; listen 80; server_name www.phpadmin.com; autoindex off; error_log logs/phpadmin_error.log error; access_log logs/phpadmin_access.log main; location / &#123; root /opt/phpMyAdmin; index index.php index.html index.htm; &#125; fastcgi_intercept_errors on; error_page 404 /404.html; location ~ \.php$ &#123; root html; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /opt/phpMyAdmin$fastcgi_script_name; include fastcgi_params; &#125; location ~ /\. &#123; deny all; error_page 403 /404.html; &#125;&#125; 配置完成后启动nginx访问配置的域名进行连接]]></content>
      <categories>
        <category>Linux运维日常</category>
      </categories>
      <tags>
        <tag>Linux运维日常</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL自动备份脚本]]></title>
    <url>%2F2019%2F04%2F24%2FMySQL%E8%87%AA%E5%8A%A8%E5%A4%87%E4%BB%BD%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[通过 shell 脚本定时备份数据库，并定时清理 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#!/bin/bash#Desc:本地数据库备份脚本#Date:2017-12-24#by:Lee-YJ#运行脚本前先创建一个备份用户，并授予权限#mysql&gt; grant select,insert,lock tables,show view,trigger on *.* to back@"127.0.0.1" identified by "oodaeh7phoe1iboh7Jua";#mysql&gt; flush privileges;mysqluser=back #用于数据库备份的用户名mysqlpassw=oodaeh7phoe1iboh7Jua #用户密码mysqlhost=127.0.0.1 #连接数据库的hostbakpath=/opt/data/Back/DBbak #备份数据库存放的路径bakdate=`date '+%Y%m%d'`baktime=`date '+%H%M'`#----------------------------获取mysql中的数据库名---------------------------------/usr/bin/mysql -u$mysqluser -h $mysqlhost -p$mysqlpassw -e "show databases;"|grep -v Database|grep -v information_schema |grep -v performance_schema|grep -v mysql|grep -v test &gt; /tmp/DBname#----------------------------数据库备份--mysqldump备份------------------------------if [ ! -d "$bakpath" ];then mkdir -p $bakpathficd $bakpathif [ ! -d "$bakdate" ];then mkdir $bakdateficd $bakdateecho "------------------------$bakdate$baktime-------------------------------------" &gt;&gt; $bakpath/DBbak.logfor DBname in `cat /tmp/DBname`;do if [ ! -d "$DBname" ];then mkdir $DBname fi /usr/bin/mysqldump --routines --triggers -u$mysqluser -h $mysqlhost -p$mysqlpassw $DBname &gt; $DBname/$baktime\.sql if [ $? = 0 ];then echo "$DBname Back OK!" &gt;&gt; $bakpath/DBbak.log else echo "$DBname Back ERROR!" &gt;&gt; $bakpath/DBbak.log fidoneecho "" &gt;&gt; $bakpath/DBbak.log #----------------------------删除5天之前的数据备份，并随机保留一份--------------------------------shopt -s extglobolddate=`date '+%Y%m%d' -d "-6 days"`cd $bakpath/$olddatefor deldbname in `ls $bakpath/$olddate`;do cd $deldbname file=`ls| sort -R | head -n1` rm -f !($file)doneif [ ! -d "$bakpath/Oldest" ];then mkdir -p $bakpath/Oldestfimv $bakpath/$olddate $bakpath/Oldest/rm -f /tmp/DBname]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Linux运维日常</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ftp虚拟用户]]></title>
    <url>%2F2019%2F04%2F23%2Fftp%E8%99%9A%E6%8B%9F%E7%94%A8%E6%88%B7%2F</url>
    <content type="text"><![CDATA[需求：进（账户）\username: ftpComeSsbq\password: ftpcome#@UkieO9\上传文件的目录：/opt/ftp/come 出（账户）\username: ftpOutSsbq\password: ftpout#@45oUkie\上床文件的目录：/opt/ftp/out 具体步骤1 安装软件1# yum -y install vsftpd 2 创建相应的ftp数据目录1# mkdir -p /opt/ftp/&#123;come,out&#125; 3 创建一个用户提供给虚拟用户使用1# useradd -s /sbin/nologin virtual 4 将ftp数据目录设置成virtual用户12345# chown virtual. /opt/ftp/ -R# ll /opt/ftp/total 8drwxr-xr-x 2 virtual virtual 4096 Apr 17 12:07 comedrwxr-xr-x 2 virtual virtual 4096 Apr 17 12:07 out 5 创建虚拟帐号与密码的文本文件(一行账号，一行密码, 注意不要有多余的空格)12345# vim /etc/vsftpd/logins.txtftpComeSsbqftp_come_#@UkieO9ftpOutSsbqftp_out_#@45oUkie 6 将创建好的密码文件txt格式转换db格式1# db_load -T -t hash -f /etc/vsftpd/logins.txt /etc/vsftpd/login.db 7 定义db文件的权限1# chmod 600 /etc/vsftpd/login.db 8 定义pam认证文件123# vim /etc/pam.d/ftpauth required /lib64/security/pam_userdb.so db=/etc/vsftpd/loginaccount required /lib64/security/pam_userdb.so db=/etc/vsftpd/login 9 配置vsftpd主配置文件123456789101112131415161718192021222324252627282930313233343536# cp /etc/vsftpd/vsftpd.conf&#123;,.bak&#125;# vim /etc/vsftpd/vsftpd.conf#禁止匿名登录FTP服务器anonymous_enable=NO#允许本地用户登录FTP服务器local_enable=YES#可以上传(全局控制) write_enable=NO#匿名用户可以上传anon_upload_enable=NO#匿名用户可以建目录anon_mkdir_write_enable=NO#匿名用户修改删除anon_other_write_enable=NO#全部用户被限制在主目录chroot_local_user=YES#将所有用户看成虚拟用户guestguest_enable=YES#指定虚拟用户，也就是将guest用户映射到virtual用户guest_username=virtual#指定为独立服务listen=YES#指定监听的端口listen_port=21#开启被动模式pasv_enable=YES#FTP服务器公网IPpasv_address=120.79.93.66#设置被动模式下，建立数据传输可使用port范围的最小值pasv_min_port=10000#设置被动模式下，建立数据传输可使用port范围的最大值pasv_max_port=10088#是否允许匿名用户下载全局可读的文件anon_world_readable_only=NO#指定虚拟用户配置文件的路径user_config_dir=/etc/vsftpd/user_conf 10 创建上面配置文件中指定的子配置文件目录 user_conf1# mkdir /etc/vsftpd/user_conf 11 定义 ftpComeSsbq 用户的配置文件1234567# vim /etc/vsftpd/user_conf/ftpComeSsbqwrite_enable=YESanon_world_readable_only=noanon_upload_enable=YESanon_mkdir_write_enable=YESanon_other_write_enable=YESlocal_root=/opt/ftp/come 12 定义 ftpOutSsbq 用户的配置文件1234567# vim /etc/vsftpd/user_conf/ftpOutSsbqwrite_enable=YESanon_world_readable_only=noanon_upload_enable=YESanon_mkdir_write_enable=YESanon_other_write_enable=YESlocal_root=/opt/ftp/out 13 启动vsftpd1# service vsftpd start 14 测试…]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux运维日常</tag>
      </tags>
  </entry>
</search>
